{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"develop/","text":"The following application examples demonstrate how to use Coral FPGA Resource Manager, to automatically scale your accelerated solutions using in-house or 3 rd -party accelerators, from high-level programming languages. InAccel provides a stack of accelerators including Machine Learning techniques, Financial formulas, SQL functions and Cryptographic algorithms. Please check InAccel Store for a complete list of the available cores. The projects in this section enable the seamless integration of InAccel Coral with the available FPGA accelerators through high-level programming interfaces. This integration makes it possible to accelerate applications using state-of-the-art frameworks for big data analytics (like Apache Spark) requiring zero changes in your original application code. What these projects basically do, is pairing together these two worlds providing complete accelerated solutions. From your perspective nothing has changed, except that from now on your applications will run lightning fast. Supported FPGA Vendors - InAccel supports both Intel and Xilinx FPGAs Currently there are two major FPGA vendors, Xilinx and Intel (formerly Altera). InAccel solutions are built for both FPGA families and all the platforms they provide. Supported Cloud Platforms - Find us on every public Cloud with FPGA instances All major cloud vendors have started deploying FPGAs in their infrastructure. InAccel is among the first companies providing production-ready solutions on Amazon, Alibaba, Nimbix and Huawei Cloud services.","title":"Overview"},{"location":"develop/#supported-fpga-vendors","text":"InAccel supports both Intel and Xilinx FPGAs Currently there are two major FPGA vendors, Xilinx and Intel (formerly Altera). InAccel solutions are built for both FPGA families and all the platforms they provide.","title":"Supported FPGA Vendors"},{"location":"develop/#supported-cloud-platforms","text":"Find us on every public Cloud with FPGA instances All major cloud vendors have started deploying FPGAs in their infrastructure. InAccel is among the first companies providing production-ready solutions on Amazon, Alibaba, Nimbix and Huawei Cloud services.","title":"Supported Cloud Platforms"},{"location":"get-started/","text":"Welcome! We are excited that you want to learn InAccel. Through this tutorial, we will guide you step-by-step on how to leverage InAccel toolset to ship and run your accelerator workloads in a simple, straightforward and efficient manner. You will mainly learn how to: Deploy a simple FPGA accelerated application Develop your own FPGA accelerated application Deploy your bitstreams Accelerate your app Deploy your app (Host/Docker/Kubernetes) Download and Install InAccel - This tutorial assumes that you have already installed InAccel. If you don't have InAccel FPGA Operator installed, choose your preferred deployment environment to download and setup InAccel. Docker Kubernetes Start the Tutorial - Prior to the hands-on tutorial describing the exact steps on how to get started, we consider valuable to provide some motivation behind the necessity of an intelligent Bitstream Repository combined with Coral FPGA Resource Manager as well as some high-level details of their functionality. Motivation - The end of Moore's Law signified the shift from general purpose computing to tailor-made, specialized architectures and hardware. As a matter of fact, all major cloud providers have incorporated FPGAs into their cloud services. While both CPUs and GPUs have developed a wide, well-established surrounding software ecosystem around their technologies, unfortunately FPGAs are unable to compete with the aforementioned hardware on the axis of software tools available. Currently, an FPGA application developer needs to be aware of low-level details of the underlying hardware. New layers of granularity need to be introduced to delegate the hardware details to accelerator designers while providing a typical software API to application developers. At InAccel, we believe that wide adoption of hardware technologies requires significant levels of granularity and abstraction, and we are working towards that direction to mitigate the steep learning curve of developing, deploying and running accelerators. Bitstream Repository Concepts - Before we proceed with this tutorial, let's take a step back and elaborate on what a repository of bitstream artifacts represents and why do we need it in the first place. What is a bitstream artifact? - A bitstream artifact is a set of files capable of fully describing an FPGA design. InAccel introduces the notion of a specification file, where a bitstream developer can explicitly declare all the properties pertaining to the FPGA binary she is willing to deploy. Therefore, a compiled bitstream along with its specification file (metadata) comprise the bitstream artifact. Why do we need a bitstream artifact? - By accompanying each bitstream with a descriptive file that contains various metadata we are generating a high-level representation of the bitstream. Through this representation we achieve to decouple the background knowledge required by the accelerator designer and the application developers. In particular, the bitstream developer is responsible to create the specification file with all the required architecture and platform details. In contrast, the application developer needs only to be aware of the bitstream/kernel identifiers as well as the argument list of each accelerator to invoke them properly in her application. Advantages of representing bitstreams as artifacts: Encapsulate every bit of information describing an FPGA design within a single file. Enable repository management, versioning capabilities and accelerator performance tracking (see below). Conceal low-level hardware details from the application developer allowing a universal, vendor agnostic (Intel / Xilinx) packaging format. Each artifact contains useful metadata for a wide range of utilities and tools. Representing self-contained entities as artifacts is a valuable software paradigm. What is a bitstream repository? - A bitstream repository is a central place in which an aggregation of the aforementioned artifacts is kept and maintained in an organized way. Protection (encryption/decryption) capabilities enable secure storage of your FPGA binaries to local and remote repositories with fine-grained access control policies. Moreover, the supplied metadata in each artifact simplify the process of maintaining snapshot/release artifacts, allowing continuous integration and delivery of your accelerated solutions through tailor made development pipelines. Now the evaluation and benchmarking of accelerators of different target vendors or versions is rendered trivial. Finally, InAccel offers an end-to-end JFrog Bitstream repository solution to fully manage the deployment lifecycle of your FPGA binaries. More information can be found here . What is bitstream deployment? - Bitstream deployment is the process of storing your bitstream artifact into a bitstream repository. Once your bitstream is deployed, you can expect its kernels to be readily available from every application requesting accelerators from Coral. Additionally, through our CLI you can list its available accelerators along with many other useful information. Coral FPGA Resource Manager Concepts - Now that we addressed some major shortcomings of the specialized hardware industry and explained the main concepts of accelerator deployment that InAccel introduces, we continue by presenting InAccel's approach on FPGA application development deficiencies. InAccel Coral is a scalable , reliable and fault-tolerant distributed acceleration system responsible for monitoring , virtualizing and orchestrating clusters of FPGAs. Coral also introduces high-level abstractions by exposing FPGAs as a single pool of accelerators to any application developer that she can easily invoke through simple API calls. Finally, Coral runs as a microservice and is able to run on top of other state-of-the-art resource managers like Hadoop YARN and Kubernetes. To summarize, Coral has the following primary goals: Serve as a universal orchestrator for FPGA resources and acceleration requests. Improve scalability and maximize performance of deployed accelerators, ensuring the secure sharing of the available resources. Abstract away cumbersome parallel programming languages (like OpenCL) without compromising flexibility. Encompass bitstream management and protection capabilities. Since we briefly covered the necessity of a universal FPGA resource manager as well as InAccel's approach on the issue, continue to Part 2 to run a sample application. You can find more information on Coral integrations and features here .","title":"Part 1: Orientation & Setup"},{"location":"get-started/#download-and-install-inaccel","text":"This tutorial assumes that you have already installed InAccel. If you don't have InAccel FPGA Operator installed, choose your preferred deployment environment to download and setup InAccel. Docker Kubernetes","title":"Download and Install InAccel"},{"location":"get-started/#start-the-tutorial","text":"Prior to the hands-on tutorial describing the exact steps on how to get started, we consider valuable to provide some motivation behind the necessity of an intelligent Bitstream Repository combined with Coral FPGA Resource Manager as well as some high-level details of their functionality.","title":"Start the Tutorial"},{"location":"get-started/#motivation","text":"The end of Moore's Law signified the shift from general purpose computing to tailor-made, specialized architectures and hardware. As a matter of fact, all major cloud providers have incorporated FPGAs into their cloud services. While both CPUs and GPUs have developed a wide, well-established surrounding software ecosystem around their technologies, unfortunately FPGAs are unable to compete with the aforementioned hardware on the axis of software tools available. Currently, an FPGA application developer needs to be aware of low-level details of the underlying hardware. New layers of granularity need to be introduced to delegate the hardware details to accelerator designers while providing a typical software API to application developers. At InAccel, we believe that wide adoption of hardware technologies requires significant levels of granularity and abstraction, and we are working towards that direction to mitigate the steep learning curve of developing, deploying and running accelerators.","title":"Motivation"},{"location":"get-started/#bitstream-repository-concepts","text":"Before we proceed with this tutorial, let's take a step back and elaborate on what a repository of bitstream artifacts represents and why do we need it in the first place.","title":"Bitstream Repository Concepts"},{"location":"get-started/#what-is-a-bitstream-artifact","text":"A bitstream artifact is a set of files capable of fully describing an FPGA design. InAccel introduces the notion of a specification file, where a bitstream developer can explicitly declare all the properties pertaining to the FPGA binary she is willing to deploy. Therefore, a compiled bitstream along with its specification file (metadata) comprise the bitstream artifact.","title":"What is a bitstream artifact?"},{"location":"get-started/#why-do-we-need-a-bitstream-artifact","text":"By accompanying each bitstream with a descriptive file that contains various metadata we are generating a high-level representation of the bitstream. Through this representation we achieve to decouple the background knowledge required by the accelerator designer and the application developers. In particular, the bitstream developer is responsible to create the specification file with all the required architecture and platform details. In contrast, the application developer needs only to be aware of the bitstream/kernel identifiers as well as the argument list of each accelerator to invoke them properly in her application. Advantages of representing bitstreams as artifacts: Encapsulate every bit of information describing an FPGA design within a single file. Enable repository management, versioning capabilities and accelerator performance tracking (see below). Conceal low-level hardware details from the application developer allowing a universal, vendor agnostic (Intel / Xilinx) packaging format. Each artifact contains useful metadata for a wide range of utilities and tools. Representing self-contained entities as artifacts is a valuable software paradigm.","title":"Why do we need a bitstream artifact?"},{"location":"get-started/#what-is-a-bitstream-repository","text":"A bitstream repository is a central place in which an aggregation of the aforementioned artifacts is kept and maintained in an organized way. Protection (encryption/decryption) capabilities enable secure storage of your FPGA binaries to local and remote repositories with fine-grained access control policies. Moreover, the supplied metadata in each artifact simplify the process of maintaining snapshot/release artifacts, allowing continuous integration and delivery of your accelerated solutions through tailor made development pipelines. Now the evaluation and benchmarking of accelerators of different target vendors or versions is rendered trivial. Finally, InAccel offers an end-to-end JFrog Bitstream repository solution to fully manage the deployment lifecycle of your FPGA binaries. More information can be found here .","title":"What is a bitstream repository?"},{"location":"get-started/#what-is-bitstream-deployment","text":"Bitstream deployment is the process of storing your bitstream artifact into a bitstream repository. Once your bitstream is deployed, you can expect its kernels to be readily available from every application requesting accelerators from Coral. Additionally, through our CLI you can list its available accelerators along with many other useful information.","title":"What is bitstream deployment?"},{"location":"get-started/#coral-fpga-resource-manager-concepts","text":"Now that we addressed some major shortcomings of the specialized hardware industry and explained the main concepts of accelerator deployment that InAccel introduces, we continue by presenting InAccel's approach on FPGA application development deficiencies. InAccel Coral is a scalable , reliable and fault-tolerant distributed acceleration system responsible for monitoring , virtualizing and orchestrating clusters of FPGAs. Coral also introduces high-level abstractions by exposing FPGAs as a single pool of accelerators to any application developer that she can easily invoke through simple API calls. Finally, Coral runs as a microservice and is able to run on top of other state-of-the-art resource managers like Hadoop YARN and Kubernetes. To summarize, Coral has the following primary goals: Serve as a universal orchestrator for FPGA resources and acceleration requests. Improve scalability and maximize performance of deployed accelerators, ensuring the secure sharing of the available resources. Abstract away cumbersome parallel programming languages (like OpenCL) without compromising flexibility. Encompass bitstream management and protection capabilities. Since we briefly covered the necessity of a universal FPGA resource manager as well as InAccel's approach on the issue, continue to Part 2 to run a sample application. You can find more information on Coral integrations and features here .","title":"Coral FPGA Resource Manager Concepts"},{"location":"get-started/overview/","text":"InAccel is an open platform for developing, shipping, and running accelerated applications. InAccel enables you to separate your applications from your accelerators so you can deliver software quickly. By taking advantage of InAccel's methodologies for shipping, testing, and deploying accelerators quickly, you can significantly reduce the delay between creating an FPGA design and running it in production. The following figure summarizes InAccel's major components : Coral FPGA Resource Manager - Coral is a fast and general-purpose FPGA resource manager. It provides high-level APIs in Java, Scala, Python and C++, and a unified engine that supports every multi-FPGA platform. Coral is also shipped with higher-level integrations including Apache Arrow for zero-copy, lightning-fast data transfers and Apache Spark for seamless machine learning acceleration. Coral runs on any UNIX-like system (e.g. Linux). It\u2019s easy to deploy on any machine, since it lives inside a containerized environment \u2014 all you need is to have the vendor-specific FPGA runtime installed on your system and run it with the container pointing to that installation. All Coral versions are packaged as docker images hosted in InAccel Docker Hub . Bitstream Repository - A bitstream repository is a central place comprised of bitstream artifacts which are kept and maintained in an organized way. The concept of a bitstream repository facilitates the development and deployment of bitstreams and eliminates cumbersome procedures of manually tracking them. For a detailed overview you can check our Tutorial on usage of bitstream repository capabilities. Accelerators - We provide both readily available accelerator cores and customized solutions based on the customer\u2019s requirements. The accelerators are provided in the form of IP cores and can be used either on premises or in the Cloud . FPGA accelerators can massively accelerate computational intensive algorithms and that is why they are a perfect fit for a plethora of cutting edge fields including data analytics, machine learning, compression algorithms etc. Wide Compatibility InAccel accelerators are fully compatible with Amazon AWS , Alibaba Cloud and Huawei Cloud FPGA instances. They are also compatible for both Intel and Xilinx FPGAs. InAccel CLI - InAccel is a lightweight and easy-to-use Command Line Interface (CLI), which bundles the aforementioned utilities and allows users to operate them from a single tool through straightforward commands. We are currently offering installation of inaccel through packages or repositories for Debian-based and RHEL-based distributions. Getting Started - After installing inaccel , you can learn the basics with our Getting Started guide.","title":"InAccel overview"},{"location":"get-started/overview/#coral-fpga-resource-manager","text":"Coral is a fast and general-purpose FPGA resource manager. It provides high-level APIs in Java, Scala, Python and C++, and a unified engine that supports every multi-FPGA platform. Coral is also shipped with higher-level integrations including Apache Arrow for zero-copy, lightning-fast data transfers and Apache Spark for seamless machine learning acceleration. Coral runs on any UNIX-like system (e.g. Linux). It\u2019s easy to deploy on any machine, since it lives inside a containerized environment \u2014 all you need is to have the vendor-specific FPGA runtime installed on your system and run it with the container pointing to that installation. All Coral versions are packaged as docker images hosted in InAccel Docker Hub .","title":"Coral FPGA Resource Manager"},{"location":"get-started/overview/#bitstream-repository","text":"A bitstream repository is a central place comprised of bitstream artifacts which are kept and maintained in an organized way. The concept of a bitstream repository facilitates the development and deployment of bitstreams and eliminates cumbersome procedures of manually tracking them. For a detailed overview you can check our Tutorial on usage of bitstream repository capabilities.","title":"Bitstream Repository"},{"location":"get-started/overview/#inaccel-cli","text":"InAccel is a lightweight and easy-to-use Command Line Interface (CLI), which bundles the aforementioned utilities and allows users to operate them from a single tool through straightforward commands. We are currently offering installation of inaccel through packages or repositories for Debian-based and RHEL-based distributions.","title":"InAccel CLI"},{"location":"get-started/overview/#getting-started","text":"After installing inaccel , you can learn the basics with our Getting Started guide.","title":"Getting Started"},{"location":"get-started/part2/","text":"Now that we have discussed about the main FPGA world concepts, let\u2019s run a simple vector addition accelerated application. Depending on the deployed environment, we are going to either use Docker or Kubernetes instructions below: Docker Kubernetes Step 1 Install docker-compose-plugin : To run the sample application you will need to have installed the docker compose plugin. If docker compose is not installed in your system please follow the instructions below: Apt Yum sudo apt docker-compose-plugin sudo yum docker-compose-plugin Step 2 Create a docker-compose.yml file: The docker-compose file is basically comprised of two services: inaccel-vadd and init and a volume shared among the two services. Init runs first and downloads the right configuration file (bitstream) using inaccel/cli docker image and finally stores it in the shared volume. inaccel-vadd , that depends on init and uses inaccel/vadd image, is then able to invoke the accelerated vadd application. Intel PAC A10 Xilinx AWS VU9P F1 Xilinx U200 Xilinx U250 Xilinx U280 Xilinx U50 38d782e3b6125343b9342433e348ac4c docker-compose.yml services : inaccel-vadd : depends_on : init : condition : service_completed_successfully image : inaccel/vadd volumes : - volume:/var/lib/inaccel init : command : - bitstream - install - https://store.inaccel.com/artifactory/bitstreams/intel/pac_a10/38d782e3b6125343b9342433e348ac4c/vector/1/1addition image : inaccel/cli volumes : - volume:/var/lib/inaccel volumes : volume : driver : inaccel AWS | shell-v04261818_201920.2 docker-compose.yml services : inaccel-vadd : depends_on : init : condition : service_completed_successfully image : inaccel/vadd volumes : - volume:/var/lib/inaccel init : command : - bitstream - install - https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/vector/1/1addition image : inaccel/cli volumes : - volume:/var/lib/inaccel volumes : volume : driver : inaccel xdma_201830.2 docker-compose.yml services : inaccel-vadd : depends_on : init : condition : service_completed_successfully image : inaccel/vadd volumes : - volume:/var/lib/inaccel init : command : - bitstream - install - https://store.inaccel.com/artifactory/bitstreams/xilinx/u200/xdma_201830.2/vector/1/1addition image : inaccel/cli volumes : - volume:/var/lib/inaccel volumes : volume : driver : inaccel Azure | gen3x16_xdma_shell_2.1 docker-compose.yml services : inaccel-vadd : depends_on : init : condition : service_completed_successfully image : inaccel/vadd volumes : - volume:/var/lib/inaccel init : command : - bitstream - install - https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/gen3x16_xdma_shell_2.1/azure/vector/1/1addition image : inaccel/cli volumes : - volume:/var/lib/inaccel volumes : volume : driver : inaccel xdma_201830.2 docker-compose.yml services : inaccel-vadd : depends_on : init : condition : service_completed_successfully image : inaccel/vadd volumes : - volume:/var/lib/inaccel init : command : - bitstream - install - https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/vector/1/1addition image : inaccel/cli volumes : - volume:/var/lib/inaccel volumes : volume : driver : inaccel xdma_201920.3 docker-compose.yml services : inaccel-vadd : depends_on : init : condition : service_completed_successfully image : inaccel/vadd volumes : - volume:/var/lib/inaccel init : command : - bitstream - install - https://store.inaccel.com/artifactory/bitstreams/xilinx/u280/xdma_201920.3/vector/1/1addition image : inaccel/cli volumes : - volume:/var/lib/inaccel volumes : volume : driver : inaccel gen3x16_xdma_201920.3 docker-compose.yml services : inaccel-vadd : depends_on : init : condition : service_completed_successfully image : inaccel/vadd volumes : - volume:/var/lib/inaccel init : command : - bitstream - install - https://store.inaccel.com/artifactory/bitstreams/xilinx/u50/gen3x16_xdma_201920.3/vector/1/1addition image : inaccel/cli volumes : - volume:/var/lib/inaccel volumes : volume : driver : inaccel Step 3 Run vector addition example: docker compose run inaccel-vadd Step 4 Clean environment: Delete any containers or volumes created as well as docker-compose.yml file itself: docker compose down --volumes rm docker-compose.yml Step 1 Create a pod.yml file: To deploy FPGA accelerated applications to Kubernetes, we have to first of all enable InAccel FPGA Operator. To do so, we use the inaccel/fpga: enabled label. Apart from that, to target particular FPGA types, we add a nodeSelector to our workload specification (e.g. intel/pac_a10: 38d782e3b6125343b9342433e348ac4c ). Additionally, we specify a resource limit to configure workloads to consume FPGAs (e.g. intel/pac_a10: 1 ). Finally, we are able to pre-fetch any required bitstreams, using a simple annotation and the bitstreams' URL. Intel PAC A10 Xilinx AWS VU9P F1 Xilinx U200 Xilinx U250 Xilinx U280 Xilinx U50 38d782e3b6125343b9342433e348ac4c pod.yml apiVersion : v1 kind : Pod metadata : annotations : inaccel/cli : | bitstream install https://store.inaccel.com/artifactory/bitstreams/intel/pac_a10/38d782e3b6125343b9342433e348ac4c/vector/1/1addition labels : inaccel/fpga : enabled name : inaccel-vadd spec : containers : - image : inaccel/vadd name : inaccel-vadd resources : limits : intel/pac_a10 : 1 nodeSelector : intel/pac_a10 : 38d782e3b6125343b9342433e348ac4c restartPolicy : Never AWS | shell-v04261818_201920.2 pod.yml apiVersion : v1 kind : Pod metadata : annotations : inaccel/cli : | bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/vector/1/1addition labels : inaccel/fpga : enabled name : inaccel-vadd spec : containers : - image : inaccel/vadd name : inaccel-vadd resources : limits : xilinx/aws-vu9p-f1 : 1 nodeSelector : xilinx/aws-vu9p-f1 : dynamic-shell restartPolicy : Never xdma_201830.2 pod.yml apiVersion : v1 kind : Pod metadata : annotations : inaccel/cli : | bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u200/xdma_201830.2/vector/1/1addition labels : inaccel/fpga : enabled name : inaccel-vadd spec : containers : - image : inaccel/vadd name : inaccel-vadd resources : limits : xilinx/u200 : 1 nodeSelector : xilinx/u200 : xdma_201830.2 restartPolicy : Never Azure | gen3x16_xdma_shell_2.1 pod.yml apiVersion : v1 kind : Pod metadata : annotations : inaccel/cli : | bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/gen3x16_xdma_shell_2.1/azure/vector/1/1addition labels : inaccel/fpga : enabled name : inaccel-vadd spec : containers : - image : inaccel/vadd name : inaccel-vadd resources : limits : xilinx/u250 : 1 nodeSelector : xilinx/u250 : gen3x16_xdma_shell_2.1 restartPolicy : Never xdma_201830.2 pod.yml apiVersion : v1 kind : Pod metadata : annotations : inaccel/cli : | bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/vector/1/1addition labels : inaccel/fpga : enabled name : inaccel-vadd spec : containers : - image : inaccel/vadd name : inaccel-vadd resources : limits : xilinx/u250 : 1 nodeSelector : xilinx/u250 : xdma_201830.2 restartPolicy : Never xdma_201920.3 pod.yml apiVersion : v1 kind : Pod metadata : annotations : inaccel/cli : | bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u280/xdma_201920.3/vector/1/1addition labels : inaccel/fpga : enabled name : inaccel-vadd spec : containers : - image : inaccel/vadd name : inaccel-vadd resources : limits : xilinx/u280 : 1 nodeSelector : xilinx/u280 : xdma_201920.3 restartPolicy : Never gen3x16_xdma_201920.3 pod.yml apiVersion : v1 kind : Pod metadata : annotations : inaccel/cli : | bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u50/gen3x16_xdma_201920.3/vector/1/1addition labels : inaccel/fpga : enabled name : inaccel-vadd spec : containers : - image : inaccel/vadd name : inaccel-vadd resources : limits : xilinx/u50 : 1 nodeSelector : xilinx/u50 : gen3x16_xdma_201920.3 restartPolicy : Never Step 2 Deploy pod.yml file and inspect the logs: kubectl apply --filename pod.yml kubectl wait --for condition = ready --timeout -1s pod/inaccel-vadd kubectl logs --follow pod/inaccel-vadd Step 3 Delete the pod created for the accelerated vadd application: kubectl delete pod/inaccel-vadd Recap - In this short section, we learned how we can deploy a simple pre-compiled application targeting datacenter FPGAs through Docker or Kubernetes. Next, we are going to package our own accelerator and write an application from scratch to cover all the scenarios of a cloud deployment targeting FPGA resources.","title":"Part 2: Sample application"},{"location":"get-started/part2/#recap","text":"In this short section, we learned how we can deploy a simple pre-compiled application targeting datacenter FPGAs through Docker or Kubernetes. Next, we are going to package our own accelerator and write an application from scratch to cover all the scenarios of a cloud deployment targeting FPGA resources.","title":"Recap"},{"location":"get-started/part3/","text":"This part of the tutorial refers primarily to bitstream developers who are willing to distribute their FPGA accelerators. By deploying their bitstreams their implemented accelerators will be available to easily get invoked by any application through Coral API. If you won't be writing your own bitstreams you may want to skip this section and move on to Part 4 to learn how to invoke prebuilt accelerators in your applications. In this section, we walk you through the process of installing an FPGA bitstream binary either locally or remotely. We will teach you how to create bitstream artifacts and how to encrypt and deploy them to a local or a remote repository. Bitstream Development (Optional) - In this section, we provide the \"Hello World\" accelerator example for bitstream development comprised of a simple vector addition kernel. It receives as input two float arrays and adds their respective elements in an output array. Since the kernel is considered trivial, we will not include the implementation in this tutorial. However you can download the source code from our GitHub repository . Build instructions for AWS - Launch an FPGA Developer AMI instance. Install the AWS FPGA development toolkit. git clone https://github.com/aws/aws-fpga.git -b v1.4.22 source aws-fpga/vitis_setup.sh Clone our demo repository and compile the included FPGA kernels. git clone https://github.com/inaccel/vadd.git v++ vadd/src/vadd.cl --platform ${ AWS_PLATFORM } -o vadd.hw.xo v++ --link vadd.hw.xo --platform ${ AWS_PLATFORM } -o vadd.hw.xclbin Warning FPGA binary compilation is a very slow process. You can still skip this part and use our prebuild FPGA binary. Configure AWS CLI with your credentials aws configure Create an Amazon FPGA Image (AFI). Make sure you specify your S3 bucket for the generation of the AFI. ${ VITIS_DIR } /tools/create_vitis_afi.sh -xclbin = vadd.hw.xclbin -o = vadd -s3_bucket = <your-aws-bucket> -s3_dcp_key = demo_dcp -s3_logs_key = demo_logs For more details on the full AWS F1 development lifecycle please refer to this Vitis Quick Start Guide . Bitstream Packaging - We assume that you have already implemented and compiled a simple bitstream comprised of the kernel mentioned above. In case you didn't complete the above steps for AWS, you can still download the prebuilt bitstream binary ( vadd.awsxclbin ) from InAccel Store . At this point, we are ready to move on to the next stage and describe our generated bistream, creating the specification file ( bitstream.json or bitstream.xml ) that will accompany the bitstream artifact. Describe your FPGA binary - As mentioned before, each bitstream artifact contains a descriptive JSON (or XML) file that defines all the details required to thoroughly describe a compiled bitstream, like the target platform, the bitstream kernels, the version and other metadata. Below we present a verbose, yet simple bitstream.json file for our bitstream: { \"name\" : \"vadd.awsxclbin\" , \"bitstreamId\" : \"vector\" , \"version\" : \"1\" , \"description\" : \"https://github.com/inaccel/vadd\" , \"platform\" : { \"vendor\" : \"xilinx\" , \"name\" : \"aws-vu9p-f1\" , \"version\" : \"shell-v04261818_201920.2\" , \"label\" : [ \"aws\" ] }, \"kernels\" : [ { \"name\" : [ \"vadd\" ], \"kernelId\" : \"addition\" , \"arguments\" : [ { \"type\" : \"float*\" , \"name\" : \"a\" , \"memory\" : [ \"0\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"b\" , \"memory\" : [ \"0\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"c\" , \"memory\" : [ \"0\" ], \"access\" : \"w\" }, { \"type\" : \"int\" , \"name\" : \"size\" } ] } ] } As you can notice, we designed bitstream specification file to contain fields which are self-descriptive in order to reduce the documentation lookup overhead. However, we will make a brief description for most of the fields with the risk of being redundant, to make you comfortable on creating your own. Let's explore the fields we used in our specification file: name - Bitstream name should match the FPGA binary name bitstreamId - Prefix to be used by applications to invoke kernels in this bitstream version - Bitstream version description - Bitstream description platform vendor - FPGA vendor name - FPGA board name version - FPGA board version Now it's time to declare your bitstream kernels. Those are declared inside the specification file as an array named kernels . Each kernel is represented as an object with the following fields: name - Actual name list of the kernel functions inside the bitstream kernelId - Alias name to invoke the specific kernels in your applications arguments - An array specifying all the kernel arguments type - Data type (a star '*' indicates that the argument is an array) name - Descriptive argument name memory - Memory bank identifier ( only for arrays, not for scalars ) access - Access permissions ( only for arrays, not for scalars ) You are now ready to deploy your first bitstream artifact. For a more detailed overview of the metadata supported by the bitstream specification file check out the detailed Bitstream Reference . Bitstream Deployment - Once you have created a bitstream artifact by providing a complete bitstream specification file along with your bitstream binary, you can easily deploy it through InAccel's CLI. Refer to Command Line Interface Reference for extensive CLI usage. The Bitstream binary and its specification file must reside in the same directory. Local deployment - Below, we present a typical workflow of deploying your bitstream artifact to your local repository. When you deploy locally, your bitstreams are stored in your local machine and become available to Coral running on your machine immediately. Follow the steps below to get started: Step 1: Deploy the bitstream artifact to your local repository. The path given as argument should contain both the binary and the specification file. $ inaccel bitstream install /local/path/to/bitstream-directory Step 2: Verify that your bitstream is properly installed to your local repository by issuing the command below. $ inaccel bitstream list CHECKSUM BITSTREAM ID VERSION KERNEL IDS PLATFORM 2c480f9424d4 vector 1 [addition] aws-vu9p-f1 (xilinx dynamic-shell) If the output of the command matches the one presented above then you have successfully installed the demo bitstream artifact to your local repository. After installing the bitstream artifact to your local repository, you can expect that all kernels of the corresponding bitstream will be easily and securely accessible from every application requesting accelerators from Coral. As a bonus, to list the available accelerators of your recently deployed bitstream along with their function prototypes , you simply have to list them using your bitstream's checksum value: $ inaccel bitstream list 2c480f9424d4 PATH /var/lib/inaccel/repository/.../xilinx/aws-vu9p-f1/dynamic-shell/aws/vector/1/1addition DESCRIPTION https://github.com/inaccel/vadd ACCELERATORS 1 x vector.addition (float* a, float* b, float* c, int size) Remote deployment (Optional) - Apart from deploying your bitstream to a local repository, you may wish to have your bitstreams also deployed remotely. Follow the steps below to explore remote repository capabilities: Step 1: Start tracking your remote repository through inaccel config command. For this tutorial, we will enter some dummy credentials where you can fill in your own according to your needs. inaccel config repository --user inaccel --password rocks --url https://demo.inaccel.com/bistreams demo-repo Notice that demo-repo serves as an id for your remote repository. Step 2: Verify successful tracking of your remote repository by inspecting ~/.inaccel/settings.xml file and search for the output presented below: <repository> <id> demo-repo </id> <user> inaccel </user> <password> rocks </password> <url> https://demo.inaccel.com/bitstreams </url> </repository> Step 3: Deploy the bitstream artifact to the demo-repo repository. Again the path given as argument should contain your bitstream binary and specification files. $ inaccel bitstream install --repository demo-repo /remote/path/to/bitstream-directory Step 4: Verify that your bitstream was properly installed to your remote repository by running: $ inaccel bitstream list --repository demo-repo CHECKSUM BITSTREAM ID VERSION KERNEL IDS PLATFORM 2c480f9424d4 vector 1 [addition] aws-vu9p-f1 (xilinx dynamic-shell) Assuming you are presented with a similar output you are ready to start leveraging your accelerators in your applications. Bitstream Resolution - InAccel offers a fully-fledged bitstream repository with various bitstreams available according to whether you are running on Community Edition (CE) or Enterprise Edition (EE) of InAccel Coral. For this tutorial we resolve vadd bitstream which contains the addition kernel needed for our application. inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/vector/1/1addition Finally, you can explore the available bitstreams on our repository, or host your own repository and collaborate with your peers. Discover available accelerators - As an application developer, you would like to explore the available accelerators in your bitstream repository. Since you may have not deployed those bitstreams yourself, simply issue inaccel bitstream list command to get an overview of the deployed bitstreams in your system along with their description and available accelerators: $ inaccel bitstream list CHECKSUM BITSTREAM ID VERSION KERNEL IDS PLATFORM 2c480f9424d4 vector 1 [addition] aws-vu9p-f1 (xilinx dynamic-shell) From the above output, we notice that our repository contains one deployed bitstream with one accelerator suitable for our goals (i.e addition ). Our next step is to inspect the bitstream of interest to list its available accelerators as function prototypes to get a notion of how to invoke them on our application. We simply issue the following command: $ inaccel bitstream list 2c480f9424d4 PATH /var/lib/inaccel/repository/.../xilinx/aws-vu9p-f1/dynamic-shell/aws/vector/1/1addition DESCRIPTION https://github.com/inaccel/vadd ACCELERATORS 1 x vector.addition (float* a, float* b, float* c, int size) That's it! You are now ready to move to Part 4: Accelerate your app , to invoke your accelerators from your applications.","title":"Part 3: Deploy your bitstreams"},{"location":"get-started/part3/#bitstream-development-optional","text":"In this section, we provide the \"Hello World\" accelerator example for bitstream development comprised of a simple vector addition kernel. It receives as input two float arrays and adds their respective elements in an output array. Since the kernel is considered trivial, we will not include the implementation in this tutorial. However you can download the source code from our GitHub repository .","title":"Bitstream Development (Optional)"},{"location":"get-started/part3/#build-instructions-for-aws","text":"Launch an FPGA Developer AMI instance. Install the AWS FPGA development toolkit. git clone https://github.com/aws/aws-fpga.git -b v1.4.22 source aws-fpga/vitis_setup.sh Clone our demo repository and compile the included FPGA kernels. git clone https://github.com/inaccel/vadd.git v++ vadd/src/vadd.cl --platform ${ AWS_PLATFORM } -o vadd.hw.xo v++ --link vadd.hw.xo --platform ${ AWS_PLATFORM } -o vadd.hw.xclbin Warning FPGA binary compilation is a very slow process. You can still skip this part and use our prebuild FPGA binary. Configure AWS CLI with your credentials aws configure Create an Amazon FPGA Image (AFI). Make sure you specify your S3 bucket for the generation of the AFI. ${ VITIS_DIR } /tools/create_vitis_afi.sh -xclbin = vadd.hw.xclbin -o = vadd -s3_bucket = <your-aws-bucket> -s3_dcp_key = demo_dcp -s3_logs_key = demo_logs For more details on the full AWS F1 development lifecycle please refer to this Vitis Quick Start Guide .","title":"Build instructions for AWS"},{"location":"get-started/part3/#bitstream-packaging","text":"We assume that you have already implemented and compiled a simple bitstream comprised of the kernel mentioned above. In case you didn't complete the above steps for AWS, you can still download the prebuilt bitstream binary ( vadd.awsxclbin ) from InAccel Store . At this point, we are ready to move on to the next stage and describe our generated bistream, creating the specification file ( bitstream.json or bitstream.xml ) that will accompany the bitstream artifact.","title":"Bitstream Packaging"},{"location":"get-started/part3/#describe-your-fpga-binary","text":"As mentioned before, each bitstream artifact contains a descriptive JSON (or XML) file that defines all the details required to thoroughly describe a compiled bitstream, like the target platform, the bitstream kernels, the version and other metadata. Below we present a verbose, yet simple bitstream.json file for our bitstream: { \"name\" : \"vadd.awsxclbin\" , \"bitstreamId\" : \"vector\" , \"version\" : \"1\" , \"description\" : \"https://github.com/inaccel/vadd\" , \"platform\" : { \"vendor\" : \"xilinx\" , \"name\" : \"aws-vu9p-f1\" , \"version\" : \"shell-v04261818_201920.2\" , \"label\" : [ \"aws\" ] }, \"kernels\" : [ { \"name\" : [ \"vadd\" ], \"kernelId\" : \"addition\" , \"arguments\" : [ { \"type\" : \"float*\" , \"name\" : \"a\" , \"memory\" : [ \"0\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"b\" , \"memory\" : [ \"0\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"c\" , \"memory\" : [ \"0\" ], \"access\" : \"w\" }, { \"type\" : \"int\" , \"name\" : \"size\" } ] } ] } As you can notice, we designed bitstream specification file to contain fields which are self-descriptive in order to reduce the documentation lookup overhead. However, we will make a brief description for most of the fields with the risk of being redundant, to make you comfortable on creating your own. Let's explore the fields we used in our specification file: name - Bitstream name should match the FPGA binary name bitstreamId - Prefix to be used by applications to invoke kernels in this bitstream version - Bitstream version description - Bitstream description platform vendor - FPGA vendor name - FPGA board name version - FPGA board version Now it's time to declare your bitstream kernels. Those are declared inside the specification file as an array named kernels . Each kernel is represented as an object with the following fields: name - Actual name list of the kernel functions inside the bitstream kernelId - Alias name to invoke the specific kernels in your applications arguments - An array specifying all the kernel arguments type - Data type (a star '*' indicates that the argument is an array) name - Descriptive argument name memory - Memory bank identifier ( only for arrays, not for scalars ) access - Access permissions ( only for arrays, not for scalars ) You are now ready to deploy your first bitstream artifact. For a more detailed overview of the metadata supported by the bitstream specification file check out the detailed Bitstream Reference .","title":"Describe your FPGA binary"},{"location":"get-started/part3/#bitstream-deployment","text":"Once you have created a bitstream artifact by providing a complete bitstream specification file along with your bitstream binary, you can easily deploy it through InAccel's CLI. Refer to Command Line Interface Reference for extensive CLI usage. The Bitstream binary and its specification file must reside in the same directory.","title":"Bitstream Deployment"},{"location":"get-started/part3/#local-deployment","text":"Below, we present a typical workflow of deploying your bitstream artifact to your local repository. When you deploy locally, your bitstreams are stored in your local machine and become available to Coral running on your machine immediately. Follow the steps below to get started: Step 1: Deploy the bitstream artifact to your local repository. The path given as argument should contain both the binary and the specification file. $ inaccel bitstream install /local/path/to/bitstream-directory Step 2: Verify that your bitstream is properly installed to your local repository by issuing the command below. $ inaccel bitstream list CHECKSUM BITSTREAM ID VERSION KERNEL IDS PLATFORM 2c480f9424d4 vector 1 [addition] aws-vu9p-f1 (xilinx dynamic-shell) If the output of the command matches the one presented above then you have successfully installed the demo bitstream artifact to your local repository. After installing the bitstream artifact to your local repository, you can expect that all kernels of the corresponding bitstream will be easily and securely accessible from every application requesting accelerators from Coral. As a bonus, to list the available accelerators of your recently deployed bitstream along with their function prototypes , you simply have to list them using your bitstream's checksum value: $ inaccel bitstream list 2c480f9424d4 PATH /var/lib/inaccel/repository/.../xilinx/aws-vu9p-f1/dynamic-shell/aws/vector/1/1addition DESCRIPTION https://github.com/inaccel/vadd ACCELERATORS 1 x vector.addition (float* a, float* b, float* c, int size)","title":"Local deployment"},{"location":"get-started/part3/#remote-deployment-optional","text":"Apart from deploying your bitstream to a local repository, you may wish to have your bitstreams also deployed remotely. Follow the steps below to explore remote repository capabilities: Step 1: Start tracking your remote repository through inaccel config command. For this tutorial, we will enter some dummy credentials where you can fill in your own according to your needs. inaccel config repository --user inaccel --password rocks --url https://demo.inaccel.com/bistreams demo-repo Notice that demo-repo serves as an id for your remote repository. Step 2: Verify successful tracking of your remote repository by inspecting ~/.inaccel/settings.xml file and search for the output presented below: <repository> <id> demo-repo </id> <user> inaccel </user> <password> rocks </password> <url> https://demo.inaccel.com/bitstreams </url> </repository> Step 3: Deploy the bitstream artifact to the demo-repo repository. Again the path given as argument should contain your bitstream binary and specification files. $ inaccel bitstream install --repository demo-repo /remote/path/to/bitstream-directory Step 4: Verify that your bitstream was properly installed to your remote repository by running: $ inaccel bitstream list --repository demo-repo CHECKSUM BITSTREAM ID VERSION KERNEL IDS PLATFORM 2c480f9424d4 vector 1 [addition] aws-vu9p-f1 (xilinx dynamic-shell) Assuming you are presented with a similar output you are ready to start leveraging your accelerators in your applications.","title":"Remote deployment (Optional)"},{"location":"get-started/part3/#bitstream-resolution","text":"InAccel offers a fully-fledged bitstream repository with various bitstreams available according to whether you are running on Community Edition (CE) or Enterprise Edition (EE) of InAccel Coral. For this tutorial we resolve vadd bitstream which contains the addition kernel needed for our application. inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/vector/1/1addition Finally, you can explore the available bitstreams on our repository, or host your own repository and collaborate with your peers.","title":"Bitstream Resolution"},{"location":"get-started/part3/#discover-available-accelerators","text":"As an application developer, you would like to explore the available accelerators in your bitstream repository. Since you may have not deployed those bitstreams yourself, simply issue inaccel bitstream list command to get an overview of the deployed bitstreams in your system along with their description and available accelerators: $ inaccel bitstream list CHECKSUM BITSTREAM ID VERSION KERNEL IDS PLATFORM 2c480f9424d4 vector 1 [addition] aws-vu9p-f1 (xilinx dynamic-shell) From the above output, we notice that our repository contains one deployed bitstream with one accelerator suitable for our goals (i.e addition ). Our next step is to inspect the bitstream of interest to list its available accelerators as function prototypes to get a notion of how to invoke them on our application. We simply issue the following command: $ inaccel bitstream list 2c480f9424d4 PATH /var/lib/inaccel/repository/.../xilinx/aws-vu9p-f1/dynamic-shell/aws/vector/1/1addition DESCRIPTION https://github.com/inaccel/vadd ACCELERATORS 1 x vector.addition (float* a, float* b, float* c, int size) That's it! You are now ready to move to Part 4: Accelerate your app , to invoke your accelerators from your applications.","title":"Discover available accelerators"},{"location":"get-started/part4/","text":"In this section, we will teach you how to create your own simple application to invoke the accelerator implemented in Part 3 . For the purpose of this tutorial, we provide the instructions to create an application using the C++ and Python APIs, though you can also check the programming interface for Java . Prerequisites - C++ Python Coral API ( coral-api ) package should be installed in your system. Apt Yum apt install coral-api yum install coral-api You can install coral-api package from PyPI using pip for python3. I.e: pip3 install coral-api Introduction - Let's assume we are willing to add the respective elements of two arrays and store their sum in a third array. To accelerate this operation we will invoke the kernel we designed in the previous part of our tutorial. Write your application - Given the above information we can now proceed to write our application code. The source code presented below accomplishes the objectives described in the above section. Even though our API was designed to be simple and intuitive, let's break it up into pieces and analyze them to gain a holistic understanding. C++ Python vadd.cpp #include <algorithm> #include <cassert> #include <inaccel/coral> #include <iostream> int main () { int size = 1024 ; // Allocate vectors inaccel :: vector < float > a ( size ), b ( size ), c ( size ); std :: generate ( a . begin (), a . end (), std :: rand ); std :: generate ( b . begin (), b . end (), std :: rand ); // Send a request for \"addition\" accelerator to the Coral FPGA Resource Manager // Request arguments must comply with the accelerator's specific argument list inaccel :: request vadd ( \"vector.addition\" ); vadd . arg ( a ). arg ( b ). arg ( c ). arg ( size ); inaccel :: submit ( vadd ). get (); for ( int i = 0 ; i < size ; i ++ ) { assert ( a [ i ] + b [ i ] == c [ i ]); } std :: cout << \"Test PASSED\" << std :: endl ; } What is inaccel::vector ? - It is simply a type alias for std::vector with inaccel::allocator . Therefore, both inaccel::vector and std::vector adhere to same usage principles. Whenever you must supply a kernel argument whose type contains a trailing star ('*'), you should use inaccel::vector . What is inaccel::request ? - It represents an accelerator request that will be later submitted to Coral for execution. The name of the request (which is passed as constructor argument) should follow the naming convention explained below. What name should I pass to inaccel::request ? - The name of your request should be identical to the corresponding accelerator's function prototype. Be aware that you must have deployed your bitstream for which you wish to invoke its kernels. If no such valid accelerator exists the request will be dismissed. For the sake of completeness, we present again the portion showing the function prototype of the available accelerator printed by inaccel bitstream list 2c480f9424d4 command: PATH /var/lib/inaccel/repository/.../xilinx/aws-vu9p-f1/dynamic-shell/aws/vector/1/1addition DESCRIPTION https://github.com/inaccel/vadd ACCELERATORS 1 x vector.addition (float* a, float* b, float* c, int size) Hence, to create a request for this kernel we simply have to invoke the inaccel::request constructor with argument vector.addition for addition . In case of invoking an accelerator with multiple versions, the latest is the one to be executed by Coral. What arguments should I pass to inaccel::request ? - The arguments for the request should match the argument list of the target accelerator as specified in its function prototype. Be aware that the ordering of arguments, as in any function, should be preserved. As mentioned above, the types which hold a trailing star represent an inaccel::vector of the specified type. The inaccel::request is populated with arguments through the inaccel::request::arg method. Therefore, based on the prototype of our accelerator, we should populate the request with arguments of the following types: inaccel::vector<float> inaccel::vector<float> inaccel::vector<float> int How to execute an inaccel::request ? - Through the API call inaccel::submit with your request as argument. By invoking that function you transmit your accelerator request to Coral for scheduling and execution. The inaccel::submit executes the accelerator request in an asynchronous manner (i.e doesn't block until completion) and return a std::future. To ensure completion you have wait on that future using get() method. Full C++ API documentation is available here . Compilation - To generate your application's executable you simply have to link against Coral API library. Below you can see an example of compiling vadd.cpp application: g++ vadd.cpp --std = c++17 -lcoral-api -pthread -o vadd Run your application - You run your application exactly the same way as any other application. Open up a terminal and execute the following command: ./vadd vadd.py import numpy as np import inaccel.coral as inaccel size = np . int32 ( 1024 ) with inaccel . allocator : a = np . random . rand ( size ) . astype ( np . float32 ) b = np . random . rand ( size ) . astype ( np . float32 ) c = np . ndarray ( size , dtype = np . float32 ) vadd = inaccel . request ( \"vector.addition\" ) vadd . arg ( a ) . arg ( b ) . arg ( c ) . arg ( size ) inaccel . submit ( vadd ) . result () np . array_equal ( c , a + b ) What is inaccel.allocator ? - inaccel.allocator is a Numpy allocator implementation that serves as a mechanishm to override the memory management strategy used for Numpy array data. What is inaccel.request ? - It represents an accelerator request that will be later submitted to Coral for execution. The name of the request (which is passed as constructor argument) should follow the naming convention explained below. What name should I pass to inaccel.request ? - The name of your request should be identical to the corresponding accelerator's function prototype. Be aware that you must have deployed your bitstream for which you wish to invoke its kernels. If no such valid accelerator exists the request will be dismissed. For the sake of completeness, we present again the portion showing the function prototype of the available accelerator printed by inaccel bitstream list 2c480f9424d4 command: PATH /var/lib/inaccel/repository/.../xilinx/aws-vu9p-f1/dynamic-shell/aws/vector/1/1addition DESCRIPTION https://github.com/inaccel/vadd ACCELERATORS 1 x vector.addition (float* a, float* b, float* c, int size) Hence, to create a request for this kernel we simply have to invoke the inaccel.request constructor with argument vector.addition for addition . In case of invoking an accelerator with multiple versions, the latest is the one to be executed by Coral. What arguments should I pass to inaccel.request ? - The arguments for the request should match the argument list of the target accelerator as specified in its function prototype. Be aware that the ordering of arguments, as in any function, should be preserved. As mentioned above, the types which hold a trailing star represent a numpy array of the specified type allocated with inaccel allocator . inaccel.request is populated with arguments using request's arg method. Therefore, based on the prototype of our accelerator, we should populate the request with arguments of the following types: numpy.ndarray of type float32 numpy.ndarray of type float32 numpy.ndarray of type float32 numpy int32 How to execute an inaccel.request ? - Through the API call inaccel.submit with your request as argument. By invoking that function you transmit your accelerator request to Coral for scheduling and execution. inaccel.submit executes the accelerator request in an asynchronous manner (i.e doesn't block until completion) and returns a Python future. To ensure completion you have to wait on that future using result() method. Full Python API documentation is available here . Run your application - You run your application exactly the same way as any other application. Open up a terminal and execute the following command: python3 vadd.py Conclusion - That's it! After this tutorial we hope you got a grasp of how to deploy bitstreams and easily access their kernels from within your accelerated applications by using Coral. Do not forget to follow the links and navigate through our documentation site to explore more advanced use cases.","title":"Part 4: Accelerate your app"},{"location":"get-started/part4/#prerequisites","text":"C++ Python Coral API ( coral-api ) package should be installed in your system. Apt Yum apt install coral-api yum install coral-api You can install coral-api package from PyPI using pip for python3. I.e: pip3 install coral-api","title":"Prerequisites"},{"location":"get-started/part4/#introduction","text":"Let's assume we are willing to add the respective elements of two arrays and store their sum in a third array. To accelerate this operation we will invoke the kernel we designed in the previous part of our tutorial.","title":"Introduction"},{"location":"get-started/part4/#write-your-application","text":"Given the above information we can now proceed to write our application code. The source code presented below accomplishes the objectives described in the above section. Even though our API was designed to be simple and intuitive, let's break it up into pieces and analyze them to gain a holistic understanding. C++ Python vadd.cpp #include <algorithm> #include <cassert> #include <inaccel/coral> #include <iostream> int main () { int size = 1024 ; // Allocate vectors inaccel :: vector < float > a ( size ), b ( size ), c ( size ); std :: generate ( a . begin (), a . end (), std :: rand ); std :: generate ( b . begin (), b . end (), std :: rand ); // Send a request for \"addition\" accelerator to the Coral FPGA Resource Manager // Request arguments must comply with the accelerator's specific argument list inaccel :: request vadd ( \"vector.addition\" ); vadd . arg ( a ). arg ( b ). arg ( c ). arg ( size ); inaccel :: submit ( vadd ). get (); for ( int i = 0 ; i < size ; i ++ ) { assert ( a [ i ] + b [ i ] == c [ i ]); } std :: cout << \"Test PASSED\" << std :: endl ; }","title":"Write your application"},{"location":"get-started/part4/#what-is-inaccelvector","text":"It is simply a type alias for std::vector with inaccel::allocator . Therefore, both inaccel::vector and std::vector adhere to same usage principles. Whenever you must supply a kernel argument whose type contains a trailing star ('*'), you should use inaccel::vector .","title":"What is inaccel::vector?"},{"location":"get-started/part4/#what-is-inaccelrequest","text":"It represents an accelerator request that will be later submitted to Coral for execution. The name of the request (which is passed as constructor argument) should follow the naming convention explained below.","title":"What is inaccel::request?"},{"location":"get-started/part4/#what-name-should-i-pass-to-inaccelrequest","text":"The name of your request should be identical to the corresponding accelerator's function prototype. Be aware that you must have deployed your bitstream for which you wish to invoke its kernels. If no such valid accelerator exists the request will be dismissed. For the sake of completeness, we present again the portion showing the function prototype of the available accelerator printed by inaccel bitstream list 2c480f9424d4 command: PATH /var/lib/inaccel/repository/.../xilinx/aws-vu9p-f1/dynamic-shell/aws/vector/1/1addition DESCRIPTION https://github.com/inaccel/vadd ACCELERATORS 1 x vector.addition (float* a, float* b, float* c, int size) Hence, to create a request for this kernel we simply have to invoke the inaccel::request constructor with argument vector.addition for addition . In case of invoking an accelerator with multiple versions, the latest is the one to be executed by Coral.","title":"What name should I pass to inaccel::request?"},{"location":"get-started/part4/#what-arguments-should-i-pass-to-inaccelrequest","text":"The arguments for the request should match the argument list of the target accelerator as specified in its function prototype. Be aware that the ordering of arguments, as in any function, should be preserved. As mentioned above, the types which hold a trailing star represent an inaccel::vector of the specified type. The inaccel::request is populated with arguments through the inaccel::request::arg method. Therefore, based on the prototype of our accelerator, we should populate the request with arguments of the following types: inaccel::vector<float> inaccel::vector<float> inaccel::vector<float> int","title":"What arguments should I pass to inaccel::request?"},{"location":"get-started/part4/#how-to-execute-an-inaccelrequest","text":"Through the API call inaccel::submit with your request as argument. By invoking that function you transmit your accelerator request to Coral for scheduling and execution. The inaccel::submit executes the accelerator request in an asynchronous manner (i.e doesn't block until completion) and return a std::future. To ensure completion you have wait on that future using get() method. Full C++ API documentation is available here .","title":"How to execute an inaccel::request?"},{"location":"get-started/part4/#compilation","text":"To generate your application's executable you simply have to link against Coral API library. Below you can see an example of compiling vadd.cpp application: g++ vadd.cpp --std = c++17 -lcoral-api -pthread -o vadd","title":"Compilation"},{"location":"get-started/part4/#run-your-application","text":"You run your application exactly the same way as any other application. Open up a terminal and execute the following command: ./vadd vadd.py import numpy as np import inaccel.coral as inaccel size = np . int32 ( 1024 ) with inaccel . allocator : a = np . random . rand ( size ) . astype ( np . float32 ) b = np . random . rand ( size ) . astype ( np . float32 ) c = np . ndarray ( size , dtype = np . float32 ) vadd = inaccel . request ( \"vector.addition\" ) vadd . arg ( a ) . arg ( b ) . arg ( c ) . arg ( size ) inaccel . submit ( vadd ) . result () np . array_equal ( c , a + b )","title":"Run your application"},{"location":"get-started/part4/#what-is-inaccelallocator","text":"inaccel.allocator is a Numpy allocator implementation that serves as a mechanishm to override the memory management strategy used for Numpy array data.","title":"What is inaccel.allocator?"},{"location":"get-started/part4/#what-is-inaccelrequest_1","text":"It represents an accelerator request that will be later submitted to Coral for execution. The name of the request (which is passed as constructor argument) should follow the naming convention explained below.","title":"What is inaccel.request?"},{"location":"get-started/part4/#what-name-should-i-pass-to-inaccelrequest_1","text":"The name of your request should be identical to the corresponding accelerator's function prototype. Be aware that you must have deployed your bitstream for which you wish to invoke its kernels. If no such valid accelerator exists the request will be dismissed. For the sake of completeness, we present again the portion showing the function prototype of the available accelerator printed by inaccel bitstream list 2c480f9424d4 command: PATH /var/lib/inaccel/repository/.../xilinx/aws-vu9p-f1/dynamic-shell/aws/vector/1/1addition DESCRIPTION https://github.com/inaccel/vadd ACCELERATORS 1 x vector.addition (float* a, float* b, float* c, int size) Hence, to create a request for this kernel we simply have to invoke the inaccel.request constructor with argument vector.addition for addition . In case of invoking an accelerator with multiple versions, the latest is the one to be executed by Coral.","title":"What name should I pass to inaccel.request?"},{"location":"get-started/part4/#what-arguments-should-i-pass-to-inaccelrequest_1","text":"The arguments for the request should match the argument list of the target accelerator as specified in its function prototype. Be aware that the ordering of arguments, as in any function, should be preserved. As mentioned above, the types which hold a trailing star represent a numpy array of the specified type allocated with inaccel allocator . inaccel.request is populated with arguments using request's arg method. Therefore, based on the prototype of our accelerator, we should populate the request with arguments of the following types: numpy.ndarray of type float32 numpy.ndarray of type float32 numpy.ndarray of type float32 numpy int32","title":"What arguments should I pass to inaccel.request?"},{"location":"get-started/part4/#how-to-execute-an-inaccelrequest_1","text":"Through the API call inaccel.submit with your request as argument. By invoking that function you transmit your accelerator request to Coral for scheduling and execution. inaccel.submit executes the accelerator request in an asynchronous manner (i.e doesn't block until completion) and returns a Python future. To ensure completion you have to wait on that future using result() method. Full Python API documentation is available here .","title":"How to execute an inaccel.request?"},{"location":"get-started/part4/#run-your-application_1","text":"You run your application exactly the same way as any other application. Open up a terminal and execute the following command: python3 vadd.py","title":"Run your application"},{"location":"get-started/part4/#conclusion","text":"That's it! After this tutorial we hope you got a grasp of how to deploy bitstreams and easily access their kernels from within your accelerated applications by using Coral. Do not forget to follow the links and navigate through our documentation site to explore more advanced use cases.","title":"Conclusion"},{"location":"glossary/accelerator/","text":"Definition of: accelerator - The phrase \"FPGA accelerator\" usually refers to a specialized FPGA design that performs a specific math function. Accelerators are compiled and linked against a target FPGA platform, resulting to a binary file known as bitstream . InAccel provides a stack of ready to use accelerators for Machine Learning techniques, Financial formulas, SQL functions and Cryptographic algorithms. Glossary terms - To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"accelerator"},{"location":"glossary/accelerator/#definition-of-accelerator","text":"The phrase \"FPGA accelerator\" usually refers to a specialized FPGA design that performs a specific math function. Accelerators are compiled and linked against a target FPGA platform, resulting to a binary file known as bitstream . InAccel provides a stack of ready to use accelerators for Machine Learning techniques, Financial formulas, SQL functions and Cryptographic algorithms.","title":"Definition of: accelerator"},{"location":"glossary/accelerator/#glossary-terms","text":"To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"Glossary terms"},{"location":"glossary/bitstream/","text":"Definition of: bitstream - A bitstream is a binary sequence that contains the programming information for an FPGA . The bitstream is typically provided by the hardware designer who builds the accelerator . A bitstream can be packaged and installed as an artifact, which can be used afterwards by a Coral application. A bitstream artifact can be also deployed to a remote repository, like InAccel Store . Glossary terms - To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"bitstream"},{"location":"glossary/bitstream/#definition-of-bitstream","text":"A bitstream is a binary sequence that contains the programming information for an FPGA . The bitstream is typically provided by the hardware designer who builds the accelerator . A bitstream can be packaged and installed as an artifact, which can be used afterwards by a Coral application. A bitstream artifact can be also deployed to a remote repository, like InAccel Store .","title":"Definition of: bitstream"},{"location":"glossary/bitstream/#glossary-terms","text":"To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"Glossary terms"},{"location":"glossary/container-runtime/","text":"Definition of: container runtime - A container runtime is responsible for all the parts of creating and running a container , configuring namespaces, cgroups, capabilities, and filesystem access controls. It allows you to manage the lifecycle of the container performing pre-start/post-stop operations. Glossary terms - To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"container runtime"},{"location":"glossary/container-runtime/#definition-of-container-runtime","text":"A container runtime is responsible for all the parts of creating and running a container , configuring namespaces, cgroups, capabilities, and filesystem access controls. It allows you to manage the lifecycle of the container performing pre-start/post-stop operations.","title":"Definition of: container runtime"},{"location":"glossary/container-runtime/#glossary-terms","text":"To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"Glossary terms"},{"location":"glossary/container/","text":"Definition of: container - A container is a runtime instance of a Docker image, that wraps up code and all its dependencies so the application runs seamlessly through different environments. A container image is a lightweight collection of software along with execution parameters for use within a container runtime . Glossary terms - To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"container"},{"location":"glossary/container/#definition-of-container","text":"A container is a runtime instance of a Docker image, that wraps up code and all its dependencies so the application runs seamlessly through different environments. A container image is a lightweight collection of software along with execution parameters for use within a container runtime .","title":"Definition of: container"},{"location":"glossary/container/#glossary-terms","text":"To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"Glossary terms"},{"location":"glossary/coral/","text":"Definition of: Coral - InAccel Coral is a distributed acceleration framework designed to orchestrate clusters of FPGAs. It abstracts away the FPGA resources (device, memory), enabling fault-tolerant heterogeneous distributed systems to easily be built and run effectively. The main concepts behind Coral are: Ease of Use Write applications quickly in C++, Java, Scala and Python. Generality Build your own repository of accelerators. Scalability Industry proven to easily scale to unlimited FPGA resources. Runs Anywhere Easy to Deploy Resource Management Automatic resource configuration and task scheduling across entire FPGA clusters in private datacenters or public cloud environments. Privacy / Isolation Coral allows the secure sharing of the hardware resources among different users and multiple processes or threads. Web UI Built-in Web UI for viewing cluster state, navigating in memory objects and monitoring acceleration tasks. Glossary terms - To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"Coral"},{"location":"glossary/coral/#definition-of-coral","text":"InAccel Coral is a distributed acceleration framework designed to orchestrate clusters of FPGAs. It abstracts away the FPGA resources (device, memory), enabling fault-tolerant heterogeneous distributed systems to easily be built and run effectively. The main concepts behind Coral are: Ease of Use Write applications quickly in C++, Java, Scala and Python. Generality Build your own repository of accelerators. Scalability Industry proven to easily scale to unlimited FPGA resources. Runs Anywhere Easy to Deploy Resource Management Automatic resource configuration and task scheduling across entire FPGA clusters in private datacenters or public cloud environments. Privacy / Isolation Coral allows the secure sharing of the hardware resources among different users and multiple processes or threads. Web UI Built-in Web UI for viewing cluster state, navigating in memory objects and monitoring acceleration tasks.","title":"Definition of: Coral"},{"location":"glossary/coral/#glossary-terms","text":"To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"Glossary terms"},{"location":"glossary/device-plugin/","text":"Definition of: device plugin - Kubernetes provides a device plugin framework that you can use to advertise system hardware resources to the Kubelet. Instead of customizing the code for Kubernetes itself, vendors can implement a device plugin that you deploy either manually or as a DaemonSet. The InAccel FPGA plugin for Kubernetes allows you to automatically: Expose the type and number of FPGAs on each node of your cluster Keep track of the health of your FPGAs Run FPGA enabled containers in your Kubernetes cluster Glossary terms - To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"device plugin"},{"location":"glossary/device-plugin/#definition-of-device-plugin","text":"Kubernetes provides a device plugin framework that you can use to advertise system hardware resources to the Kubelet. Instead of customizing the code for Kubernetes itself, vendors can implement a device plugin that you deploy either manually or as a DaemonSet. The InAccel FPGA plugin for Kubernetes allows you to automatically: Expose the type and number of FPGAs on each node of your cluster Keep track of the health of your FPGAs Run FPGA enabled containers in your Kubernetes cluster","title":"Definition of: device plugin"},{"location":"glossary/device-plugin/#glossary-terms","text":"To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"Glossary terms"},{"location":"glossary/fpga/","text":"Definition of: FPGA - Field Programmable Gate Array (FPGA) is semiconductor device that is based around a matrix of configurable logic blocks (CLBs) connected via programmable interconnects. It can be reprogrammed to desired application or functionality requirements after manufacturing. This feature distinguishes FPGAs from Application Specific Integrated Circuits (ASICs), which are custom manufactured for specific design tasks. Due to its programmable nature, FPGA can be an ideal fit for many different markets and applications like Networking, Machine Learning, Data Centers, High Performance Computing and more. Glossary terms - To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"FPGA"},{"location":"glossary/fpga/#definition-of-fpga","text":"Field Programmable Gate Array (FPGA) is semiconductor device that is based around a matrix of configurable logic blocks (CLBs) connected via programmable interconnects. It can be reprogrammed to desired application or functionality requirements after manufacturing. This feature distinguishes FPGAs from Application Specific Integrated Circuits (ASICs), which are custom manufactured for specific design tasks. Due to its programmable nature, FPGA can be an ideal fit for many different markets and applications like Networking, Machine Learning, Data Centers, High Performance Computing and more.","title":"Definition of: FPGA"},{"location":"glossary/fpga/#glossary-terms","text":"To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"Glossary terms"},{"location":"glossary/inaccel-store/","text":"Definition of: InAccel Store - The InAccel Store is a centralized resource for working with InAccel and its components. It provides the following services: Intel / Xilinx bitstream hosting (vendor agnostic) Platform defined project search Full automation with InAccel CLI Glossary terms - To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime coral device plugin FPGA InAccel InAccel Store repository","title":"InAccel Store"},{"location":"glossary/inaccel-store/#definition-of-inaccel-store","text":"The InAccel Store is a centralized resource for working with InAccel and its components. It provides the following services: Intel / Xilinx bitstream hosting (vendor agnostic) Platform defined project search Full automation with InAccel CLI","title":"Definition of: InAccel Store"},{"location":"glossary/inaccel-store/#glossary-terms","text":"To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime coral device plugin FPGA InAccel InAccel Store repository","title":"Glossary terms"},{"location":"glossary/inaccel/","text":"Definition of: InAccel - The term InAccel can refer to The InAccel product as a whole, which is a toolset for developers and sysadmins to develop, ship, and run FPGA accelerated applications The inaccel software package (that includes the CLI to manage the available resources) Glossary terms - To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"InAccel"},{"location":"glossary/inaccel/#definition-of-inaccel","text":"The term InAccel can refer to The InAccel product as a whole, which is a toolset for developers and sysadmins to develop, ship, and run FPGA accelerated applications The inaccel software package (that includes the CLI to manage the available resources)","title":"Definition of: InAccel"},{"location":"glossary/inaccel/#glossary-terms","text":"To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"Glossary terms"},{"location":"glossary/repository/","text":"Definition of: repository - A repository is a set of bitstreams. A repository can be shared by pushing it to a file server. The different bitstreams in the repository can be grouped and versioned efficiently using a simple specification file (in JSON/XML format). Here is InAccel Store , the first public bitstream repository. Glossary terms - To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"repository"},{"location":"glossary/repository/#definition-of-repository","text":"A repository is a set of bitstreams. A repository can be shared by pushing it to a file server. The different bitstreams in the repository can be grouped and versioned efficiently using a simple specification file (in JSON/XML format). Here is InAccel Store , the first public bitstream repository.","title":"Definition of: repository"},{"location":"glossary/repository/#glossary-terms","text":"To see a definition for a term, and all topics in the documentation that have been tagged with that term, click any entry below: accelerator bitstream container container runtime Coral device plugin FPGA InAccel InAccel Store repository","title":"Glossary terms"},{"location":"install/","text":"Install InAccel on Linux - InAccel is available on a variety of Linux platforms. Find your preferred operating system below. Supported platforms - InAccel provides .deb and .rpm packages for the following Linux distributions and architectures: Platform x86_64 / amd64 Amazon Linux CentOS Debian Fedora Red Hat Enterprise Linux Ubuntu Get started - After setting up InAccel, you can learn the basics with Getting started with InAccel .","title":"Overview"},{"location":"install/#install-inaccel-on-linux","text":"InAccel is available on a variety of Linux platforms. Find your preferred operating system below.","title":"Install InAccel on Linux"},{"location":"install/#supported-platforms","text":"InAccel provides .deb and .rpm packages for the following Linux distributions and architectures: Platform x86_64 / amd64 Amazon Linux CentOS Debian Fedora Red Hat Enterprise Linux Ubuntu","title":"Supported platforms"},{"location":"install/#get-started","text":"After setting up InAccel, you can learn the basics with Getting started with InAccel .","title":"Get started"},{"location":"install/deb/","text":"Install InAccel on Debian / Ubuntu - To get started with InAccel on Debian / Ubuntu follow the installation steps. Installation methods - You can install InAccel in different ways, depending on your needs: Most users set up InAccel\u2019s repository and install from it, for ease of installation and upgrade tasks. This is the recommended approach. Some users download the DEB packages and install them manually and manage upgrades completely manually. This is useful in situations such as installing InAccel on air-gapped systems with no access to the internet. In testing and development environments, some users choose to use the automated convenience script to install InAccel. Install using the repository - Before you install InAccel for the first time on a new host machine, you need to set up the InAccel repository. Afterward, you can install and update InAccel from the repository. Set up the repository - Update the apt package index and install packages to allow apt to use a repository over HTTPS: sudo apt-get update sudo apt-get install -y \\ ca-certificates \\ curl \\ gnupg Add InAccel's official GPG key: sudo mkdir -m 0755 -p /etc/apt/keyrings curl -fsSL https://setup.inaccel.com/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/inaccel.gpg Use the following command to set up the repository. echo \\ \"deb [arch=\" $( dpkg --print-architecture ) \" signed-by=/etc/apt/keyrings/inaccel.gpg] https://dl.cloudsmith.io/public/inaccel/stable/deb/ $( . /etc/os-release && echo \" $ID \" ) \\ \" $( . /etc/os-release && echo \" $VERSION_CODENAME \" ) \" main\" | \\ sudo tee /etc/apt/sources.list.d/inaccel.list > /dev/null Install InAccel - Update the apt package index. sudo apt-get update Receiving a GPG error when running apt-get update? Your default umask may be incorrectly configured, preventing detection of the repository public key file. Try granting read permission for the InAccel public key file before updating the package index: sudo chmod a+r /etc/apt/keyrings/inaccel.gpg sudo apt-get update Install the latest version of InAccel, InAccel CLI, InAccel Docker, and InAccel FPGA: sudo apt-get install inaccel inaccel-cli inaccel-docker inaccel-fpga This command installs InAccel, but it doesn\u2019t start InAccel. Start InAccel and configure InAccel to start on boot. sudo systemctl enable --now inaccel Install from a package - If you cannot use InAccel's repository to install InAccel, you can download the .deb files for your release and install it manually. You need to download a new files each time you want to upgrade InAccel. Go to https://cloudsmith.io/~inaccel/repos/stable/groups/?q=format:deb . Download the following deb files for the InAccel, InAccel CLI, InAccel Docker, and InAccel FPGA packages: inaccel_<version>_amd64.deb inaccel-cli_<version>_amd64.deb inaccel-docker_<version>_amd64.deb inaccel-fpga_<version>_amd64.deb Install the .deb packages. Update the path in the following example to where you downloaded the InAccel packages. sudo apt-get install /path/to/package.deb The InAccel daemon doesn't start automatically. Start InAccel and configure InAccel to start on boot. sudo systemctl enable --now inaccel Upgrade InAccel - To upgrade InAccel, download the newer package files and repeat the installation procedure , pointing to the new files. Install using the convenience script - InAccel provides a convenience script at https://setup.inaccel.com/repository to install InAccel into development environments non-interactively. The convenience script isn\u2019t recommended for production environments, but it\u2019s useful for creating a provisioning script tailored to your needs. Also refer to the install using the repository steps to learn about installation steps to install using the package repository. Always examine scripts downloaded from the internet before running them locally. Before installing, make yourself familiar with potential risks and limitations of the convenience script: The script requires root or sudo privileges to run. The script attempts to detect your Linux distribution and version and configure your package management system for you. The script doesn\u2019t allow you to customize most installation parameters. The script installs dependencies and recommendations without asking for confirmation. This may install a large number of packages, depending on the current configuration of your host machine. This example downloads the script from https://setup.inaccel.com/repository and runs it to install the latest stable release of InAccel on Linux: curl -fsSL https://setup.inaccel.com/repository -o setup-inaccel.sh sudo sh setup-inaccel.sh install Upgrade InAccel after using the convenience script - If you installed InAccel using the convenience script, you should upgrade InAccel using your package manager directly. There\u2019s no advantage to re-running the convenience script. Re-running it can cause issues if it attempts to re-install repositories which already exist on the host machine. Uninstall InAccel - Uninstall the InAccel, InAccel CLI, InAccel Docker, and InAccel FPGA packages: sudo apt-get purge inaccel inaccel-cli inaccel-docker inaccel-fpga Bitstreams, or customized configuration files on your host are not automatically removed. To delete all bitstreams: sudo rm -rf /var/lib/inaccel You must delete any edited configuration files manually. Next steps - Review the topics in Develop with InAccel to learn how to build new applications using InAccel.","title":"Debian / Ubuntu"},{"location":"install/deb/#install-inaccel-on-debian-ubuntu","text":"To get started with InAccel on Debian / Ubuntu follow the installation steps.","title":"Install InAccel on Debian / Ubuntu"},{"location":"install/deb/#installation-methods","text":"You can install InAccel in different ways, depending on your needs: Most users set up InAccel\u2019s repository and install from it, for ease of installation and upgrade tasks. This is the recommended approach. Some users download the DEB packages and install them manually and manage upgrades completely manually. This is useful in situations such as installing InAccel on air-gapped systems with no access to the internet. In testing and development environments, some users choose to use the automated convenience script to install InAccel.","title":"Installation methods"},{"location":"install/deb/#install-using-the-repository","text":"Before you install InAccel for the first time on a new host machine, you need to set up the InAccel repository. Afterward, you can install and update InAccel from the repository.","title":"Install using the repository"},{"location":"install/deb/#install-from-a-package","text":"If you cannot use InAccel's repository to install InAccel, you can download the .deb files for your release and install it manually. You need to download a new files each time you want to upgrade InAccel. Go to https://cloudsmith.io/~inaccel/repos/stable/groups/?q=format:deb . Download the following deb files for the InAccel, InAccel CLI, InAccel Docker, and InAccel FPGA packages: inaccel_<version>_amd64.deb inaccel-cli_<version>_amd64.deb inaccel-docker_<version>_amd64.deb inaccel-fpga_<version>_amd64.deb Install the .deb packages. Update the path in the following example to where you downloaded the InAccel packages. sudo apt-get install /path/to/package.deb The InAccel daemon doesn't start automatically. Start InAccel and configure InAccel to start on boot. sudo systemctl enable --now inaccel","title":"Install from a package"},{"location":"install/deb/#install-using-the-convenience-script","text":"InAccel provides a convenience script at https://setup.inaccel.com/repository to install InAccel into development environments non-interactively. The convenience script isn\u2019t recommended for production environments, but it\u2019s useful for creating a provisioning script tailored to your needs. Also refer to the install using the repository steps to learn about installation steps to install using the package repository. Always examine scripts downloaded from the internet before running them locally. Before installing, make yourself familiar with potential risks and limitations of the convenience script: The script requires root or sudo privileges to run. The script attempts to detect your Linux distribution and version and configure your package management system for you. The script doesn\u2019t allow you to customize most installation parameters. The script installs dependencies and recommendations without asking for confirmation. This may install a large number of packages, depending on the current configuration of your host machine. This example downloads the script from https://setup.inaccel.com/repository and runs it to install the latest stable release of InAccel on Linux: curl -fsSL https://setup.inaccel.com/repository -o setup-inaccel.sh sudo sh setup-inaccel.sh install","title":"Install using the convenience script"},{"location":"install/deb/#uninstall-inaccel","text":"Uninstall the InAccel, InAccel CLI, InAccel Docker, and InAccel FPGA packages: sudo apt-get purge inaccel inaccel-cli inaccel-docker inaccel-fpga Bitstreams, or customized configuration files on your host are not automatically removed. To delete all bitstreams: sudo rm -rf /var/lib/inaccel You must delete any edited configuration files manually.","title":"Uninstall InAccel"},{"location":"install/deb/#next-steps","text":"Review the topics in Develop with InAccel to learn how to build new applications using InAccel.","title":"Next steps"},{"location":"install/rpm/","text":"Install InAccel on Amazon Linux / CentOS / Fedora / Red Hat Enterprise Linux - To get started with InAccel on Amazon Linux / CentOS / Fedora / Red Hat Enterprise Linux follow the installation steps. Installation methods - You can install InAccel in different ways, depending on your needs: Most users set up InAccel\u2019s repository and install from it, for ease of installation and upgrade tasks. This is the recommended approach. Some users download the RPM packages and install them manually and manage upgrades completely manually. This is useful in situations such as installing InAccel on air-gapped systems with no access to the internet. In testing and development environments, some users choose to use the automated convenience script to install InAccel. Install using the repository - Before you install InAccel for the first time on a new host machine, you need to set up the InAccel repository. Afterward, you can install and update InAccel from the repository. Set up the repository - Install the yum-utils package (which provides the yum-config-manager utility) and set up the repository. sudo yum install -y yum-utils sudo yum-config-manager \\ --add-repo \\ https://setup.inaccel.com/inaccel.repo Install InAccel - Install the latest version of InAccel, InAccel CLI, InAccel Docker, and InAccel FPGA: sudo yum install inaccel inaccel-cli inaccel-docker inaccel-fpga This command installs InAccel, but it doesn\u2019t start InAccel. Start InAccel and configure InAccel to start on boot. sudo systemctl enable --now inaccel Install from packages - If you cannot use InAccel's repository to install InAccel, you can download the .rpm files for your release and install them manually. You need to download new files each time you want to upgrade InAccel. Go to https://cloudsmith.io/~inaccel/repos/stable/groups/?q=format:rpm . Download the following rpm files for the InAccel, InAccel CLI, InAccel Docker, and InAccel FPGA packages: inaccel-<version>-1.x86_64.rpm inaccel-cli-<version>-1.x86_64.rpm inaccel-docker-<version>-1.x86_64.rpm inaccel-fpga-<version>-1.x86_64.rpm Install the .rpm packages. Update the path in the following example to where you downloaded the InAccel packages. sudo yum install /path/to/package.rpm The InAccel daemon doesn't start automatically. Start InAccel and configure InAccel to start on boot. sudo systemctl enable --now inaccel Upgrade InAccel - To upgrade InAccel, download the newer package files and repeat the installation procedure , pointing to the new files. Install using the convenience script - InAccel provides a convenience script at https://setup.inaccel.com/repository to install InAccel into development environments non-interactively. The convenience script isn\u2019t recommended for production environments, but it\u2019s useful for creating a provisioning script tailored to your needs. Also refer to the install using the repository steps to learn about installation steps to install using the package repository. Always examine scripts downloaded from the internet before running them locally. Before installing, make yourself familiar with potential risks and limitations of the convenience script: The script requires root or sudo privileges to run. The script attempts to detect your Linux distribution and version and configure your package management system for you. The script doesn\u2019t allow you to customize most installation parameters. The script installs dependencies and recommendations without asking for confirmation. This may install a large number of packages, depending on the current configuration of your host machine. This example downloads the script from https://setup.inaccel.com/repository and runs it to install the latest stable release of InAccel on Linux: curl -fsSL https://setup.inaccel.com/repository -o setup-inaccel.sh sudo sh setup-inaccel.sh install Upgrade InAccel after using the convenience script - If you installed InAccel using the convenience script, you should upgrade InAccel using your package manager directly. There\u2019s no advantage to re-running the convenience script. Re-running it can cause issues if it attempts to re-install repositories which already exist on the host machine. Uninstall InAccel - Uninstall the InAccel, InAccel CLI, InAccel Docker, and InAccel FPGA packages: sudo yum remove inaccel inaccel-cli inaccel-docker inaccel-fpga Bitstreams, or customized configuration files on your host are not automatically removed. To delete all bitstreams: sudo rm -rf /var/lib/inaccel You must delete any edited configuration files manually. Next steps - Review the topics in Develop with InAccel to learn how to build new applications using InAccel.","title":"Amazon Linux / CentOS / Fedora / Red Hat Enterprise Linux"},{"location":"install/rpm/#install-inaccel-on-amazon-linux-centos-fedora-red-hat-enterprise-linux","text":"To get started with InAccel on Amazon Linux / CentOS / Fedora / Red Hat Enterprise Linux follow the installation steps.","title":"Install InAccel on Amazon Linux / CentOS / Fedora / Red Hat Enterprise Linux"},{"location":"install/rpm/#installation-methods","text":"You can install InAccel in different ways, depending on your needs: Most users set up InAccel\u2019s repository and install from it, for ease of installation and upgrade tasks. This is the recommended approach. Some users download the RPM packages and install them manually and manage upgrades completely manually. This is useful in situations such as installing InAccel on air-gapped systems with no access to the internet. In testing and development environments, some users choose to use the automated convenience script to install InAccel.","title":"Installation methods"},{"location":"install/rpm/#install-using-the-repository","text":"Before you install InAccel for the first time on a new host machine, you need to set up the InAccel repository. Afterward, you can install and update InAccel from the repository.","title":"Install using the repository"},{"location":"install/rpm/#install-from-packages","text":"If you cannot use InAccel's repository to install InAccel, you can download the .rpm files for your release and install them manually. You need to download new files each time you want to upgrade InAccel. Go to https://cloudsmith.io/~inaccel/repos/stable/groups/?q=format:rpm . Download the following rpm files for the InAccel, InAccel CLI, InAccel Docker, and InAccel FPGA packages: inaccel-<version>-1.x86_64.rpm inaccel-cli-<version>-1.x86_64.rpm inaccel-docker-<version>-1.x86_64.rpm inaccel-fpga-<version>-1.x86_64.rpm Install the .rpm packages. Update the path in the following example to where you downloaded the InAccel packages. sudo yum install /path/to/package.rpm The InAccel daemon doesn't start automatically. Start InAccel and configure InAccel to start on boot. sudo systemctl enable --now inaccel","title":"Install from packages"},{"location":"install/rpm/#install-using-the-convenience-script","text":"InAccel provides a convenience script at https://setup.inaccel.com/repository to install InAccel into development environments non-interactively. The convenience script isn\u2019t recommended for production environments, but it\u2019s useful for creating a provisioning script tailored to your needs. Also refer to the install using the repository steps to learn about installation steps to install using the package repository. Always examine scripts downloaded from the internet before running them locally. Before installing, make yourself familiar with potential risks and limitations of the convenience script: The script requires root or sudo privileges to run. The script attempts to detect your Linux distribution and version and configure your package management system for you. The script doesn\u2019t allow you to customize most installation parameters. The script installs dependencies and recommendations without asking for confirmation. This may install a large number of packages, depending on the current configuration of your host machine. This example downloads the script from https://setup.inaccel.com/repository and runs it to install the latest stable release of InAccel on Linux: curl -fsSL https://setup.inaccel.com/repository -o setup-inaccel.sh sudo sh setup-inaccel.sh install","title":"Install using the convenience script"},{"location":"install/rpm/#uninstall-inaccel","text":"Uninstall the InAccel, InAccel CLI, InAccel Docker, and InAccel FPGA packages: sudo yum remove inaccel inaccel-cli inaccel-docker inaccel-fpga Bitstreams, or customized configuration files on your host are not automatically removed. To delete all bitstreams: sudo rm -rf /var/lib/inaccel You must delete any edited configuration files manually.","title":"Uninstall InAccel"},{"location":"install/rpm/#next-steps","text":"Review the topics in Develop with InAccel to learn how to build new applications using InAccel.","title":"Next steps"},{"location":"labs/auto-scaling-aws/","text":"In this lab we will go through the setup of a hybrid Kubernetes (K8s) cluster with FPGA support. By the term hybrid , we indicate that there is going to be a mix of on-prem and AWS nodes. Of course this approach is way more complex compared to a classic K8s deployment but it has its own benefits too. Today, we introduce a way of automatically expanding your local Kubernetes cluster with AWS EC2 instances, while taking advantage of the performance improvements that FPGA accelerators can offer. To make the overall configuration and deployment as seamless and easy to use as possible, we use kubeadm for initializing the cluster's control-plane and further on joining new nodes, AWS Auto Scaling Groups and AWS Launch Configurations driven by Cluster Autoscaler for the instantiation of Amazon compute resources and InAccel FPGA Operator for the discovery, announcement and management of the available FPGAs, but also for the ease of deployment of containerized hardware accelerated applications, especially in cases where nodes have multiple FPGA cards (e.g f1.4xlarge and f1.16xlarge ) and users want to instantly take advantage of them without code modifications. This tutorial is split in the following 3 sections: Preparation and tools Cluster bring-up Evaluation with FPGA workloads Preparation and tools - In this section we will go through the prerequisites for setting up our cluster. To begin with, a big challenge is to make the nodes in our private network communicate with the nodes residing in the AWS VPCs. For that reason, we need to create a Virtual Private Network (VPN) and connect all the nodes to it. For the purposes of this demonstration we will host an OpenVPN Access Server on the K8s master node, with the following configuration: Network Settings Option Protocol UDP Port number 1194 VPN Settings Option Should clients be able to communicate with each other on the VPN IP Network? Yes, using Routing Allow access from these private subnets to all VPN client IP addresses and subnets Yes Should client Internet traffic be routed through the VPN? No Should clients be allowed to access network services on the VPN gateway IP address? Yes Do not alter clients' DNS server settings Yes Advanced VPN Option Should clients be able to communicate with each other on the VPN IP Network? Yes Below you will also find the full list of the required software packages: Master node: docker , helm , kubeadm , openvpn-as Worker nodes: docker , inaccel , kubeadm , openvpn , xrt Note On every node docker and kubeadm must be installed. On every worker node Xilinx XRT is required accompanied by InAccel runtime (as the default runtime). OpenVPN server and client packages are also needed for the VPN connections on the master and worker nodes, respectively. Important Make sure that swap is turned off on every node ( kubernetes/kubernetes#53533 ). sudo swapoff -a Always enable and start the Docker service before adding a node. sudo systemctl enable docker sudo systemctl start docker For the AWS worker nodes we have prepared a community AMI with all the required dependencies pre-installed. Cluster bring-up - Now that everything is ready, let's move on to the actual cluster bring-up. For all the worker nodes (both on-prem and AWS ones) we have created a bootstrap script that simplifies the process of bootstrapping a new node into our hybrid Kubernetes cluster. Click here to inspect bootstrap.sh script! #!/bin/bash set -o pipefail set -o nounset set -o errexit err_report () { echo \"Exited with error on line $1 \" } trap 'err_report $LINENO' ERR IFS = $'\\n\\t' function print_help { echo \"usage: $0 [options]\" echo \"Bootstraps an instance into an InAccel hybrid Kubernetes cluster\" echo \"\" echo \"-h,--help print this help\" echo \"--apiserver-endpoint The API Server endpoint.\" echo \"--discovery-token-ca-cert-hash For token-based discovery, validate that the root CA public key matches this hash.\" echo \"--labels Labels to add when registering the node in the cluster. Labels must be key=value pairs separated by ','.\" echo \"--openvpn-config Read OpenVPN configuration options from file.\" echo \"--openvpn-login Authenticate with OpenVPN server using username/password.\" echo \"--token Use this token for both discovery-token and tls-bootstrap-token.\" } while [[ $# -gt 0 ]] ; do key = \" $1 \" case $key in -h | --help ) print_help exit 1 ;; --apiserver-endpoint ) API_SERVER_ENDPOINT = \" $2 \" shift shift ;; --discovery-token-ca-cert-hash ) CA_CERT_HASHES = \" $2 \" shift shift ;; --labels ) NODE_LABELS = \" $2 \" shift shift ;; --openvpn-config ) CONFIG = \" $2 \" shift shift ;; --openvpn-login ) AUTH_USER_PASS = \" $2 \" shift shift ;; --token ) TOKEN = \" $2 \" shift shift ;; * ) # unknown option shift # past argument ;; esac done set +u openvpn --config $CONFIG --auth-user-pass $AUTH_USER_PASS & while true ; do export TUNNEL_IP = $( ifconfig tun0 | grep 'inet ' | awk '{print $2}' ) [[ -z $TUNNEL_IP ]] || break sleep 3 done cat > init.conf << EOF apiVersion: kubeadm.k8s.io/v1beta2 discovery: bootstrapToken: apiServerEndpoint: $API_SERVER_ENDPOINT token: $TOKEN caCertHashes: - $CA_CERT_HASHES unsafeSkipCAVerification: true kind: JoinConfiguration nodeRegistration: name: $TUNNEL_IP kubeletExtraArgs: EOF AWS_INSTANCE_TYPE = $( curl -s http://169.254.169.254/latest/meta-data/instance-type ) || AWS_INSTANCE_TYPE = \"none\" if [[ $AWS_INSTANCE_TYPE == \"none\" ]] ; then cat >> init.conf << EOF node-labels: $NODE_LABELS EOF else if [[ ! -z $NODE_LABELS ]] ; then NODE_LABELS += \",\" fi NODE_LABELS += \"node.kubernetes.io/instance-type= $AWS_INSTANCE_TYPE \" cat >> init.conf << EOF node-labels: $NODE_LABELS EOF fi ip route add 10 .96.0.0/16 dev tun0 src $TUNNEL_IP kubeadm join --config init.conf Setup the Master node - Initialize the Kubernetes control-plane. Use the VPN IP, that the OpenVPN Access Server has assigned to that node (e.g 172.27.224.1 ), as the IP address the API Server will advertise it's listening on. sudo kubeadm init \\ --apiserver-advertise-address = 172 .27.224.1 \\ --kubernetes-version stable-1.18 To make helm and kubectl work for your non-root user, use the commands from the kubeadm init output. Deploy Calico network policy engine for Kubernetes. kubectl apply -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml Deploy Cluster Autoscaler for AWS. helm repo add stable https://kubernetes-charts.storage.googleapis.com helm install cluster-autoscaler stable/cluster-autoscaler \\ --set autoDiscovery.clusterName = InAccel \\ --set awsAccessKeyID = <your-aws-access-key-id> \\ --set awsRegion = us-east-1 \\ --set awsSecretAccessKey = <your-aws-secret-access-key> \\ --set cloudProvider = aws Deploy InAccel FPGA Operator . helm repo add inaccel https://setup.inaccel.com/helm helm install inaccel inaccel/fpga-operator \\ --set license = <your-license> \\ --set nodeSelector.inaccel/fpga = enabled Setup the local Worker nodes - For the local nodes, you can simply use the bootstrap.sh script providing all the necessary arguments. For example: ./bootstrap.sh \\ --apiserver-endpoint <your-apiserver-endpoint> \\ --discovery-token-ca-cert-hash <your-discovery-token-ca-cert-hash> \\ --labels inaccel/fpga = enabled \\ --openvpn-config <your-openvpn-config> \\ --openvpn-login <your-openvpn-login> \\ --token <your-token> Note In case that you don't remember your kubeadm tokens, you can always issue new ones using the following command: kubeadm token create --print-join-command --ttl 0 Setup the AWS Worker nodes - For the AWS nodes, we provide an AMI including all the required packages mentioned in the previous section, but also the bootstrap.sh script. Let's now create two new Auto Scaling Groups with specific Launch Configurations in order to be used by the Cluster Autoscaler . The process of creating the above resources is pretty simple if you have already configured the AWS CLI ( aws ). f1.2xlarge f1.4xlarge aws autoscaling create-launch-configuration \\ --image-id ami-02e0c24a82677f084 \\ --instance-type f1.2xlarge \\ --key-name <your-key-name> \\ --launch-configuration-name f1-2xlarge \\ --region us-east-1 \\ --user-data <your-user-data> aws autoscaling create-launch-configuration \\ --image-id ami-02e0c24a82677f084 \\ --instance-type f1.4xlarge \\ --key-name <your-key-name> \\ --launch-configuration-name f1-4xlarge \\ --region us-east-1 \\ --user-data <your-user-data> f1.2xlarge f1.4xlarge aws autoscaling create-auto-scaling-group \\ --auto-scaling-group-name f1-2xlarge \\ --availability-zones us-east-1a us-east-1b us-east-1c \\ --launch-configuration-name f1-2xlarge \\ --max-size <your-max-size> \\ --min-size 0 \\ --tags \\ ResourceId = f1-2xlarge,ResourceType = auto-scaling-group,Key = k8s.io/cluster-autoscaler/enabled,Value = true,PropagateAtLaunch = true \\ ResourceId = f1-2xlarge,ResourceType = auto-scaling-group,Key = k8s.io/cluster-autoscaler/InAccel,Value = owned,PropagateAtLaunch = true \\ ResourceId = f1-2xlarge,ResourceType = auto-scaling-group,Key = k8s.io/cluster-autoscaler/node-template/label/node.kubernetes.io/instance-type,Value = f1.2xlarge,PropagateAtLaunch = true \\ ResourceId = f1-2xlarge,ResourceType = auto-scaling-group,Key = k8s.io/cluster-autoscaler/node-template/resources/xilinx/aws-vu9p-f1,Value = 1 ,PropagateAtLaunch = true aws autoscaling create-auto-scaling-group \\ --auto-scaling-group-name f1-4xlarge \\ --availability-zones us-east-1a us-east-1b us-east-1c \\ --launch-configuration-name f1-4xlarge \\ --max-size <your-max-size> \\ --min-size 0 \\ --tags \\ ResourceId = f1-4xlarge,ResourceType = auto-scaling-group,Key = k8s.io/cluster-autoscaler/enabled,Value = true,PropagateAtLaunch = true \\ ResourceId = f1-4xlarge,ResourceType = auto-scaling-group,Key = k8s.io/cluster-autoscaler/InAccel,Value = owned,PropagateAtLaunch = true \\ ResourceId = f1-4xlarge,ResourceType = auto-scaling-group,Key = k8s.io/cluster-autoscaler/node-template/label/node.kubernetes.io/instance-type,Value = f1.4xlarge,PropagateAtLaunch = true \\ ResourceId = f1-4xlarge,ResourceType = auto-scaling-group,Key = k8s.io/cluster-autoscaler/node-template/resources/xilinx/aws-vu9p-f1,Value = 2 ,PropagateAtLaunch = true Note When creating the launch configuration, in user-data flag you should pass the base64 encoding of the bootstrap command you would like to run upon creating a new EC2 instance, using that launch configuration. For example: echo -n \"/opt/inaccel/bootstrap.sh \\ --apiserver-endpoint <your-apiserver-endpoint> \\ --discovery-token-ca-cert-hash <your-discovery-token-ca-cert-hash> \\ --labels inaccel/fpga=enabled \\ --openvpn-config <your-openvpn-config> \\ --openvpn-login <your-openvpn-login> \\ --token <your-token>\" | base64 Evaluation with FPGA workloads - To evaluate our hybrid setup, we configured a K8s cluster consisted of a single worker node hosting 2 FPGAs of the Xilinx Alveo family (one U250 and one U280). We then deployed 2 jobs that could be used as part of a larger ML experiment to tune the parameters of an XGBoost model, requesting 4 com.inaccel.xgboost.exact accelerators each. Under the hood, InAccel FPGA Operator had already processed our bitstream repository and had extracted the information that the specified accelerator was only available for Xilinx Alveo U250 and AWS Xilinx VU9P FPGA boards. The operator was also aware of the available FPGA resources on every node, but also the ones that could be available by bursting to the Cloud. Submitting the first job, InAccel calculated that the idle Alveo U250 could fit the accelerator requirements and scheduled that job to run on the on-prem node. The second job submission with the same accelerator requirements found the local worker node with insufficient capacity of com.inaccel.xgboost.exact accelerators, since the Alveo U250 was still occupied. The FPGA operator knowing that each xilinx/aws-vu9p-f1 could fit only 2 replicas of the requested accelerator, translated the request to an amount of 2 AWS Xilinx VU9P FPGAs, which led the cluster-autoscaler to trigger a scale-up event at the f1.4xlarge node group. The new node automatically joined the cluster and the job was successfully assigned to it. But let's see it in action:","title":"Hybrid auto-scalable FPGA deployment"},{"location":"labs/auto-scaling-aws/#preparation-and-tools","text":"In this section we will go through the prerequisites for setting up our cluster. To begin with, a big challenge is to make the nodes in our private network communicate with the nodes residing in the AWS VPCs. For that reason, we need to create a Virtual Private Network (VPN) and connect all the nodes to it. For the purposes of this demonstration we will host an OpenVPN Access Server on the K8s master node, with the following configuration: Network Settings Option Protocol UDP Port number 1194 VPN Settings Option Should clients be able to communicate with each other on the VPN IP Network? Yes, using Routing Allow access from these private subnets to all VPN client IP addresses and subnets Yes Should client Internet traffic be routed through the VPN? No Should clients be allowed to access network services on the VPN gateway IP address? Yes Do not alter clients' DNS server settings Yes Advanced VPN Option Should clients be able to communicate with each other on the VPN IP Network? Yes Below you will also find the full list of the required software packages: Master node: docker , helm , kubeadm , openvpn-as Worker nodes: docker , inaccel , kubeadm , openvpn , xrt Note On every node docker and kubeadm must be installed. On every worker node Xilinx XRT is required accompanied by InAccel runtime (as the default runtime). OpenVPN server and client packages are also needed for the VPN connections on the master and worker nodes, respectively. Important Make sure that swap is turned off on every node ( kubernetes/kubernetes#53533 ). sudo swapoff -a Always enable and start the Docker service before adding a node. sudo systemctl enable docker sudo systemctl start docker For the AWS worker nodes we have prepared a community AMI with all the required dependencies pre-installed.","title":"Preparation and tools"},{"location":"labs/auto-scaling-aws/#cluster-bring-up","text":"Now that everything is ready, let's move on to the actual cluster bring-up. For all the worker nodes (both on-prem and AWS ones) we have created a bootstrap script that simplifies the process of bootstrapping a new node into our hybrid Kubernetes cluster. Click here to inspect bootstrap.sh script! #!/bin/bash set -o pipefail set -o nounset set -o errexit err_report () { echo \"Exited with error on line $1 \" } trap 'err_report $LINENO' ERR IFS = $'\\n\\t' function print_help { echo \"usage: $0 [options]\" echo \"Bootstraps an instance into an InAccel hybrid Kubernetes cluster\" echo \"\" echo \"-h,--help print this help\" echo \"--apiserver-endpoint The API Server endpoint.\" echo \"--discovery-token-ca-cert-hash For token-based discovery, validate that the root CA public key matches this hash.\" echo \"--labels Labels to add when registering the node in the cluster. Labels must be key=value pairs separated by ','.\" echo \"--openvpn-config Read OpenVPN configuration options from file.\" echo \"--openvpn-login Authenticate with OpenVPN server using username/password.\" echo \"--token Use this token for both discovery-token and tls-bootstrap-token.\" } while [[ $# -gt 0 ]] ; do key = \" $1 \" case $key in -h | --help ) print_help exit 1 ;; --apiserver-endpoint ) API_SERVER_ENDPOINT = \" $2 \" shift shift ;; --discovery-token-ca-cert-hash ) CA_CERT_HASHES = \" $2 \" shift shift ;; --labels ) NODE_LABELS = \" $2 \" shift shift ;; --openvpn-config ) CONFIG = \" $2 \" shift shift ;; --openvpn-login ) AUTH_USER_PASS = \" $2 \" shift shift ;; --token ) TOKEN = \" $2 \" shift shift ;; * ) # unknown option shift # past argument ;; esac done set +u openvpn --config $CONFIG --auth-user-pass $AUTH_USER_PASS & while true ; do export TUNNEL_IP = $( ifconfig tun0 | grep 'inet ' | awk '{print $2}' ) [[ -z $TUNNEL_IP ]] || break sleep 3 done cat > init.conf << EOF apiVersion: kubeadm.k8s.io/v1beta2 discovery: bootstrapToken: apiServerEndpoint: $API_SERVER_ENDPOINT token: $TOKEN caCertHashes: - $CA_CERT_HASHES unsafeSkipCAVerification: true kind: JoinConfiguration nodeRegistration: name: $TUNNEL_IP kubeletExtraArgs: EOF AWS_INSTANCE_TYPE = $( curl -s http://169.254.169.254/latest/meta-data/instance-type ) || AWS_INSTANCE_TYPE = \"none\" if [[ $AWS_INSTANCE_TYPE == \"none\" ]] ; then cat >> init.conf << EOF node-labels: $NODE_LABELS EOF else if [[ ! -z $NODE_LABELS ]] ; then NODE_LABELS += \",\" fi NODE_LABELS += \"node.kubernetes.io/instance-type= $AWS_INSTANCE_TYPE \" cat >> init.conf << EOF node-labels: $NODE_LABELS EOF fi ip route add 10 .96.0.0/16 dev tun0 src $TUNNEL_IP kubeadm join --config init.conf","title":"Cluster bring-up"},{"location":"labs/auto-scaling-aws/#setup-the-master-node","text":"Initialize the Kubernetes control-plane. Use the VPN IP, that the OpenVPN Access Server has assigned to that node (e.g 172.27.224.1 ), as the IP address the API Server will advertise it's listening on. sudo kubeadm init \\ --apiserver-advertise-address = 172 .27.224.1 \\ --kubernetes-version stable-1.18 To make helm and kubectl work for your non-root user, use the commands from the kubeadm init output. Deploy Calico network policy engine for Kubernetes. kubectl apply -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml Deploy Cluster Autoscaler for AWS. helm repo add stable https://kubernetes-charts.storage.googleapis.com helm install cluster-autoscaler stable/cluster-autoscaler \\ --set autoDiscovery.clusterName = InAccel \\ --set awsAccessKeyID = <your-aws-access-key-id> \\ --set awsRegion = us-east-1 \\ --set awsSecretAccessKey = <your-aws-secret-access-key> \\ --set cloudProvider = aws Deploy InAccel FPGA Operator . helm repo add inaccel https://setup.inaccel.com/helm helm install inaccel inaccel/fpga-operator \\ --set license = <your-license> \\ --set nodeSelector.inaccel/fpga = enabled","title":"Setup the Master node"},{"location":"labs/auto-scaling-aws/#setup-the-local-worker-nodes","text":"For the local nodes, you can simply use the bootstrap.sh script providing all the necessary arguments. For example: ./bootstrap.sh \\ --apiserver-endpoint <your-apiserver-endpoint> \\ --discovery-token-ca-cert-hash <your-discovery-token-ca-cert-hash> \\ --labels inaccel/fpga = enabled \\ --openvpn-config <your-openvpn-config> \\ --openvpn-login <your-openvpn-login> \\ --token <your-token> Note In case that you don't remember your kubeadm tokens, you can always issue new ones using the following command: kubeadm token create --print-join-command --ttl 0","title":"Setup the local Worker nodes"},{"location":"labs/auto-scaling-aws/#setup-the-aws-worker-nodes","text":"For the AWS nodes, we provide an AMI including all the required packages mentioned in the previous section, but also the bootstrap.sh script. Let's now create two new Auto Scaling Groups with specific Launch Configurations in order to be used by the Cluster Autoscaler . The process of creating the above resources is pretty simple if you have already configured the AWS CLI ( aws ). f1.2xlarge f1.4xlarge aws autoscaling create-launch-configuration \\ --image-id ami-02e0c24a82677f084 \\ --instance-type f1.2xlarge \\ --key-name <your-key-name> \\ --launch-configuration-name f1-2xlarge \\ --region us-east-1 \\ --user-data <your-user-data> aws autoscaling create-launch-configuration \\ --image-id ami-02e0c24a82677f084 \\ --instance-type f1.4xlarge \\ --key-name <your-key-name> \\ --launch-configuration-name f1-4xlarge \\ --region us-east-1 \\ --user-data <your-user-data> f1.2xlarge f1.4xlarge aws autoscaling create-auto-scaling-group \\ --auto-scaling-group-name f1-2xlarge \\ --availability-zones us-east-1a us-east-1b us-east-1c \\ --launch-configuration-name f1-2xlarge \\ --max-size <your-max-size> \\ --min-size 0 \\ --tags \\ ResourceId = f1-2xlarge,ResourceType = auto-scaling-group,Key = k8s.io/cluster-autoscaler/enabled,Value = true,PropagateAtLaunch = true \\ ResourceId = f1-2xlarge,ResourceType = auto-scaling-group,Key = k8s.io/cluster-autoscaler/InAccel,Value = owned,PropagateAtLaunch = true \\ ResourceId = f1-2xlarge,ResourceType = auto-scaling-group,Key = k8s.io/cluster-autoscaler/node-template/label/node.kubernetes.io/instance-type,Value = f1.2xlarge,PropagateAtLaunch = true \\ ResourceId = f1-2xlarge,ResourceType = auto-scaling-group,Key = k8s.io/cluster-autoscaler/node-template/resources/xilinx/aws-vu9p-f1,Value = 1 ,PropagateAtLaunch = true aws autoscaling create-auto-scaling-group \\ --auto-scaling-group-name f1-4xlarge \\ --availability-zones us-east-1a us-east-1b us-east-1c \\ --launch-configuration-name f1-4xlarge \\ --max-size <your-max-size> \\ --min-size 0 \\ --tags \\ ResourceId = f1-4xlarge,ResourceType = auto-scaling-group,Key = k8s.io/cluster-autoscaler/enabled,Value = true,PropagateAtLaunch = true \\ ResourceId = f1-4xlarge,ResourceType = auto-scaling-group,Key = k8s.io/cluster-autoscaler/InAccel,Value = owned,PropagateAtLaunch = true \\ ResourceId = f1-4xlarge,ResourceType = auto-scaling-group,Key = k8s.io/cluster-autoscaler/node-template/label/node.kubernetes.io/instance-type,Value = f1.4xlarge,PropagateAtLaunch = true \\ ResourceId = f1-4xlarge,ResourceType = auto-scaling-group,Key = k8s.io/cluster-autoscaler/node-template/resources/xilinx/aws-vu9p-f1,Value = 2 ,PropagateAtLaunch = true Note When creating the launch configuration, in user-data flag you should pass the base64 encoding of the bootstrap command you would like to run upon creating a new EC2 instance, using that launch configuration. For example: echo -n \"/opt/inaccel/bootstrap.sh \\ --apiserver-endpoint <your-apiserver-endpoint> \\ --discovery-token-ca-cert-hash <your-discovery-token-ca-cert-hash> \\ --labels inaccel/fpga=enabled \\ --openvpn-config <your-openvpn-config> \\ --openvpn-login <your-openvpn-login> \\ --token <your-token>\" | base64","title":"Setup the AWS Worker nodes"},{"location":"labs/auto-scaling-aws/#evaluation-with-fpga-workloads","text":"To evaluate our hybrid setup, we configured a K8s cluster consisted of a single worker node hosting 2 FPGAs of the Xilinx Alveo family (one U250 and one U280). We then deployed 2 jobs that could be used as part of a larger ML experiment to tune the parameters of an XGBoost model, requesting 4 com.inaccel.xgboost.exact accelerators each. Under the hood, InAccel FPGA Operator had already processed our bitstream repository and had extracted the information that the specified accelerator was only available for Xilinx Alveo U250 and AWS Xilinx VU9P FPGA boards. The operator was also aware of the available FPGA resources on every node, but also the ones that could be available by bursting to the Cloud. Submitting the first job, InAccel calculated that the idle Alveo U250 could fit the accelerator requirements and scheduled that job to run on the on-prem node. The second job submission with the same accelerator requirements found the local worker node with insufficient capacity of com.inaccel.xgboost.exact accelerators, since the Alveo U250 was still occupied. The FPGA operator knowing that each xilinx/aws-vu9p-f1 could fit only 2 replicas of the requested accelerator, translated the request to an amount of 2 AWS Xilinx VU9P FPGAs, which led the cluster-autoscaler to trigger a scale-up event at the f1.4xlarge node group. The new node automatically joined the cluster and the job was successfully assigned to it. But let's see it in action:","title":"Evaluation with FPGA workloads"},{"location":"labs/binderhub-aws/","text":"Introduction - In this article we are going to present the first ever FPGA accelerated execution of Jupyter Notebooks over BinderHub. BinderHub enables end users to easily create computing environments from Git repos, making it easier than ever to run applications without the need of installing packages, setting up environments etc. It then serves the custom computing environment at a URL which users can access remotely. To achieve this functionality BinderHub uses a JupyterHub running on Kubernetes. However, this awesome concept misses the ability of running applications in an accelerated environment. And this is where InAccel comes in. InAccel, a world-leader in application acceleration through the use of adaptive acceleration platforms (ACAP, FPGA) provides an FPGA operator that allows instant deployment, scaling and virtualization of FPGAs making the utilization of FPGA clusters easier than ever. InAccel's FPGA Kubernetes device plugin enables users to accelerate their Pods within the snap of a finger. That said it really makes sense to combine all that together to provide seamless acceleration for any computational intensive workload. Supposedly having already deployed a Kubernetes cluster over a bunch of servers hosting FPGAs and having setup BinderHub you only need to deploy InAccel FPGA Operator to enable FPGA accelerated notebooks: Deploy InAccel FPGA Operator: helm repo add inaccel https://setup.inaccel.com/helm helm repo update helm install my-fpga-operator inaccel/fpga-operator Hint If you want to run the enterprise edition of Coral or for example to specify the monitor port, you can do so by setting the corresponding values at the step of installing InAccel FPGA Operator. You can find a list of all the available parameters here . Example: helm install my-fpga-operator inaccel/fpga-operator --set license = <your-license> --set monitor.port = <your-monitor-port> You can now run any FPGA accelerated application simply by specifying a Git repo in your BinderHub endpoint. A Use Case Scenario using Amazon EKS - Amazon offers its so called EKS service for creating Kubernetes clusters. What is more, Amazon's F1 (FPGA) instances are supported by EKS meaning that a user can easily create a Kubernetes cluster hosting F1 instances, to accelerate applications using the power of FPGAs and this is why we are using it in this tutorial. At this point we are going to guide you through the whole procedure of creating a Kubernetes cluster in Amazon AWS using EKS service and how to further on deploy BinderHub, InAccel FPGA Operator and run an FPGA accelerated application. Before beginning make sure you have the required access/premissions to perform the actions below using your AWS account. For this guide we used an account with the following policies attached: IAMFullAccess AmazonVPCFullAccess AmazonEKSAdminPolicy AWSCloudFormationFullAccess AmazonEC2FullAccess Hint You can change a user's permissions by selecting the IAM service from Amazon AWS Console. Then from Users panel select your user and click on Add permissions . Select Attach existing policies directly and attach the desired policies Install python3-pip . Apt Yum sudo apt install -y python3-pip sudo yum install -y python3-pip Download and install eksctl . curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_ $( uname -s ) _amd64.tar.gz\" | tar xz -C /tmp sudo mv /tmp/eksctl /usr/bin Install and configure awscli . sudo pip3 install --upgrade awscli aws configure Download and install kubectl . sudo curl -o /usr/bin/kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.22.6/2022-03-09/bin/linux/amd64/kubectl sudo chmod +x /usr/bin/kubectl Setup helm3 : curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh Create a Kubernetes Cluster in AWS: At this ponint we create a Kubernetes cluster in AWS named binderhub that will by default spawn two worker nodes with the eks default values. eksctl create cluster \\ --asg-access \\ --name binderhub \\ --nodegroup-name default \\ --region us-east-1 \\ --version 1 .22 \\ --zones us-east-1a,us-east-1b,us-east-1c Create an auto-scaling group of F1 instances : We set the minimum number of this nodegroup's nodes to zero (0) since we don't want to be charged for the FPGA instances unless a pod is requesting FPGA resources. In such case, an FPGA (F1) node will be automatically spawned and provisioned to handle this acceleration request. To enable this functionality we also have to deploy cluster autoscaler (next step). eksctl create nodegroup \\ --asg-access \\ --cluster binderhub \\ --managed = false \\ --name f1-2xlarge \\ --node-labels inaccel/fpga = enabled \\ --node-type f1.2xlarge \\ --nodes 0 \\ --nodes-max 3 \\ --nodes-min 0 \\ --tags k8s.io/cluster-autoscaler/enabled = true \\ --tags k8s.io/cluster-autoscaler/binderhub = owned \\ --tags k8s.io/cluster-autoscaler/node-template/label/node.kubernetes.io/instance-type = f1.2xlarge \\ --tags k8s.io/cluster-autoscaler/node-template/resources/xilinx/aws-vu9p-f1 = 1 Deploy Kubernetes cluser-autoscaler: We make sure that the cluster autoscaler points to the correct Kubernetes cluster by specifying the autoDiscovery.clusterName and awsRegion properties. helm repo add cluster-autoscaler https://kubernetes.github.io/autoscaler helm repo update helm install cluster-autoscaler cluster-autoscaler/cluster-autoscaler \\ --set autoDiscovery.clusterName = binderhub \\ --set awsRegion = us-east-1 Prepare the binderhub deployment: To deploy Binderhub we have to first create a yaml file that will host all the required configurations. For example, we specify that the pods to be spawned by BinderHub, should be labeled as inaccel/fpga: \"enabled\" and should request exactly one xilinx/aws-vu9p-f1 FPGA resource. Make sure you replace docker-id , organization-name , prefix and password with your own values. config.yaml config : BinderHub : use_registry : true image_prefix : <docker-id OR organization-name>/<prefix>- dind : enabled : true jupyterhub : singleuser : profileList : - display_name : \"FPGA Server\" description : \"Spawns a notebook server with access to an FPGA\" kubespawner_override : extra_labels : inaccel/fpga : \"enabled\" extra_resource_limits : xilinx/aws-vu9p-f1 : \"1\" registry : username : <docker-id> password : <password> Hint Dockerhub registry is used in this example. If you want to connect BinderHub to a different docker registry please consider BinderHub documentation Install BinderHub. You can modify the --version argument as you see fit. You can find a list of all the available versions here . helm repo add jupyterhub https://jupyterhub.github.io/helm-chart helm repo update helm install binder jupyterhub/binderhub --version = 0 .2.0-n905.h3d3e24e --namespace = binder -f config.yaml --create-namespace Connect BinderHub and JupyterHub. kubectl --namespace = binder get svc proxy-public | awk 'NR>1 {print $4}' Hint If the above command returns <pending> just wait a few moments and execute it again. Copy the output of the above command and edit config.yaml file adding the following: config.yaml config : BinderHub : hub_url : http://<output of the above command> The whole config.yaml file should look like this: config.yaml config : BinderHub : use_registry : true image_prefix : <docker-id OR organization-name>/<prefix>- hub_url : http://<output-of-the-above-command> dind : enabled : true jupyterhub : singleuser : profileList : - display_name : \"FPGA Server\" description : \"Spawns a notebook server with access to an FPGA\" kubespawner_override : extra_labels : inaccel/fpga : \"enabled\" extra_resource_limits : xilinx/aws-vu9p-f1 : \"1\" registry : username : <docker-id> password : <password> Update Binder beployment. If you chose a different version when installing BinderHub make sure you set the same one here: helm upgrade binder jupyterhub/binderhub --version = 0 .2.0-n905.h3d3e24e --namespace = binder -f config.yaml Deploy InAccel FPGA Operator: helm repo add inaccel https://setup.inaccel.com/helm helm repo update helm install inaccel-fpga-operator inaccel/fpga-operator You are all set! Get BinderHub endpoint and paste it in a web browser. kubectl --namespace = binder get svc binder Hint Wait a couple of minutes for the DNS record returned in the previous step to be registered and to become fully functional. Run Xilinx Vitis example applications - To promote the benefits of Xilinx's Vitis accelerators, we have created several jupyter notebooks that invoke the accelerators and can be instantly spawned using BinderHub. We have modified the Vitis software libraries to use our framework's API and have implemented the corresponding notebooks. In the BinderHub endpoint you setup previously, paste the following URL and then hit Launch : https://github.com/inaccel/Vitis_Notebooks You should then see a docker image being built and after a while you should be reirected to a fully working Jupyter Notebook environment. Run any of the notebooks available or create your own.","title":"Zero to FPGAs using BinderHub"},{"location":"labs/binderhub-aws/#introduction","text":"In this article we are going to present the first ever FPGA accelerated execution of Jupyter Notebooks over BinderHub. BinderHub enables end users to easily create computing environments from Git repos, making it easier than ever to run applications without the need of installing packages, setting up environments etc. It then serves the custom computing environment at a URL which users can access remotely. To achieve this functionality BinderHub uses a JupyterHub running on Kubernetes. However, this awesome concept misses the ability of running applications in an accelerated environment. And this is where InAccel comes in. InAccel, a world-leader in application acceleration through the use of adaptive acceleration platforms (ACAP, FPGA) provides an FPGA operator that allows instant deployment, scaling and virtualization of FPGAs making the utilization of FPGA clusters easier than ever. InAccel's FPGA Kubernetes device plugin enables users to accelerate their Pods within the snap of a finger. That said it really makes sense to combine all that together to provide seamless acceleration for any computational intensive workload. Supposedly having already deployed a Kubernetes cluster over a bunch of servers hosting FPGAs and having setup BinderHub you only need to deploy InAccel FPGA Operator to enable FPGA accelerated notebooks: Deploy InAccel FPGA Operator: helm repo add inaccel https://setup.inaccel.com/helm helm repo update helm install my-fpga-operator inaccel/fpga-operator Hint If you want to run the enterprise edition of Coral or for example to specify the monitor port, you can do so by setting the corresponding values at the step of installing InAccel FPGA Operator. You can find a list of all the available parameters here . Example: helm install my-fpga-operator inaccel/fpga-operator --set license = <your-license> --set monitor.port = <your-monitor-port> You can now run any FPGA accelerated application simply by specifying a Git repo in your BinderHub endpoint.","title":"Introduction"},{"location":"labs/binderhub-aws/#a-use-case-scenario-using-amazon-eks","text":"Amazon offers its so called EKS service for creating Kubernetes clusters. What is more, Amazon's F1 (FPGA) instances are supported by EKS meaning that a user can easily create a Kubernetes cluster hosting F1 instances, to accelerate applications using the power of FPGAs and this is why we are using it in this tutorial. At this point we are going to guide you through the whole procedure of creating a Kubernetes cluster in Amazon AWS using EKS service and how to further on deploy BinderHub, InAccel FPGA Operator and run an FPGA accelerated application. Before beginning make sure you have the required access/premissions to perform the actions below using your AWS account. For this guide we used an account with the following policies attached: IAMFullAccess AmazonVPCFullAccess AmazonEKSAdminPolicy AWSCloudFormationFullAccess AmazonEC2FullAccess Hint You can change a user's permissions by selecting the IAM service from Amazon AWS Console. Then from Users panel select your user and click on Add permissions . Select Attach existing policies directly and attach the desired policies Install python3-pip . Apt Yum sudo apt install -y python3-pip sudo yum install -y python3-pip Download and install eksctl . curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_ $( uname -s ) _amd64.tar.gz\" | tar xz -C /tmp sudo mv /tmp/eksctl /usr/bin Install and configure awscli . sudo pip3 install --upgrade awscli aws configure Download and install kubectl . sudo curl -o /usr/bin/kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.22.6/2022-03-09/bin/linux/amd64/kubectl sudo chmod +x /usr/bin/kubectl Setup helm3 : curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh Create a Kubernetes Cluster in AWS: At this ponint we create a Kubernetes cluster in AWS named binderhub that will by default spawn two worker nodes with the eks default values. eksctl create cluster \\ --asg-access \\ --name binderhub \\ --nodegroup-name default \\ --region us-east-1 \\ --version 1 .22 \\ --zones us-east-1a,us-east-1b,us-east-1c Create an auto-scaling group of F1 instances : We set the minimum number of this nodegroup's nodes to zero (0) since we don't want to be charged for the FPGA instances unless a pod is requesting FPGA resources. In such case, an FPGA (F1) node will be automatically spawned and provisioned to handle this acceleration request. To enable this functionality we also have to deploy cluster autoscaler (next step). eksctl create nodegroup \\ --asg-access \\ --cluster binderhub \\ --managed = false \\ --name f1-2xlarge \\ --node-labels inaccel/fpga = enabled \\ --node-type f1.2xlarge \\ --nodes 0 \\ --nodes-max 3 \\ --nodes-min 0 \\ --tags k8s.io/cluster-autoscaler/enabled = true \\ --tags k8s.io/cluster-autoscaler/binderhub = owned \\ --tags k8s.io/cluster-autoscaler/node-template/label/node.kubernetes.io/instance-type = f1.2xlarge \\ --tags k8s.io/cluster-autoscaler/node-template/resources/xilinx/aws-vu9p-f1 = 1 Deploy Kubernetes cluser-autoscaler: We make sure that the cluster autoscaler points to the correct Kubernetes cluster by specifying the autoDiscovery.clusterName and awsRegion properties. helm repo add cluster-autoscaler https://kubernetes.github.io/autoscaler helm repo update helm install cluster-autoscaler cluster-autoscaler/cluster-autoscaler \\ --set autoDiscovery.clusterName = binderhub \\ --set awsRegion = us-east-1 Prepare the binderhub deployment: To deploy Binderhub we have to first create a yaml file that will host all the required configurations. For example, we specify that the pods to be spawned by BinderHub, should be labeled as inaccel/fpga: \"enabled\" and should request exactly one xilinx/aws-vu9p-f1 FPGA resource. Make sure you replace docker-id , organization-name , prefix and password with your own values. config.yaml config : BinderHub : use_registry : true image_prefix : <docker-id OR organization-name>/<prefix>- dind : enabled : true jupyterhub : singleuser : profileList : - display_name : \"FPGA Server\" description : \"Spawns a notebook server with access to an FPGA\" kubespawner_override : extra_labels : inaccel/fpga : \"enabled\" extra_resource_limits : xilinx/aws-vu9p-f1 : \"1\" registry : username : <docker-id> password : <password> Hint Dockerhub registry is used in this example. If you want to connect BinderHub to a different docker registry please consider BinderHub documentation Install BinderHub. You can modify the --version argument as you see fit. You can find a list of all the available versions here . helm repo add jupyterhub https://jupyterhub.github.io/helm-chart helm repo update helm install binder jupyterhub/binderhub --version = 0 .2.0-n905.h3d3e24e --namespace = binder -f config.yaml --create-namespace Connect BinderHub and JupyterHub. kubectl --namespace = binder get svc proxy-public | awk 'NR>1 {print $4}' Hint If the above command returns <pending> just wait a few moments and execute it again. Copy the output of the above command and edit config.yaml file adding the following: config.yaml config : BinderHub : hub_url : http://<output of the above command> The whole config.yaml file should look like this: config.yaml config : BinderHub : use_registry : true image_prefix : <docker-id OR organization-name>/<prefix>- hub_url : http://<output-of-the-above-command> dind : enabled : true jupyterhub : singleuser : profileList : - display_name : \"FPGA Server\" description : \"Spawns a notebook server with access to an FPGA\" kubespawner_override : extra_labels : inaccel/fpga : \"enabled\" extra_resource_limits : xilinx/aws-vu9p-f1 : \"1\" registry : username : <docker-id> password : <password> Update Binder beployment. If you chose a different version when installing BinderHub make sure you set the same one here: helm upgrade binder jupyterhub/binderhub --version = 0 .2.0-n905.h3d3e24e --namespace = binder -f config.yaml Deploy InAccel FPGA Operator: helm repo add inaccel https://setup.inaccel.com/helm helm repo update helm install inaccel-fpga-operator inaccel/fpga-operator You are all set! Get BinderHub endpoint and paste it in a web browser. kubectl --namespace = binder get svc binder Hint Wait a couple of minutes for the DNS record returned in the previous step to be registered and to become fully functional.","title":"A Use Case Scenario using Amazon EKS"},{"location":"labs/binderhub-aws/#run-xilinx-vitis-example-applications","text":"To promote the benefits of Xilinx's Vitis accelerators, we have created several jupyter notebooks that invoke the accelerators and can be instantly spawned using BinderHub. We have modified the Vitis software libraries to use our framework's API and have implemented the corresponding notebooks. In the BinderHub endpoint you setup previously, paste the following URL and then hit Launch : https://github.com/inaccel/Vitis_Notebooks You should then see a docker image being built and after a while you should be reirected to a fully working Jupyter Notebook environment. Run any of the notebooks available or create your own.","title":"Run Xilinx Vitis example applications"},{"location":"labs/bitstream-repository/","text":"In this lab we will demonstrate how you can easily setup your own private Bitstream Repository based on JFrog Artifactory and served via HTTPS. For the installation we will use Artifactory OSS Docker image that can be pulled from Bintray and run as a Docker container. To do this, you need to have Docker client properly installed and configured on your machine. For details about installing and using Docker, please refer to the Docker documentation . If you already have admin access to an existing JFrog installation, feel free to skip the first 2 steps of this tutorial and learn how to Setup a new Bitstream repository . 1. NGINX proxy + Let's Encrypt - If you are planning to expose this repository both inside and outside your organisation, you may want to guarantee stable and secure access from anywhere. For this purpose, we recommend using a reverse proxy server based on NGINX accompanied by automated creation, renewal and use of Let's Encrypt (SSL/TLS) certificates. Warning Certificates will only be issued for domains that correctly resolve to the host, provided the host is publicly reachable on both port 80 and 443 . NGINX proxy - To enable the NGINX reverse proxy simply run the following Docker command, with the specified configuration. inaccel.conf : # Disable any limits to avoid HTTP 413 for large bitstream uploads client_max_body_size 0 ; The above parameter, instructs NGINX proxy to accept POST requests of any size. Depending on the target platform, FPGA binaries can be significantly large, which makes this configuration crucial in most of the cases. docker run --detach \\ --name nginx-proxy \\ --publish 80 :80 \\ --publish 443 :443 \\ --volume /etc/nginx/certs \\ --volume /etc/nginx/vhost.d \\ --volume /usr/share/nginx/html \\ --volume /var/run/docker.sock:/tmp/docker.sock:ro \\ --volume ${ PWD } /inaccel.conf:/etc/nginx/conf.d/inaccel.conf:ro \\ jwilder/nginx-proxy Let's Encrypt - Then we will start the container responsible for the automated generation of SSL/TLS certificates, that will accompany NGINX proxy. docker run --detach \\ --name nginx-proxy-letsencrypt \\ --volumes-from nginx-proxy \\ --volume /var/run/docker.sock:/var/run/docker.sock:ro \\ jrcs/letsencrypt-nginx-proxy-companion 2. JFrog Artifactory - Since the Artifactory instance running in a Docker container is mutable, all data and configuration files will be lost once the container is removed. If you want your data to persist (for example when upgrading to a new version), you need to store them on an external volume mounted to the Docker container. The example below creates a Docker named volume called artifactory_data and mounts it to the Artifactory container under /var/opt/jfrog/artifactory . By default, the named volume is a local directory under /var/lib/docker/volumes/<name> , but can be set to work with other locations. For more details, please refer to the Docker documentation on how to use volumes . docker volume create --name artifactory_data Use now the following Docker command to run JFrog artifactory, shielded by NGINX proxy - Let's Encrypt companion. If you skipped the first step, just make sure that you publish port 8081 to make it available from the host directly (e.g. --publish 80:8081 ). NGINX proxy + Let's Encrypt Standalone docker run --detach \\ --name artifactory \\ --env EXTRA_JAVA_OPTIONS = '-server -Xms512m -Xmx4g -Xss256k -XX:+UseG1GC' \\ --env \"LETSENCRYPT_HOST= ${ YOUR_DOMAIN } \" \\ --env \"LETSENCRYPT_EMAIL= ${ EMAIL } \" \\ --env \"VIRTUAL_HOST= ${ YOUR_DOMAIN } \" \\ --env \"VIRTUAL_PORT=8081\" \\ --expose 8081 \\ --restart always \\ --ulimit nproc = 65535 \\ --ulimit nofile = 32000 :40000 \\ --volume artifactory_data:/var/opt/jfrog/artifactory \\ docker.bintray.io/jfrog/artifactory-oss:latest docker run --detach \\ --name artifactory \\ --env EXTRA_JAVA_OPTIONS = '-server -Xms512m -Xmx4g -Xss256k -XX:+UseG1GC' \\ --publish 80 :8081 \\ --restart always \\ --ulimit nproc = 65535 \\ --ulimit nofile = 32000 :40000 \\ --volume artifactory_data:/var/opt/jfrog/artifactory \\ docker.bintray.io/jfrog/artifactory-oss:latest Once the Artifactory container is up and running, you access Artifactory in the usual way by browsing to https://${YOUR_DOMAIN}/artifactory or http://${SERVER_IP}/artifactory respectively. Artifactory has a default user with admin privileges predefined in the system: User: admin Password: password We strongly recommend changing the admin password as soon as installation is complete, ideally during the welcome wizard set up. 3. Setup a new Bitstream repository - Assuming that you now have a JFrog artifactory installation and a user with admin privileges we can move on the repository creation. Since InAccel packaging format is not yet part of the official JFrog integrations, we will base our Bitstream repository on the Generic package type. Generic repositories have no particular type, and allow you to upload packages of any type in any layout. To create a new Local repository , in the Admin module, go to Repositories | Local and click New to display the New Local Repository screen. Make sure that your repository configuration is similar to the one in the screenshot below, save and finish! Aggregate local and remote repositories (optional) - JFrog gives you the capability to proxy and cache any Remote repository , but also aggregate all these (local and remote) resources under a single Virtual repository . For example, to extend your local Bitstream repository with the accelerators at InAccel Store , you will first have to include it in your Remote Repositories. To do so, create a new repository of that type and use https://store.inaccel.com/artifactory/bitstreams as the remote Artifactory server URL. Then add a new virtual repository ( bitstreams ) that will gather resources both from your local Bitstream repo and InAccel Store. Warning If you wish to use the same URL both for resolving artifacts from your virtual repository and deploying artifacts to your local repository, select bitstreams-local as the Default Deployment Repository in the above screen. 4. Deploy and Resolve Bistream artifacts - In this final step we will show you how to use InAccel CLI to deploy your bitstreams from your development machines directly to your new Bitstream repository and then resolve them in your runtime environment where your Coral FPGA resource manager instance runs. For detailed installation instructions, of InAccel command line, click here . Configure You can persist the details of a Bitstream repository in your local InAccel settings: NGINX proxy + Let's Encrypt Standalone inaccel config repository \\ --user = admin \\ --password = ${ ENCRYPTED_PASSWORD } \\ --url = https:// ${ YOUR_DOMAIN } /artifactory/bitstreams \\ tutorial inaccel config repository \\ --user = admin \\ --password = ${ ENCRYPTED_PASSWORD } \\ --url = http:// ${ SERVER_IP } /artifactory/bitstreams \\ tutorial Deploy You can upload any Bitstream artifact using the following command: inaccel bitstream install --repository = tutorial ${ PATH_TO_DIRECTORY } Resolve You can download a Bitstream artifact directly using the following command: NGINX proxy + Let's Encrypt Standalone inaccel bitstream install https:// ${ YOUR_DOMAIN } /artifactory/bitstreams/ ${ TARGET_DIRECTORY_PATH } inaccel bitstream install http:// ${ SERVER_IP } /artifactory/bitstreams/ ${ TARGET_DIRECTORY_PATH } Info In our Getting Started guide Part 3 you have the chance to walk through the complete Bitstream lifecycle (development, packaging, deployment and resolution).","title":"Rolling your own Bitstream Repository"},{"location":"labs/bitstream-repository/#1-nginx-proxy-lets-encrypt","text":"If you are planning to expose this repository both inside and outside your organisation, you may want to guarantee stable and secure access from anywhere. For this purpose, we recommend using a reverse proxy server based on NGINX accompanied by automated creation, renewal and use of Let's Encrypt (SSL/TLS) certificates. Warning Certificates will only be issued for domains that correctly resolve to the host, provided the host is publicly reachable on both port 80 and 443 .","title":"1. NGINX proxy + Let's Encrypt"},{"location":"labs/bitstream-repository/#nginx-proxy","text":"To enable the NGINX reverse proxy simply run the following Docker command, with the specified configuration. inaccel.conf : # Disable any limits to avoid HTTP 413 for large bitstream uploads client_max_body_size 0 ; The above parameter, instructs NGINX proxy to accept POST requests of any size. Depending on the target platform, FPGA binaries can be significantly large, which makes this configuration crucial in most of the cases. docker run --detach \\ --name nginx-proxy \\ --publish 80 :80 \\ --publish 443 :443 \\ --volume /etc/nginx/certs \\ --volume /etc/nginx/vhost.d \\ --volume /usr/share/nginx/html \\ --volume /var/run/docker.sock:/tmp/docker.sock:ro \\ --volume ${ PWD } /inaccel.conf:/etc/nginx/conf.d/inaccel.conf:ro \\ jwilder/nginx-proxy","title":"NGINX proxy"},{"location":"labs/bitstream-repository/#lets-encrypt","text":"Then we will start the container responsible for the automated generation of SSL/TLS certificates, that will accompany NGINX proxy. docker run --detach \\ --name nginx-proxy-letsencrypt \\ --volumes-from nginx-proxy \\ --volume /var/run/docker.sock:/var/run/docker.sock:ro \\ jrcs/letsencrypt-nginx-proxy-companion","title":"Let's Encrypt"},{"location":"labs/bitstream-repository/#2-jfrog-artifactory","text":"Since the Artifactory instance running in a Docker container is mutable, all data and configuration files will be lost once the container is removed. If you want your data to persist (for example when upgrading to a new version), you need to store them on an external volume mounted to the Docker container. The example below creates a Docker named volume called artifactory_data and mounts it to the Artifactory container under /var/opt/jfrog/artifactory . By default, the named volume is a local directory under /var/lib/docker/volumes/<name> , but can be set to work with other locations. For more details, please refer to the Docker documentation on how to use volumes . docker volume create --name artifactory_data Use now the following Docker command to run JFrog artifactory, shielded by NGINX proxy - Let's Encrypt companion. If you skipped the first step, just make sure that you publish port 8081 to make it available from the host directly (e.g. --publish 80:8081 ). NGINX proxy + Let's Encrypt Standalone docker run --detach \\ --name artifactory \\ --env EXTRA_JAVA_OPTIONS = '-server -Xms512m -Xmx4g -Xss256k -XX:+UseG1GC' \\ --env \"LETSENCRYPT_HOST= ${ YOUR_DOMAIN } \" \\ --env \"LETSENCRYPT_EMAIL= ${ EMAIL } \" \\ --env \"VIRTUAL_HOST= ${ YOUR_DOMAIN } \" \\ --env \"VIRTUAL_PORT=8081\" \\ --expose 8081 \\ --restart always \\ --ulimit nproc = 65535 \\ --ulimit nofile = 32000 :40000 \\ --volume artifactory_data:/var/opt/jfrog/artifactory \\ docker.bintray.io/jfrog/artifactory-oss:latest docker run --detach \\ --name artifactory \\ --env EXTRA_JAVA_OPTIONS = '-server -Xms512m -Xmx4g -Xss256k -XX:+UseG1GC' \\ --publish 80 :8081 \\ --restart always \\ --ulimit nproc = 65535 \\ --ulimit nofile = 32000 :40000 \\ --volume artifactory_data:/var/opt/jfrog/artifactory \\ docker.bintray.io/jfrog/artifactory-oss:latest Once the Artifactory container is up and running, you access Artifactory in the usual way by browsing to https://${YOUR_DOMAIN}/artifactory or http://${SERVER_IP}/artifactory respectively. Artifactory has a default user with admin privileges predefined in the system: User: admin Password: password We strongly recommend changing the admin password as soon as installation is complete, ideally during the welcome wizard set up.","title":"2. JFrog Artifactory"},{"location":"labs/bitstream-repository/#3-setup-a-new-bitstream-repository","text":"Assuming that you now have a JFrog artifactory installation and a user with admin privileges we can move on the repository creation. Since InAccel packaging format is not yet part of the official JFrog integrations, we will base our Bitstream repository on the Generic package type. Generic repositories have no particular type, and allow you to upload packages of any type in any layout. To create a new Local repository , in the Admin module, go to Repositories | Local and click New to display the New Local Repository screen. Make sure that your repository configuration is similar to the one in the screenshot below, save and finish!","title":"3. Setup a new Bitstream repository"},{"location":"labs/bitstream-repository/#aggregate-local-and-remote-repositories-optional","text":"JFrog gives you the capability to proxy and cache any Remote repository , but also aggregate all these (local and remote) resources under a single Virtual repository . For example, to extend your local Bitstream repository with the accelerators at InAccel Store , you will first have to include it in your Remote Repositories. To do so, create a new repository of that type and use https://store.inaccel.com/artifactory/bitstreams as the remote Artifactory server URL. Then add a new virtual repository ( bitstreams ) that will gather resources both from your local Bitstream repo and InAccel Store. Warning If you wish to use the same URL both for resolving artifacts from your virtual repository and deploying artifacts to your local repository, select bitstreams-local as the Default Deployment Repository in the above screen.","title":"Aggregate local and remote repositories (optional)"},{"location":"labs/bitstream-repository/#4-deploy-and-resolve-bistream-artifacts","text":"In this final step we will show you how to use InAccel CLI to deploy your bitstreams from your development machines directly to your new Bitstream repository and then resolve them in your runtime environment where your Coral FPGA resource manager instance runs. For detailed installation instructions, of InAccel command line, click here . Configure You can persist the details of a Bitstream repository in your local InAccel settings: NGINX proxy + Let's Encrypt Standalone inaccel config repository \\ --user = admin \\ --password = ${ ENCRYPTED_PASSWORD } \\ --url = https:// ${ YOUR_DOMAIN } /artifactory/bitstreams \\ tutorial inaccel config repository \\ --user = admin \\ --password = ${ ENCRYPTED_PASSWORD } \\ --url = http:// ${ SERVER_IP } /artifactory/bitstreams \\ tutorial Deploy You can upload any Bitstream artifact using the following command: inaccel bitstream install --repository = tutorial ${ PATH_TO_DIRECTORY } Resolve You can download a Bitstream artifact directly using the following command: NGINX proxy + Let's Encrypt Standalone inaccel bitstream install https:// ${ YOUR_DOMAIN } /artifactory/bitstreams/ ${ TARGET_DIRECTORY_PATH } inaccel bitstream install http:// ${ SERVER_IP } /artifactory/bitstreams/ ${ TARGET_DIRECTORY_PATH } Info In our Getting Started guide Part 3 you have the chance to walk through the complete Bitstream lifecycle (development, packaging, deployment and resolution).","title":"4. Deploy and Resolve Bistream artifacts"},{"location":"labs/h2o/","text":"The purpose of this tutorial is to demonstrate the easiness of accelerating a Machine Learning application using FPGAs on H2O-3 . A user that is comfortable with the H2O framework and features can continue using it seamlessly without knowing how the FPGA accelerated libraries are integrated in the familiar API. Introduction - H2O is an open-source , in-memory, distributed, fast, and scalable machine learning & predictive analytics platform that allows you to build ML models on big data and provides easy productionalization of those models in an enterprise environment. H2O\u2019s core code is written in Java. Inside H2O, a distributed Key/Value store is used to access and reference data, models, objects, etc., across all nodes and machines. H2O\u2019s REST API allows access to all the capabilities of H2O from an external program or script via JSON over HTTP. The Rest API is used by H2O\u2019s web interface (Flow UI), R binding (H2O-R), and Python binding (H2O-Python). InAccel's integration with H2O is the first effort for an FPGA backend on this distributed framework, while the existing solutions for fast and efficient machine learning models include mainly GPUs, like H2O4GPU. Requirements - Operating System : Ubuntu 12.04 or later RHEL/CentOS 6 or later Programming Language : No matter which language will be used for development (Python in our case), Java 7 or later is needed to run H2O and can be installed from the Java Downloads page. Other : A Web browser is required to use H2O\u2019s web UI, Flow. Installation Steps - This section will walk you through the installation of InAccel + H2O. 1. Get InAccel - InAccel is an open platform for developing, shipping, and running accelerated applications. InAccel enables you to separate your applications from your accelerators so you can deliver high-performance software quickly. You can setup InAccel toolset on any Linux platform and enable FPGA accelerator orchestration with a free license issued by InAccel, using this link . 2. Install InAccel - H2O Python package - pip install --extra-index-url https://test.pypi.org/simple inaccel-h2o API walkthrough - Now, we are going to explore the capabilities of the H2O framework with a simple machine learning example inside a common Jupyter Notebook. First of all, we need to fetch the available accelerators from InAccel Store . AWS-VU9P-F1 U250 ! inaccel bitstream install https : // store . inaccel . com / artifactory / bitstreams / xilinx / aws - vu9p - f1 / dynamic - shell / aws / com / inaccel / xgboost / 0.1 / 2 exact ! inaccel bitstream install https : // store . inaccel . com / artifactory / bitstreams / xilinx / u250 / xdma_201830 .2 / com / inaccel / xgboost / 0.2 / 4 exact Import the h2o package, the algorithm class ( H2OXGBoostEstimator ) and initialize a new single node cluster. import h2o from h2o.estimators import H2OXGBoostEstimator h2o . init () Let's load and prepare the data using the H2O library utilities. The training set consists of 65k images while the rest 35k are used for validation purposes. You will find more information about this dataset (SVHN) in the previous Tutorial Lab . images = h2o . import_file ( 'svhn.gz' ) train , valid = images . split_frame ( ratios = [ 0.65 ]) response = 'class' train [ response ] = train [ response ] . asfactor () valid [ response ] = valid [ response ] . asfactor () predictors = images . columns [: - 1 ] After the data preparation stage, we can build a new XGboost model defining a set of learning parameters. You will notice that the only addition to the native XGBoost parameter list is the tree_method , which is now modified for FPGA-accelerated execution without extra code changes. Changing the tree_method param from exact to fpga_exact and retraining the model, you will achieve more than 5x speed-up compared to an 8-threaded Intel Xeon CPU execution, without compromising the classification or regression outcome. Find more about InAccel XGBoost project here . params = { 'col_sample_rate_per_tree' : 0.9 , 'learn_rate' : 0.2 , 'max_depth' : 15 , 'ntrees' : 10 , 'tree_method' : 'fpga_exact' } model = H2OXGBoostEstimator ( ** params ) model . train ( x = predictors , y = response , training_frame = train , validation_frame = valid ) The model can be saved as a MOJO object in order to be easily loaded for use in production. model . download_mojo ( path = 'savedmodel.zip' ) You may also want to access Flow Web UI which is usually running at http://localhost:54321 or in a different port that is announced during cluster initialization. There you can explore all model metrics, learning parameters and training/validation properties. You can also import directly the saved model in the MOJO format and generate predictions for new frames of images.","title":"Build H2O models with the FPGA backend"},{"location":"labs/h2o/#introduction","text":"H2O is an open-source , in-memory, distributed, fast, and scalable machine learning & predictive analytics platform that allows you to build ML models on big data and provides easy productionalization of those models in an enterprise environment. H2O\u2019s core code is written in Java. Inside H2O, a distributed Key/Value store is used to access and reference data, models, objects, etc., across all nodes and machines. H2O\u2019s REST API allows access to all the capabilities of H2O from an external program or script via JSON over HTTP. The Rest API is used by H2O\u2019s web interface (Flow UI), R binding (H2O-R), and Python binding (H2O-Python). InAccel's integration with H2O is the first effort for an FPGA backend on this distributed framework, while the existing solutions for fast and efficient machine learning models include mainly GPUs, like H2O4GPU.","title":"Introduction"},{"location":"labs/h2o/#requirements","text":"Operating System : Ubuntu 12.04 or later RHEL/CentOS 6 or later Programming Language : No matter which language will be used for development (Python in our case), Java 7 or later is needed to run H2O and can be installed from the Java Downloads page. Other : A Web browser is required to use H2O\u2019s web UI, Flow.","title":"Requirements"},{"location":"labs/h2o/#installation-steps","text":"This section will walk you through the installation of InAccel + H2O.","title":"Installation Steps"},{"location":"labs/h2o/#1-get-inaccel","text":"InAccel is an open platform for developing, shipping, and running accelerated applications. InAccel enables you to separate your applications from your accelerators so you can deliver high-performance software quickly. You can setup InAccel toolset on any Linux platform and enable FPGA accelerator orchestration with a free license issued by InAccel, using this link .","title":"1. Get InAccel"},{"location":"labs/h2o/#2-install-inaccel-h2o-python-package","text":"pip install --extra-index-url https://test.pypi.org/simple inaccel-h2o","title":"2. Install InAccel - H2O Python package"},{"location":"labs/h2o/#api-walkthrough","text":"Now, we are going to explore the capabilities of the H2O framework with a simple machine learning example inside a common Jupyter Notebook. First of all, we need to fetch the available accelerators from InAccel Store . AWS-VU9P-F1 U250 ! inaccel bitstream install https : // store . inaccel . com / artifactory / bitstreams / xilinx / aws - vu9p - f1 / dynamic - shell / aws / com / inaccel / xgboost / 0.1 / 2 exact ! inaccel bitstream install https : // store . inaccel . com / artifactory / bitstreams / xilinx / u250 / xdma_201830 .2 / com / inaccel / xgboost / 0.2 / 4 exact Import the h2o package, the algorithm class ( H2OXGBoostEstimator ) and initialize a new single node cluster. import h2o from h2o.estimators import H2OXGBoostEstimator h2o . init () Let's load and prepare the data using the H2O library utilities. The training set consists of 65k images while the rest 35k are used for validation purposes. You will find more information about this dataset (SVHN) in the previous Tutorial Lab . images = h2o . import_file ( 'svhn.gz' ) train , valid = images . split_frame ( ratios = [ 0.65 ]) response = 'class' train [ response ] = train [ response ] . asfactor () valid [ response ] = valid [ response ] . asfactor () predictors = images . columns [: - 1 ] After the data preparation stage, we can build a new XGboost model defining a set of learning parameters. You will notice that the only addition to the native XGBoost parameter list is the tree_method , which is now modified for FPGA-accelerated execution without extra code changes. Changing the tree_method param from exact to fpga_exact and retraining the model, you will achieve more than 5x speed-up compared to an 8-threaded Intel Xeon CPU execution, without compromising the classification or regression outcome. Find more about InAccel XGBoost project here . params = { 'col_sample_rate_per_tree' : 0.9 , 'learn_rate' : 0.2 , 'max_depth' : 15 , 'ntrees' : 10 , 'tree_method' : 'fpga_exact' } model = H2OXGBoostEstimator ( ** params ) model . train ( x = predictors , y = response , training_frame = train , validation_frame = valid ) The model can be saved as a MOJO object in order to be easily loaded for use in production. model . download_mojo ( path = 'savedmodel.zip' ) You may also want to access Flow Web UI which is usually running at http://localhost:54321 or in a different port that is announced during cluster initialization. There you can explore all model metrics, learning parameters and training/validation properties. You can also import directly the saved model in the MOJO format and generate predictions for new frames of images.","title":"API walkthrough"},{"location":"labs/openshift-aws/","text":"Introduction - In this tutorial we are going to present the necessary steps in order to run FPGA-accelerator aware Jupyter Notebooks over OpenShift. Red Hat OpenShift is a Kubernetes distribution focused on developer experience and application security. OpenShift helps you develop and deploy applications to one or more hosts. These can be public facing web applications, or backend applications, including micro services or databases. Build your Red Hat OpenShift environment on AWS - The deployment process includes these steps: Sign up for a Red Hat subscription . If you don't already have an AWS account, sign up at https://aws.amazon.com . Get an InAccel license key , if it's the first time that you use InAccel toolset. Launch the Quick Start . Each deployment takes about 1.5 hours. Specify stack details. Please make sure that the following parameters are set correctly: Parameter Description Allowed External Access CIDR The CIDR IP range that is permitted to access the instances SSH Key Name The name of an existing public/private key pair OpenShift UI Password Password for OpenShift Admin UI Red Hat Subscription Information Redhat RHN User Name, Password and PoolID InAccel Information InAccel License Key When you\u2019re done, click Next . Hint If you don\u2019t have easy access to the Red Hat Subscription Manager, you can launch a RHEL instance on AWS to determine whether your account includes the necessary subscription and associated pool ID. Run the following on the instance to access your account and get a list of your available subscriptions. The output may include a number of sections, use the pool ID of the section that includes something like Red Hat OpenShift Enterprise and has available entitlements. sudo subscription-manager register sudo subscription-manager list --available --all sudo subscription-manager unregister Specify tags (key-value pairs) to apply to resources in your stack. On the Review page, review and confirm the template as well as the stack details. In Capabilities section, tick the boxes to acknowledge that the template might create IAM resources. Finally click the Create stack button to deploy the CloudFormation stack. Monitor the status of the stack. When the status becomes CREATE_COMPLETE , the OpenShift Container Platform cluster is ready. Use the URLs displayed in the Outputs tab for the stack to view the resources that were created. If you need more details on how to customize further your deployment, check the full AWS Deployment Guide . Gain insight into OpenShift Container Platform + InAccel stack Using InAccel OpenShift template the EC2 nodes of the cluster with available FPGAs ( f1.2xlarge , f1.4xlarge , f1.16xlarge ) are automatically: Pre-configured with the required FPGA drivers Upgraded to the latest stable version of the Docker engine Set up with the InAccel toolset (using the related Ansible role ) Finally InAccel FPGA Operator is deployed to the underlying Kubernetes cluster (using the related Helm chart ). Adding Support for FPGA-enabled Jupyter Notebooks - To make it easy to deploy Jupyter Notebooks from the OpenShift web console, the first thing we are going to do is to load in an image stream definition. This is a definition which tells OpenShift where an existing Docker-formatted image can be found for running an InAccel Jupyter Notebook instance. The image stream definition also specifies metadata which helps OpenShift categorise any images. Using that information OpenShift can then add InAccel Jupyter App as a choice in the catalog of applications available for installation from the web console. To load the image stream definition from the web console, you can select Import YAML/JSON and copy & paste the definition from the following snippet into the related field and click Create . Click here to view the full template! apiVersion : v1 kind : Template message : 'The Jupyter Notebook is running at: .../?token=${APPLICATION_TOKEN}' metadata : annotations : description : Turn a Git repo into a collection of interactive notebooks iconClass : icon-python openshift.io/display-name : InAccel Jupyter App tags : inaccel,jupyter,python,builder name : inaccel-jupyter-app parameters : - displayName : Application Name name : APPLICATION_NAME required : true - displayName : Application Token from : '[a-z0-9]{48}' generate : expression name : APPLICATION_TOKEN required : false - displayName : Git repository URL name : GIT_URI required : true - displayName : Git branch or tag name : GIT_REF required : true value : master - displayName : Path to a notebook directory (optional) name : CONTEXT_DIR required : false - displayName : FPGA Platform name : FPGA_LIMIT_KEY required : true - displayName : Number of FPGAs name : FPGA_LIMIT_VALUE required : true objects : - apiVersion : v1 kind : BuildConfig metadata : labels : app : ${APPLICATION_NAME} name : ${APPLICATION_NAME} spec : output : to : kind : ImageStreamTag name : ${APPLICATION_NAME}:latest source : contextDir : ${CONTEXT_DIR} git : ref : ${GIT_REF} uri : ${GIT_URI} type : Git strategy : sourceStrategy : from : kind : DockerImage name : inaccel/jupyter:lab scripts : 'https://raw.githubusercontent.com/jupyter/docker-stacks/master/examples/source-to-image' type : Source triggers : - type : ConfigChange - apiVersion : v1 kind : DeploymentConfig metadata : labels : app : ${APPLICATION_NAME} name : ${APPLICATION_NAME} spec : replicas : 1 selector : app : ${APPLICATION_NAME} deploymentconfig : ${APPLICATION_NAME} strategy : type : Recreate triggers : - type : ConfigChange - type : ImageChange imageChangeParams : automatic : true containerNames : - jupyter-notebook from : kind : ImageStreamTag name : ${APPLICATION_NAME}:latest template : metadata : annotations : alpha.image.policy.openshift.io/resolve-names : '*' labels : app : ${APPLICATION_NAME} deploymentconfig : ${APPLICATION_NAME} spec : automountServiceAccountToken : false containers : - command : - start-notebook.sh - --NotebookApp.token=${APPLICATION_TOKEN} image : ${APPLICATION_NAME}:latest name : jupyter-notebook ports : - containerPort : 8888 protocol : TCP resources : limits : ${FPGA_LIMIT_KEY} : ${FPGA_LIMIT_VALUE} securityContext : supplementalGroups : - 100 - apiVersion : v1 kind : ImageStream metadata : labels : app : ${APPLICATION_NAME} name : ${APPLICATION_NAME} - apiVersion : v1 kind : Route metadata : labels : app : ${APPLICATION_NAME} name : ${APPLICATION_NAME} spec : port : targetPort : 8888-tcp tls : insecureEdgeTerminationPolicy : Redirect termination : edge to : kind : Service name : ${APPLICATION_NAME} - apiVersion : v1 kind : Service metadata : labels : app : ${APPLICATION_NAME} name : ${APPLICATION_NAME} spec : ports : - name : 8888-tcp port : 8888 protocol : TCP targetPort : 8888 selector : app : ${APPLICATION_NAME} deploymentconfig : ${APPLICATION_NAME} type : ClusterIP Having loaded the image stream definitions, InAccel Jupyter App will now be able to be selected from the catalog of applications that can be installed from the UI. For a complete walkthrough on how to deploy a new FPGA-accelerated Jupyter application check the following step-by-step tutorial:","title":"Using OpenShift for accelerated Data Analytics"},{"location":"labs/openshift-aws/#introduction","text":"In this tutorial we are going to present the necessary steps in order to run FPGA-accelerator aware Jupyter Notebooks over OpenShift. Red Hat OpenShift is a Kubernetes distribution focused on developer experience and application security. OpenShift helps you develop and deploy applications to one or more hosts. These can be public facing web applications, or backend applications, including micro services or databases.","title":"Introduction"},{"location":"labs/openshift-aws/#build-your-red-hat-openshift-environment-on-aws","text":"The deployment process includes these steps: Sign up for a Red Hat subscription . If you don't already have an AWS account, sign up at https://aws.amazon.com . Get an InAccel license key , if it's the first time that you use InAccel toolset. Launch the Quick Start . Each deployment takes about 1.5 hours. Specify stack details. Please make sure that the following parameters are set correctly: Parameter Description Allowed External Access CIDR The CIDR IP range that is permitted to access the instances SSH Key Name The name of an existing public/private key pair OpenShift UI Password Password for OpenShift Admin UI Red Hat Subscription Information Redhat RHN User Name, Password and PoolID InAccel Information InAccel License Key When you\u2019re done, click Next . Hint If you don\u2019t have easy access to the Red Hat Subscription Manager, you can launch a RHEL instance on AWS to determine whether your account includes the necessary subscription and associated pool ID. Run the following on the instance to access your account and get a list of your available subscriptions. The output may include a number of sections, use the pool ID of the section that includes something like Red Hat OpenShift Enterprise and has available entitlements. sudo subscription-manager register sudo subscription-manager list --available --all sudo subscription-manager unregister Specify tags (key-value pairs) to apply to resources in your stack. On the Review page, review and confirm the template as well as the stack details. In Capabilities section, tick the boxes to acknowledge that the template might create IAM resources. Finally click the Create stack button to deploy the CloudFormation stack. Monitor the status of the stack. When the status becomes CREATE_COMPLETE , the OpenShift Container Platform cluster is ready. Use the URLs displayed in the Outputs tab for the stack to view the resources that were created. If you need more details on how to customize further your deployment, check the full AWS Deployment Guide . Gain insight into OpenShift Container Platform + InAccel stack Using InAccel OpenShift template the EC2 nodes of the cluster with available FPGAs ( f1.2xlarge , f1.4xlarge , f1.16xlarge ) are automatically: Pre-configured with the required FPGA drivers Upgraded to the latest stable version of the Docker engine Set up with the InAccel toolset (using the related Ansible role ) Finally InAccel FPGA Operator is deployed to the underlying Kubernetes cluster (using the related Helm chart ).","title":"Build your Red Hat OpenShift environment on AWS"},{"location":"labs/openshift-aws/#adding-support-for-fpga-enabled-jupyter-notebooks","text":"To make it easy to deploy Jupyter Notebooks from the OpenShift web console, the first thing we are going to do is to load in an image stream definition. This is a definition which tells OpenShift where an existing Docker-formatted image can be found for running an InAccel Jupyter Notebook instance. The image stream definition also specifies metadata which helps OpenShift categorise any images. Using that information OpenShift can then add InAccel Jupyter App as a choice in the catalog of applications available for installation from the web console. To load the image stream definition from the web console, you can select Import YAML/JSON and copy & paste the definition from the following snippet into the related field and click Create . Click here to view the full template! apiVersion : v1 kind : Template message : 'The Jupyter Notebook is running at: .../?token=${APPLICATION_TOKEN}' metadata : annotations : description : Turn a Git repo into a collection of interactive notebooks iconClass : icon-python openshift.io/display-name : InAccel Jupyter App tags : inaccel,jupyter,python,builder name : inaccel-jupyter-app parameters : - displayName : Application Name name : APPLICATION_NAME required : true - displayName : Application Token from : '[a-z0-9]{48}' generate : expression name : APPLICATION_TOKEN required : false - displayName : Git repository URL name : GIT_URI required : true - displayName : Git branch or tag name : GIT_REF required : true value : master - displayName : Path to a notebook directory (optional) name : CONTEXT_DIR required : false - displayName : FPGA Platform name : FPGA_LIMIT_KEY required : true - displayName : Number of FPGAs name : FPGA_LIMIT_VALUE required : true objects : - apiVersion : v1 kind : BuildConfig metadata : labels : app : ${APPLICATION_NAME} name : ${APPLICATION_NAME} spec : output : to : kind : ImageStreamTag name : ${APPLICATION_NAME}:latest source : contextDir : ${CONTEXT_DIR} git : ref : ${GIT_REF} uri : ${GIT_URI} type : Git strategy : sourceStrategy : from : kind : DockerImage name : inaccel/jupyter:lab scripts : 'https://raw.githubusercontent.com/jupyter/docker-stacks/master/examples/source-to-image' type : Source triggers : - type : ConfigChange - apiVersion : v1 kind : DeploymentConfig metadata : labels : app : ${APPLICATION_NAME} name : ${APPLICATION_NAME} spec : replicas : 1 selector : app : ${APPLICATION_NAME} deploymentconfig : ${APPLICATION_NAME} strategy : type : Recreate triggers : - type : ConfigChange - type : ImageChange imageChangeParams : automatic : true containerNames : - jupyter-notebook from : kind : ImageStreamTag name : ${APPLICATION_NAME}:latest template : metadata : annotations : alpha.image.policy.openshift.io/resolve-names : '*' labels : app : ${APPLICATION_NAME} deploymentconfig : ${APPLICATION_NAME} spec : automountServiceAccountToken : false containers : - command : - start-notebook.sh - --NotebookApp.token=${APPLICATION_TOKEN} image : ${APPLICATION_NAME}:latest name : jupyter-notebook ports : - containerPort : 8888 protocol : TCP resources : limits : ${FPGA_LIMIT_KEY} : ${FPGA_LIMIT_VALUE} securityContext : supplementalGroups : - 100 - apiVersion : v1 kind : ImageStream metadata : labels : app : ${APPLICATION_NAME} name : ${APPLICATION_NAME} - apiVersion : v1 kind : Route metadata : labels : app : ${APPLICATION_NAME} name : ${APPLICATION_NAME} spec : port : targetPort : 8888-tcp tls : insecureEdgeTerminationPolicy : Redirect termination : edge to : kind : Service name : ${APPLICATION_NAME} - apiVersion : v1 kind : Service metadata : labels : app : ${APPLICATION_NAME} name : ${APPLICATION_NAME} spec : ports : - name : 8888-tcp port : 8888 protocol : TCP targetPort : 8888 selector : app : ${APPLICATION_NAME} deploymentconfig : ${APPLICATION_NAME} type : ClusterIP Having loaded the image stream definitions, InAccel Jupyter App will now be able to be selected from the catalog of applications that can be installed from the UI. For a complete walkthrough on how to deploy a new FPGA-accelerated Jupyter application check the following step-by-step tutorial:","title":"Adding Support for FPGA-enabled Jupyter Notebooks"},{"location":"labs/rancher/","text":"In this lab, we will lay out the absolute easiest way to begin using FPGA resources in Kubernetes clusters. To really simplify things, we will describe the process for enabling FPGAs in terms of the Rancher user interface. The Rancher UI is simply a client to the Rancher RESTful APIs. You can use other clients to the APIs, such as Golang, Python and Terraform, in GitOps, DevOps and other automated solutions. We won\u2019t delve into any of those here. Fundamentally, the process is simple: Build out your infrastructure for a Kubernetes cluster Install Kubernetes Install the fpga-operator via Helm Getting Up and Running with Rancher and Available FPGA Resources - Rancher is a multi-cluster management solution that spawns Kubernetes instances using either local or cloud resources. Recently, Amazon AWS and Microsoft Azure made available FPGA instances to the public in order to leverage the needs for application acceleration. On the other hand, InAccel has developed an fpga-operator that is compatible with all the available FPGAs in the Cloud, including Intel and Xilinx ones. The fpga-operator performs the following: Installs the right FPGA drivers Instantiates the InAccel CSI and Device Plugins Deploys and starts InAccel Coral Resource Manager and Coral Monitor FPGA drivers - InAccel performs all the checks required to install the right drivers for each specific OS type and version. CSI and Device plugins - InAccel has developed a Device plugin based on the Kubernetes standard, that works for all FPGA devices and is able to announce and keep track of all the FPGAs of a cluster. It has also developed a CSI plugin that eases the deployment of FPGA accelerated pods by performing any required actions regarding volumes that have to be mounted etc. Coral Resource Manager and Monitor - InAccel Coral is the framework that manages, orchestrates and scales FPGA resquests accross a cluster of available FPGA resources. InAccel Coral Monitor is the tool for visualizing the state of each FPGA including structural information (temperature, power etc.) as well as information about its current configuration (loaded bitstream, number and name of the kernels, memory connections and usage, execution time for each accelerated request etc.). Prerequisites - Here is the bill of materials (BOM) for getting up and running with FPGAs in Rancher: Rancher InAccel FPGA Operator Infrastructure - we\u2019ll use FPGA nodes on AWS There is plenty of documentation to set up an HA installation of Rancher, so we will assume you already have Rancher installed. Process Steps - Install a Kubernetes cluster with FPGAs - With Rancher installed, we will first build and configure a Kubernetes cluster (you can use any cluster with FPGAs.) Under the Global context, we select Add Cluster and under the section \"With RKE and new nodes in an infrastructure provider,\" we select the Amazon EC2 provider. Right after, we are going to create two node pool templates: one for the master nodes and one for the worker ones. To do so, we select the plus icon ( + ) under the Template column. The template for the master nodes is the following: The template for the worker nodes (FPGA instances) is the following and is based on the f1.2xlarge machine type that comes with a VU9P Xilinx FPGA device. We do NOT need to specify a custom AMI with pre-installed FPGA drivers or tools, since our FPGA Operator will take care of the whole system setup: After we have finished creating the two templates we specify the nodes that will be spawned. We choose to spawn one master and one worker node for this tutorial, as shown below: We left all other options set to the defaults. After a couple of minutes the cluster is up and running. Set Up the FPGA Operator - It is time to setup a catalog in Rancher using the FPGA Operator repository . Using the Rancher Apps & Marketplace menu, we search for the FPGA Operator chart. Selecting the chart, we can get more information on the chart itself as well as the default configuration and the values that can be configured. We select Install . We don't have to specify something in steps 1 and 2 so we just proceed by selecting the Next and Install buttons respectively. After a few moments the application is successfully deployed. To get more information on the components deployed we can select the application name. To view the pod that InAccel FPGA Operator DaemonSet has deployed we select the name right to the DaemonSet property. We can also get a more detailed view of the containers inside that pod. We can further inspect InAccel Coral logs using the three dots right to the DaemonSet pod entry and select View Logs . Make use of the FPGAs - Now that FPGAs are accessible, we can deploy an FPGA-capable workload. We can also verify that this installation was successful by looking at the cluster details in Rancher. We see that the FPGA Operator has kindly labeled our nodes for FPGA usage. Deploy an FPGA-Capable workload - We open the Workload menu and select Pods from the menu column. Then from the right top corner we select Create from YAML . For this workload, we have already prepared a pod specification (available also on github ) that simply invokes a vector addition workload for FPGA acceleration: And after a couple of seconds we see that our workload has successfully ran on the FPGA device and returned the correct results ( Test PASSED ). Conclusion - This simplified approach to getting Kubernetes up and running with FPGAs takes advantage of these two things: The FPGA Operator from InAccel Rancher\u2019s cluster deployment and catalog app integration","title":"Get Up and Running with FPGAs in Rancher Kubernetes Clusters"},{"location":"labs/rancher/#getting-up-and-running-with-rancher-and-available-fpga-resources","text":"Rancher is a multi-cluster management solution that spawns Kubernetes instances using either local or cloud resources. Recently, Amazon AWS and Microsoft Azure made available FPGA instances to the public in order to leverage the needs for application acceleration. On the other hand, InAccel has developed an fpga-operator that is compatible with all the available FPGAs in the Cloud, including Intel and Xilinx ones. The fpga-operator performs the following: Installs the right FPGA drivers Instantiates the InAccel CSI and Device Plugins Deploys and starts InAccel Coral Resource Manager and Coral Monitor","title":"Getting Up and Running with Rancher and Available FPGA Resources"},{"location":"labs/rancher/#fpga-drivers","text":"InAccel performs all the checks required to install the right drivers for each specific OS type and version.","title":"FPGA drivers"},{"location":"labs/rancher/#csi-and-device-plugins","text":"InAccel has developed a Device plugin based on the Kubernetes standard, that works for all FPGA devices and is able to announce and keep track of all the FPGAs of a cluster. It has also developed a CSI plugin that eases the deployment of FPGA accelerated pods by performing any required actions regarding volumes that have to be mounted etc.","title":"CSI and Device plugins"},{"location":"labs/rancher/#coral-resource-manager-and-monitor","text":"InAccel Coral is the framework that manages, orchestrates and scales FPGA resquests accross a cluster of available FPGA resources. InAccel Coral Monitor is the tool for visualizing the state of each FPGA including structural information (temperature, power etc.) as well as information about its current configuration (loaded bitstream, number and name of the kernels, memory connections and usage, execution time for each accelerated request etc.).","title":"Coral Resource Manager and Monitor"},{"location":"labs/rancher/#prerequisites","text":"Here is the bill of materials (BOM) for getting up and running with FPGAs in Rancher: Rancher InAccel FPGA Operator Infrastructure - we\u2019ll use FPGA nodes on AWS There is plenty of documentation to set up an HA installation of Rancher, so we will assume you already have Rancher installed.","title":"Prerequisites"},{"location":"labs/rancher/#process-steps","text":"","title":"Process Steps"},{"location":"labs/rancher/#install-a-kubernetes-cluster-with-fpgas","text":"With Rancher installed, we will first build and configure a Kubernetes cluster (you can use any cluster with FPGAs.) Under the Global context, we select Add Cluster and under the section \"With RKE and new nodes in an infrastructure provider,\" we select the Amazon EC2 provider. Right after, we are going to create two node pool templates: one for the master nodes and one for the worker ones. To do so, we select the plus icon ( + ) under the Template column. The template for the master nodes is the following: The template for the worker nodes (FPGA instances) is the following and is based on the f1.2xlarge machine type that comes with a VU9P Xilinx FPGA device. We do NOT need to specify a custom AMI with pre-installed FPGA drivers or tools, since our FPGA Operator will take care of the whole system setup: After we have finished creating the two templates we specify the nodes that will be spawned. We choose to spawn one master and one worker node for this tutorial, as shown below: We left all other options set to the defaults. After a couple of minutes the cluster is up and running.","title":"Install a Kubernetes cluster with FPGAs"},{"location":"labs/rancher/#set-up-the-fpga-operator","text":"It is time to setup a catalog in Rancher using the FPGA Operator repository . Using the Rancher Apps & Marketplace menu, we search for the FPGA Operator chart. Selecting the chart, we can get more information on the chart itself as well as the default configuration and the values that can be configured. We select Install . We don't have to specify something in steps 1 and 2 so we just proceed by selecting the Next and Install buttons respectively. After a few moments the application is successfully deployed. To get more information on the components deployed we can select the application name. To view the pod that InAccel FPGA Operator DaemonSet has deployed we select the name right to the DaemonSet property. We can also get a more detailed view of the containers inside that pod. We can further inspect InAccel Coral logs using the three dots right to the DaemonSet pod entry and select View Logs .","title":"Set Up the FPGA Operator"},{"location":"labs/rancher/#make-use-of-the-fpgas","text":"Now that FPGAs are accessible, we can deploy an FPGA-capable workload. We can also verify that this installation was successful by looking at the cluster details in Rancher. We see that the FPGA Operator has kindly labeled our nodes for FPGA usage.","title":"Make use of the FPGAs"},{"location":"labs/rancher/#deploy-an-fpga-capable-workload","text":"We open the Workload menu and select Pods from the menu column. Then from the right top corner we select Create from YAML . For this workload, we have already prepared a pod specification (available also on github ) that simply invokes a vector addition workload for FPGA acceleration: And after a couple of seconds we see that our workload has successfully ran on the FPGA device and returned the correct results ( Test PASSED ).","title":"Deploy an FPGA-Capable workload"},{"location":"labs/rancher/#conclusion","text":"This simplified approach to getting Kubernetes up and running with FPGAs takes advantage of these two things: The FPGA Operator from InAccel Rancher\u2019s cluster deployment and catalog app integration","title":"Conclusion"},{"location":"project/keras/","text":"Deep Learning for humans - How to install - pip install inaccel-keras FPGA Platforms - Get the available accelerators for your platform. U250 U280 xilinx xdma_201830.2 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/xilinx/com/researchlabs/1.0/2mobilenet inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/xilinx/com/researchlabs/1.2/1resnet50 xilinx xdma_201920.3 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u280/xdma_201920.3/xilinx/com/researchlabs/1.0/1resnet50 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u280/xdma_201920.3/xilinx/com/researchlabs/1.1/2mobilenet Examples - Classify ImageNet classes with ResNet50 - Weights are downloaded automatically when instantiating a model. They are stored at ~/.keras/inaccel/models/ . import time from inaccel.keras.applications.resnet50 import ResNet50 from inaccel.keras.preprocessing.image import ImageDataGenerator model = ResNet50 ( weights = 'imagenet' ) data = ImageDataGenerator ( dtype = 'int8' ) images = data . flow_from_directory ( 'imagenet/' , target_size = ( 224 , 224 ), class_mode = None , batch_size = 64 ) begin = time . monotonic () preds = model . predict ( images , workers = 16 ) end = time . monotonic () print ( 'Duration for' , len ( preds ), 'images: %.3f sec' % ( end - begin )) print ( 'FPS: %.3f ' % ( len ( preds ) / ( end - begin ))) import numpy as np from inaccel.keras.applications.resnet50 import decode_predictions , ResNet50 from inaccel.keras.preprocessing.image import load_img model = ResNet50 ( weights = 'imagenet' ) dog = load_img ( 'data/dog.jpg' , target_size = ( 224 , 224 )) dog = np . expand_dims ( dog , axis = 0 ) elephant = load_img ( 'data/elephant.jpg' , target_size = ( 224 , 224 )) elephant = np . expand_dims ( elephant , axis = 0 ) images = np . vstack ([ dog , elephant ]) preds = model . predict ( images ) print ( 'Predicted:' , decode_predictions ( preds , top = 1 ))","title":"Keras"},{"location":"project/keras/#deep-learning-for-humans","text":"","title":"Deep Learning for humans"},{"location":"project/keras/#how-to-install","text":"pip install inaccel-keras","title":"How to install"},{"location":"project/keras/#fpga-platforms","text":"Get the available accelerators for your platform. U250 U280 xilinx xdma_201830.2 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/xilinx/com/researchlabs/1.0/2mobilenet inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/xilinx/com/researchlabs/1.2/1resnet50 xilinx xdma_201920.3 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u280/xdma_201920.3/xilinx/com/researchlabs/1.0/1resnet50 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u280/xdma_201920.3/xilinx/com/researchlabs/1.1/2mobilenet","title":"FPGA Platforms"},{"location":"project/keras/#examples","text":"","title":"Examples"},{"location":"project/lz4/","text":"Extremely Fast Compression algorithm - How to install - Debian RedHat curl -sS https://setup.inaccel.com/repository | sh apt install coral-api git clone https://github.com/inaccel/lz4 make install -C lz4/programs curl -sS https://setup.inaccel.com/repository | sh yum install coral-api git clone https://github.com/inaccel/lz4 make install -C lz4/programs FPGA Platforms - Get the available accelerators for your platform. AWS VU9P F1 U280 U50 xilinx dynamic-shell inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/com/xilinx/vitis/dataCompression/lz4/1.1/8compress xilinx xdma_201920.3 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u280/xdma_201920.3/com/xilinx/vitis/dataCompression/lz4/1.1/8compress xilinx gen3x16_xdma_201920.3 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u50/gen3x16_xdma_201920.3/com/xilinx/vitis/dataCompression/lz4/1.1/7compress Examples - Enable Acceleration - Usage: lz4 -F [ arg ] [ input ] [ output ]","title":"LZ4"},{"location":"project/lz4/#extremely-fast-compression-algorithm","text":"","title":"Extremely Fast Compression algorithm"},{"location":"project/lz4/#how-to-install","text":"Debian RedHat curl -sS https://setup.inaccel.com/repository | sh apt install coral-api git clone https://github.com/inaccel/lz4 make install -C lz4/programs curl -sS https://setup.inaccel.com/repository | sh yum install coral-api git clone https://github.com/inaccel/lz4 make install -C lz4/programs","title":"How to install"},{"location":"project/lz4/#fpga-platforms","text":"Get the available accelerators for your platform. AWS VU9P F1 U280 U50 xilinx dynamic-shell inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/com/xilinx/vitis/dataCompression/lz4/1.1/8compress xilinx xdma_201920.3 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u280/xdma_201920.3/com/xilinx/vitis/dataCompression/lz4/1.1/8compress xilinx gen3x16_xdma_201920.3 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u50/gen3x16_xdma_201920.3/com/xilinx/vitis/dataCompression/lz4/1.1/7compress","title":"FPGA Platforms"},{"location":"project/lz4/#examples","text":"","title":"Examples"},{"location":"project/openssl/","text":"TLS/SSL and crypto library - How to install - Debian RedHat curl -sS https://setup.inaccel.com/repository | sh apt install inaccel-openssl curl -sS https://setup.inaccel.com/repository | sh yum install inaccel-openssl FPGA Platforms - Get the available accelerators for your platform. AWS VU9P F1 U200 U250 U280 U50 xilinx dynamic-shell inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/openssl/crypto/aes/1.0.2/4cbc-decrypt_4cfb128-decrypt_4ctr128-crypt xilinx xdma_201830.2 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u200/xdma_201830.2/openssl/crypto/aes/1.0.2/4cbc-decrypt_4cfb128-decrypt_4ctr128-crypt xilinx xdma_201830.2 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/openssl/crypto/aes/1.0.2/4cbc-decrypt_4cfb128-decrypt_4ctr128-crypt xilinx xdma_201920.3 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u280/xdma_201920.3/openssl/crypto/aes/1.0.2/4cbc-decrypt_4cfb128-decrypt_4ctr128-crypt xilinx gen3x16_xdma_201920.3 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u50/gen3x16_xdma_201920.3/openssl/crypto/aes/1.0.2/4cbc-decrypt_4cfb128-decrypt_4ctr128-crypt Examples - 128-EEA2 decrypt - Pass the following options to the linker: -lcoral-api -linaccel-crypto #include <inaccel/openssl/aes.h> #include <inaccel/shm.h> #include <stdio.h> #include <stdlib.h> #include <string.h> #define LENGTH 200 const unsigned char IN [ LENGTH ] = { 0xB9 , 0x67 , 0xD5 , 0x53 , 0x0D , 0xFC , 0x48 , 0x37 , 0x1E , 0xE0 , 0xEA , 0x48 , 0xB7 , 0x1D , 0xA4 , 0x10 , 0x6F , 0x6C , 0xC0 , 0x54 , 0x1F , 0x06 , 0xBF , 0x66 , 0x40 , 0x4E , 0xC3 , 0x75 , 0x4E , 0x0A , 0x0D , 0xC8 , 0xDD , 0x66 , 0x2C , 0x0E , 0xFE , 0xE0 , 0x95 , 0x8D , 0x3F , 0x12 , 0xCF , 0x9C , 0xF2 , 0xCA , 0xDA , 0xFF , 0xC6 , 0x2E , 0xE3 , 0xD1 , 0x5A , 0x45 , 0xB3 , 0xF6 , 0xAA , 0x4C , 0x6E , 0x78 , 0x5B , 0x57 , 0x86 , 0xFC , 0xA2 , 0xCF , 0x4C , 0x36 , 0xE3 , 0xAA , 0xE6 , 0x8C , 0x8D , 0xB7 , 0x02 , 0xBE , 0x99 , 0xD1 , 0x88 , 0x5A , 0x99 , 0xCA , 0x88 , 0xE3 , 0xBA , 0xCC , 0xA1 , 0xFD , 0xB2 , 0x83 , 0xC4 , 0xC4 , 0x07 , 0x0A , 0xC7 , 0x2D , 0x06 , 0xCB , 0x2E , 0x5E , 0xC6 , 0xBE , 0x0C , 0xA6 , 0x27 , 0xAA , 0xDA , 0x49 , 0x45 , 0x81 , 0x62 , 0x36 , 0x0D , 0xEE , 0x93 , 0xF9 , 0x8D , 0x3A , 0x05 , 0xB6 , 0xE5 , 0xA7 , 0x19 , 0x08 , 0x26 , 0xF6 , 0x56 , 0xF0 , 0x1B , 0x2A , 0x58 , 0x5B , 0xC7 , 0x1B , 0xE9 , 0x1A , 0x3B , 0x9D , 0xEF , 0xF1 , 0xA7 , 0x50 , 0x2D , 0x70 , 0x9D , 0x6C , 0x8E , 0x60 , 0xE0 , 0x31 , 0xE4 , 0xDE , 0x50 , 0xE0 , 0xF6 , 0x80 , 0x02 , 0x94 , 0xD3 , 0xEF , 0xA7 , 0x5B , 0xC3 , 0x66 , 0x5D , 0x82 , 0x8E , 0xEB , 0x48 , 0x68 , 0x0B , 0x4B , 0xD4 , 0xC9 , 0xEE , 0x1F , 0x5F , 0x06 , 0xF7 , 0xBE , 0xAC , 0x2B , 0x9B , 0x67 , 0xF7 , 0x43 , 0x79 , 0xB2 , 0xCC , 0xB8 , 0xE9 , 0x15 , 0xCE , 0x5E , 0xD1 , 0x1E , 0xB4 , 0x8C , 0x12 , 0x70 }; const unsigned char OUT [ LENGTH ] = { 0xA4 , 0x98 , 0xA3 , 0x56 , 0x54 , 0xBF , 0xF2 , 0xFD , 0xFA , 0x7A , 0x6C , 0x53 , 0x15 , 0x14 , 0xEA , 0xE3 , 0x2E , 0x52 , 0x8F , 0x20 , 0x57 , 0x09 , 0x58 , 0x28 , 0xA8 , 0x06 , 0xD5 , 0xD1 , 0x53 , 0x83 , 0x78 , 0x76 , 0x1C , 0x9A , 0x4B , 0xEE , 0x59 , 0x3E , 0x6B , 0x53 , 0x37 , 0xD6 , 0x25 , 0x4B , 0x69 , 0x8F , 0xAE , 0x96 , 0x60 , 0x3D , 0x36 , 0xB6 , 0xC5 , 0x8D , 0xDE , 0x6D , 0x12 , 0x33 , 0x3E , 0xE3 , 0xB5 , 0xB5 , 0x5A , 0xD0 , 0xCF , 0x24 , 0x3E , 0x28 , 0xE0 , 0xA8 , 0xFA , 0x97 , 0xFD , 0x1F , 0xE1 , 0x67 , 0x2D , 0x0F , 0x7C , 0x8D , 0xCB , 0x31 , 0x43 , 0x90 , 0xBD , 0xA0 , 0x7C , 0xCF , 0xD2 , 0xB9 , 0xB2 , 0x88 , 0xED , 0x8B , 0xD7 , 0xBC , 0x2F , 0x16 , 0x64 , 0x0F , 0x3D , 0xDD , 0xA5 , 0x3B , 0x7B , 0x07 , 0x21 , 0xA8 , 0x94 , 0x1C , 0x35 , 0xDE , 0x4D , 0xF6 , 0xED , 0x89 , 0x97 , 0x69 , 0xD7 , 0x69 , 0xA1 , 0x0A , 0x70 , 0x8F , 0x94 , 0x48 , 0xCA , 0x42 , 0xDC , 0xAD , 0xD0 , 0x98 , 0x8A , 0xF4 , 0x52 , 0x06 , 0x7A , 0x72 , 0x2D , 0x0F , 0x0E , 0x61 , 0x6C , 0x5A , 0xD6 , 0x5A , 0xE2 , 0x6D , 0xC2 , 0xBA , 0x56 , 0x64 , 0x43 , 0x45 , 0x72 , 0x56 , 0x0C , 0xBB , 0x98 , 0xE7 , 0x69 , 0x68 , 0xFF , 0x72 , 0x5C , 0x51 , 0x77 , 0x56 , 0xC3 , 0x23 , 0x64 , 0x50 , 0x03 , 0xCF , 0xA9 , 0xD9 , 0xA8 , 0x0B , 0x46 , 0x6B , 0x44 , 0x1B , 0x4E , 0x86 , 0x60 , 0x3F , 0xDC , 0x6B , 0xF9 , 0x74 , 0xD2 , 0x62 , 0x5B , 0xD1 , 0x54 , 0x36 , 0x22 , 0xCA , 0x8B , 0x64 }; const unsigned char USERKEY [ 16 ] = { 0x0A , 0x8B , 0x6B , 0xD8 , 0xD9 , 0xB0 , 0x8B , 0x08 , 0xD6 , 0x4E , 0x32 , 0xD1 , 0x81 , 0x77 , 0x77 , 0xFB }; const unsigned int COUNT = 0x00000000 ; const unsigned char BEARER = 0x04 ; const unsigned char DIRECTION = 0x00 ; int main ( int argc , char * argv []) { unsigned char * in = ( unsigned char * ) inaccel_alloc ( LENGTH ); if ( ! in ) { perror ( \"inaccel_alloc\" ); return EXIT_FAILURE ; } unsigned char * out = ( unsigned char * ) inaccel_alloc ( LENGTH ); if ( ! out ) { perror ( \"inaccel_alloc\" ); return EXIT_FAILURE ; } memcpy ( in , IN , LENGTH ); inaccel_AES_KEY key ; if ( inaccel_AES_set_encrypt_key ( USERKEY , 128 , & key )) { perror ( \"inaccel_AES_set_encrypt_key\" ); return EXIT_FAILURE ; } unsigned char ivec [ inaccel_AES_BLOCK_SIZE ] = { 0x00 , 0x00 , 0x00 , 0x00 , 0x00 , 0x00 , 0x00 , 0x00 , 0x00 , 0x00 , 0x00 , 0x00 , 0x00 , 0x00 , 0x00 , 0x00 }; memcpy ( ivec , & COUNT , sizeof ( COUNT )); ivec [ sizeof ( COUNT )] = ( BEARER << 3 ) | ( DIRECTION << 2 ); unsigned char ecount_buf [ inaccel_AES_BLOCK_SIZE ]; unsigned int num = 0 ; if ( inaccel_AES_ctr128_encrypt ( in , out , ((( LENGTH - 1 ) >> 4 ) + 1 ) << 4 , & key , ivec , ecount_buf , & num )) { perror ( \"inaccel_AES_ctr128_encrypt\" ); return EXIT_FAILURE ; } if ( memcmp ( out , OUT , LENGTH )) { fprintf ( stderr , \"bad decrypt \\n \" ); return EXIT_FAILURE ; } inaccel_free ( in ); inaccel_free ( out ); return EXIT_SUCCESS ; }","title":"OpenSSL"},{"location":"project/openssl/#tlsssl-and-crypto-library","text":"","title":"TLS/SSL and crypto library"},{"location":"project/openssl/#how-to-install","text":"Debian RedHat curl -sS https://setup.inaccel.com/repository | sh apt install inaccel-openssl curl -sS https://setup.inaccel.com/repository | sh yum install inaccel-openssl","title":"How to install"},{"location":"project/openssl/#fpga-platforms","text":"Get the available accelerators for your platform. AWS VU9P F1 U200 U250 U280 U50 xilinx dynamic-shell inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/openssl/crypto/aes/1.0.2/4cbc-decrypt_4cfb128-decrypt_4ctr128-crypt xilinx xdma_201830.2 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u200/xdma_201830.2/openssl/crypto/aes/1.0.2/4cbc-decrypt_4cfb128-decrypt_4ctr128-crypt xilinx xdma_201830.2 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/openssl/crypto/aes/1.0.2/4cbc-decrypt_4cfb128-decrypt_4ctr128-crypt xilinx xdma_201920.3 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u280/xdma_201920.3/openssl/crypto/aes/1.0.2/4cbc-decrypt_4cfb128-decrypt_4ctr128-crypt xilinx gen3x16_xdma_201920.3 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u50/gen3x16_xdma_201920.3/openssl/crypto/aes/1.0.2/4cbc-decrypt_4cfb128-decrypt_4ctr128-crypt","title":"FPGA Platforms"},{"location":"project/openssl/#examples","text":"","title":"Examples"},{"location":"project/scikit-learn/","text":"Machine Learning in Python - How to install - pip install inaccel-scikit-learn FPGA Platforms - Get the available accelerators for your platform. AWS-VU9P-F1 PAC_A10 U200 U250 U280 xilinx dynamic-shell inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/com/inaccel/ml/KMeans/1.0/4Centroids inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/com/inaccel/ml/KMeans/1.0/4Centroids1 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/com/inaccel/ml/LogisticRegression/1.0/4Gradients inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/com/inaccel/ml/NaiveBayes/1.0/4Classifier intel 9926ab6d6c925a68aabca7d84c545738 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/intel/pac_a10/9926ab6d6c925a68aabca7d84c545738/com/inaccel/ml/KMeans/1.0/1Centroids inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/intel/pac_a10/9926ab6d6c925a68aabca7d84c545738/com/inaccel/ml/LogisticRegression/1.0/1Gradients xilinx xdma_201820.1 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u200/xdma_201820.1/com/inaccel/ml/KMeans/1.0/4Centroids inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u200/xdma_201820.1/com/inaccel/ml/KMeans/1.0/4Centroids1 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u200/xdma_201820.1/com/inaccel/ml/LogisticRegression/1.0/4Gradients xilinx xdma_201830.2 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/com/inaccel/ml/KMeans/1.0/4Centroids inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/com/inaccel/ml/KMeans/1.0/4Centroids1 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/com/inaccel/ml/LogisticRegression/1.0/4Gradients inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/com/inaccel/ml/NaiveBayes/1.0/4Classifier xilinx xdma_201920.3 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u280/xdma_201920.3/com/inaccel/ml/NaiveBayes/1.0/4Classifier Examples - MNIST classification using multinomial Logistic Regression - Here we fit a multinomial Logistic Regression on a subset of the MNIST digits classification task. Test accuracy reaches > 0.9. from sklearn.datasets import fetch_openml from inaccel.sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import StandardScaler from timeit import default_timer as timestamp X , y = fetch_openml ( 'mnist_784' , return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y ) features = StandardScaler () X_train = features . fit_transform ( X_train ) X_test = features . transform ( X_test ) label = LabelEncoder () y_train = label . fit_transform ( y_train ) y_test = label . transform ( y_test ) begin = timestamp () model = LogisticRegression () . fit ( X_train , y_train ) end = timestamp () print ( 'time= %.3f ' % ( end - begin )) predictions = model . predict ( X_test ) print ( 'accuracy= %.3f ' % accuracy_score ( y_test , predictions ))","title":"scikit-learn"},{"location":"project/scikit-learn/#machine-learning-in-python","text":"","title":"Machine Learning in Python"},{"location":"project/scikit-learn/#how-to-install","text":"pip install inaccel-scikit-learn","title":"How to install"},{"location":"project/scikit-learn/#fpga-platforms","text":"Get the available accelerators for your platform. AWS-VU9P-F1 PAC_A10 U200 U250 U280 xilinx dynamic-shell inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/com/inaccel/ml/KMeans/1.0/4Centroids inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/com/inaccel/ml/KMeans/1.0/4Centroids1 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/com/inaccel/ml/LogisticRegression/1.0/4Gradients inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/com/inaccel/ml/NaiveBayes/1.0/4Classifier intel 9926ab6d6c925a68aabca7d84c545738 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/intel/pac_a10/9926ab6d6c925a68aabca7d84c545738/com/inaccel/ml/KMeans/1.0/1Centroids inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/intel/pac_a10/9926ab6d6c925a68aabca7d84c545738/com/inaccel/ml/LogisticRegression/1.0/1Gradients xilinx xdma_201820.1 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u200/xdma_201820.1/com/inaccel/ml/KMeans/1.0/4Centroids inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u200/xdma_201820.1/com/inaccel/ml/KMeans/1.0/4Centroids1 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u200/xdma_201820.1/com/inaccel/ml/LogisticRegression/1.0/4Gradients xilinx xdma_201830.2 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/com/inaccel/ml/KMeans/1.0/4Centroids inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/com/inaccel/ml/KMeans/1.0/4Centroids1 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/com/inaccel/ml/LogisticRegression/1.0/4Gradients inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/com/inaccel/ml/NaiveBayes/1.0/4Classifier xilinx xdma_201920.3 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u280/xdma_201920.3/com/inaccel/ml/NaiveBayes/1.0/4Classifier","title":"FPGA Platforms"},{"location":"project/scikit-learn/#examples","text":"","title":"Examples"},{"location":"project/tf-quant-finance/","text":"High-performance TensorFlow library for quantitative finance. - How to install - pip install inaccel-tf-quant-finance FPGA Platforms - Get the available accelerators for your platform. PAC_A10 intel 38d782e3b6125343b9342433e348ac4c inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/intel/pac_a10/38d782e3b6125343b9342433e348ac4c/com/inaccel/quantitativeFinance/blackScholes/1.0/1binary-price inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/intel/pac_a10/38d782e3b6125343b9342433e348ac4c/com/inaccel/quantitativeFinance/blackScholes/1.0/1option-price Examples - Black Scholes pricing - Here we see how to price vanilla options in the Black Scholes framework using the library. import numpy as np from inaccel.tf_quant_finance.black_scholes import option_price # Calculate discount factors (e^-rT) rate = 0.05 expiries = np . array ([ 0.5 , 1.0 , 2.0 , 1.3 ]) discount_factors = np . exp ( - rate * expiries ) # Current value of assets. spots = np . array ([ 0.9 , 1.0 , 1.1 , 0.9 ]) # Forward value of assets at expiry. forwards = spots / discount_factors # Strike prices given by: strikes = np . array ([ 1.0 , 2.0 , 1.0 , 0.5 ]) # Indicate whether options are call (True) or put (False) is_call_options = np . array ([ True , True , False , False ]) # The volatilites at which the options are to be priced. volatilities = np . array ([ 0.7 , 1.1 , 2.0 , 0.5 ]) # Calculate the prices given the volatilities and term structure. prices = option_price ( volatilities = volatilities , strikes = strikes , expiries = expiries , forwards = forwards , discount_factors = discount_factors , is_call_options = is_call_options )","title":"TF Quant Finance"},{"location":"project/tf-quant-finance/#high-performance-tensorflow-library-for-quantitative-finance","text":"","title":"High-performance TensorFlow library for quantitative finance."},{"location":"project/tf-quant-finance/#how-to-install","text":"pip install inaccel-tf-quant-finance","title":"How to install"},{"location":"project/tf-quant-finance/#fpga-platforms","text":"Get the available accelerators for your platform. PAC_A10 intel 38d782e3b6125343b9342433e348ac4c inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/intel/pac_a10/38d782e3b6125343b9342433e348ac4c/com/inaccel/quantitativeFinance/blackScholes/1.0/1binary-price inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/intel/pac_a10/38d782e3b6125343b9342433e348ac4c/com/inaccel/quantitativeFinance/blackScholes/1.0/1option-price","title":"FPGA Platforms"},{"location":"project/tf-quant-finance/#examples","text":"","title":"Examples"},{"location":"project/xgboost/","text":"Scalable, Portable and Distributed Gradient Boosting - How to install - pip install --extra-index-url https://test.pypi.org/simple inaccel-xgboost FPGA Platforms - Get the available accelerators for your platform. AWS-VU9P-F1 U250 xilinx dynamic-shell inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/com/inaccel/xgboost/0.1/2exact xilinx xdma_201830.2 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/com/inaccel/xgboost/0.2/4exact Examples - Get Started with XGBoost - This is a quick start tutorial for you to quickly try out XGBoost on the demo dataset on a classification task. import xgboost as xgb from sklearn.datasets import fetch_openml from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import Normalizer from timeit import default_timer as timestamp X , y = fetch_openml ( 'SVHN' , return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.35 ) features = Normalizer () X_train = features . fit_transform ( X_train ) X_test = features . transform ( X_test ) label = LabelEncoder () y_train = label . fit_transform ( y_train ) y_test = label . transform ( y_test ) params = { 'alpha' : 0.0 , 'eta' : 0.3 , 'max_depth' : 10 , 'num_class' : len ( label . classes_ ), 'objective' : 'multi:softmax' , 'subsample' : 1.0 , 'tree_method' : 'fpga_exact' } dtrain = xgb . DMatrix ( X_train , y_train ) dtest = xgb . DMatrix ( X_test , y_test ) begin = timestamp () model = xgb . train ( params , dtrain , 10 ) end = timestamp () print ( 'time= %.3f ' % ( end - begin )) predictions = model . predict ( dtest ) print ( 'accuracy= %.3f ' % accuracy_score ( y_test , predictions ))","title":"XGBoost"},{"location":"project/xgboost/#scalable-portable-and-distributed-gradient-boosting","text":"","title":"Scalable, Portable and Distributed Gradient Boosting"},{"location":"project/xgboost/#how-to-install","text":"pip install --extra-index-url https://test.pypi.org/simple inaccel-xgboost","title":"How to install"},{"location":"project/xgboost/#fpga-platforms","text":"Get the available accelerators for your platform. AWS-VU9P-F1 U250 xilinx dynamic-shell inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/aws-vu9p-f1/dynamic-shell/aws/com/inaccel/xgboost/0.1/2exact xilinx xdma_201830.2 inaccel bitstream install https://store.inaccel.com/artifactory/bitstreams/xilinx/u250/xdma_201830.2/com/inaccel/xgboost/0.2/4exact","title":"FPGA Platforms"},{"location":"project/xgboost/#examples","text":"","title":"Examples"},{"location":"reference/docker/inaccel/","text":"Description - Simplifying FPGA management in Docker Usage - docker inaccel [OPTIONS] COMMAND Options - Name, shorthand Default Description --env, -e Set environment variables --env-file Specify an alternate environment file --profile Specify a profile to enable --project-name, -p inaccel/fpga-operator Specify an alternate project name --pull Always attempt to pull a newer version of the project --tag, -t latest Specify the project tag to use --version, -v Print version information and quit Child commands - Command Description docker inaccel config Validate and view the config file docker inaccel down Stop and remove containers and networks docker inaccel exec Execute a command in a running container docker inaccel logs View output from containers docker inaccel ps List containers docker inaccel run Run a one-off command docker inaccel up Create and start containers","title":"docker inaccel (base subcommand)"},{"location":"reference/docker/inaccel/#description","text":"Simplifying FPGA management in Docker","title":"Description"},{"location":"reference/docker/inaccel/#usage","text":"docker inaccel [OPTIONS] COMMAND","title":"Usage"},{"location":"reference/docker/inaccel/#options","text":"Name, shorthand Default Description --env, -e Set environment variables --env-file Specify an alternate environment file --profile Specify a profile to enable --project-name, -p inaccel/fpga-operator Specify an alternate project name --pull Always attempt to pull a newer version of the project --tag, -t latest Specify the project tag to use --version, -v Print version information and quit","title":"Options"},{"location":"reference/docker/inaccel/#child-commands","text":"Command Description docker inaccel config Validate and view the config file docker inaccel down Stop and remove containers and networks docker inaccel exec Execute a command in a running container docker inaccel logs View output from containers docker inaccel ps List containers docker inaccel run Run a one-off command docker inaccel up Create and start containers","title":"Child commands"},{"location":"reference/docker/inaccel/config/","text":"Description - Validate and view the config file Usage - docker inaccel config [OPTIONS] Options - Name, shorthand Default Description --profiles Print the profile names, one per line --quiet, -q Only validate the configuration, don't print anything --resolve-image-digests Pin image tags to digests --services Print the service names, one per line Parent command - Command Description docker inaccel Simplifying FPGA management in Docker","title":"docker inaccel config"},{"location":"reference/docker/inaccel/config/#description","text":"Validate and view the config file","title":"Description"},{"location":"reference/docker/inaccel/config/#usage","text":"docker inaccel config [OPTIONS]","title":"Usage"},{"location":"reference/docker/inaccel/config/#options","text":"Name, shorthand Default Description --profiles Print the profile names, one per line --quiet, -q Only validate the configuration, don't print anything --resolve-image-digests Pin image tags to digests --services Print the service names, one per line","title":"Options"},{"location":"reference/docker/inaccel/config/#parent-command","text":"Command Description docker inaccel Simplifying FPGA management in Docker","title":"Parent command"},{"location":"reference/docker/inaccel/down/","text":"Description - Stop and remove containers and networks Usage - docker inaccel down [OPTIONS] [SERVICE] Options - Name, shorthand Default Description --volumes, -v Remove volumes attached to containers Parent command - Command Description docker inaccel Simplifying FPGA management in Docker","title":"docker inaccel down"},{"location":"reference/docker/inaccel/down/#description","text":"Stop and remove containers and networks","title":"Description"},{"location":"reference/docker/inaccel/down/#usage","text":"docker inaccel down [OPTIONS] [SERVICE]","title":"Usage"},{"location":"reference/docker/inaccel/down/#options","text":"Name, shorthand Default Description --volumes, -v Remove volumes attached to containers","title":"Options"},{"location":"reference/docker/inaccel/down/#parent-command","text":"Command Description docker inaccel Simplifying FPGA management in Docker","title":"Parent command"},{"location":"reference/docker/inaccel/exec/","text":"Description - Execute a command in a running container Usage - docker inaccel exec [OPTIONS] COMMAND [ARG...] Options - Name, shorthand Default Description --env, -e Set environment variables --index 1 Index of the container if there are multiple instances of a service --no-tty, -T Disable pseudo-TTY allocation --service, -s Service name --user, -u Username or UID (format: <name --workdir, -w Working directory inside the container Parent command - Command Description docker inaccel Simplifying FPGA management in Docker","title":"docker inaccel exec"},{"location":"reference/docker/inaccel/exec/#description","text":"Execute a command in a running container","title":"Description"},{"location":"reference/docker/inaccel/exec/#usage","text":"docker inaccel exec [OPTIONS] COMMAND [ARG...]","title":"Usage"},{"location":"reference/docker/inaccel/exec/#options","text":"Name, shorthand Default Description --env, -e Set environment variables --index 1 Index of the container if there are multiple instances of a service --no-tty, -T Disable pseudo-TTY allocation --service, -s Service name --user, -u Username or UID (format: <name --workdir, -w Working directory inside the container","title":"Options"},{"location":"reference/docker/inaccel/exec/#parent-command","text":"Command Description docker inaccel Simplifying FPGA management in Docker","title":"Parent command"},{"location":"reference/docker/inaccel/logs/","text":"Description - View output from containers Usage - docker inaccel logs [OPTIONS] [PATTERN] Options - Name, shorthand Default Description --follow, -f Follow log output --index 1 Index of the container if there are multiple instances of a service --no-color Produce monochrome output --service, -s Service name --tail, -n 10 Number of lines to show from the end of the logs --timestamps, -t Show timestamps Parent command - Command Description docker inaccel Simplifying FPGA management in Docker","title":"docker inaccel logs"},{"location":"reference/docker/inaccel/logs/#description","text":"View output from containers","title":"Description"},{"location":"reference/docker/inaccel/logs/#usage","text":"docker inaccel logs [OPTIONS] [PATTERN]","title":"Usage"},{"location":"reference/docker/inaccel/logs/#options","text":"Name, shorthand Default Description --follow, -f Follow log output --index 1 Index of the container if there are multiple instances of a service --no-color Produce monochrome output --service, -s Service name --tail, -n 10 Number of lines to show from the end of the logs --timestamps, -t Show timestamps","title":"Options"},{"location":"reference/docker/inaccel/logs/#parent-command","text":"Command Description docker inaccel Simplifying FPGA management in Docker","title":"Parent command"},{"location":"reference/docker/inaccel/ps/","text":"Description - List containers Usage - docker inaccel ps [OPTIONS] Options - Name, shorthand Default Description --no-trunc Don't truncate output --quiet, -q Only display container IDs Parent command - Command Description docker inaccel Simplifying FPGA management in Docker","title":"docker inaccel ps"},{"location":"reference/docker/inaccel/ps/#description","text":"List containers","title":"Description"},{"location":"reference/docker/inaccel/ps/#usage","text":"docker inaccel ps [OPTIONS]","title":"Usage"},{"location":"reference/docker/inaccel/ps/#options","text":"Name, shorthand Default Description --no-trunc Don't truncate output --quiet, -q Only display container IDs","title":"Options"},{"location":"reference/docker/inaccel/ps/#parent-command","text":"Command Description docker inaccel Simplifying FPGA management in Docker","title":"Parent command"},{"location":"reference/docker/inaccel/run/","text":"Description - Run a one-off command Usage - docker inaccel run [OPTIONS] SERVICE [COMMAND] [ARG...] Options - Name, shorthand Default Description --entrypoint Override the entrypoint of the container --env, -e Set environment variables --no-deps Don't start linked services --no-tty, -T Disable pseudo-TTY allocation --publish, p Publish a container's port(s) to the host --user, -u Username or UID (format: <name --volume, -v Bind mount a volume --workdir, -w Working directory inside the container Parent command - Command Description docker inaccel Simplifying FPGA management in Docker","title":"docker inaccel run"},{"location":"reference/docker/inaccel/run/#description","text":"Run a one-off command","title":"Description"},{"location":"reference/docker/inaccel/run/#usage","text":"docker inaccel run [OPTIONS] SERVICE [COMMAND] [ARG...]","title":"Usage"},{"location":"reference/docker/inaccel/run/#options","text":"Name, shorthand Default Description --entrypoint Override the entrypoint of the container --env, -e Set environment variables --no-deps Don't start linked services --no-tty, -T Disable pseudo-TTY allocation --publish, p Publish a container's port(s) to the host --user, -u Username or UID (format: <name --volume, -v Bind mount a volume --workdir, -w Working directory inside the container","title":"Options"},{"location":"reference/docker/inaccel/run/#parent-command","text":"Command Description docker inaccel Simplifying FPGA management in Docker","title":"Parent command"},{"location":"reference/docker/inaccel/up/","text":"Description - Create and start containers Usage - docker inaccel up [OPTIONS] [SERVICE] Options - Name, shorthand Default Description --always-recreate-deps Recreate dependent containers --force-recreate Recreate containers even if their configuration and image haven't changed --no-deps Don't start linked services --no-recreate If containers already exist, don't recreate them Parent command - Command Description docker inaccel Simplifying FPGA management in Docker","title":"docker inaccel up"},{"location":"reference/docker/inaccel/up/#description","text":"Create and start containers","title":"Description"},{"location":"reference/docker/inaccel/up/#usage","text":"docker inaccel up [OPTIONS] [SERVICE]","title":"Usage"},{"location":"reference/docker/inaccel/up/#options","text":"Name, shorthand Default Description --always-recreate-deps Recreate dependent containers --force-recreate Recreate containers even if their configuration and image haven't changed --no-deps Don't start linked services --no-recreate If containers already exist, don't recreate them","title":"Options"},{"location":"reference/docker/inaccel/up/#parent-command","text":"Command Description docker inaccel Simplifying FPGA management in Docker","title":"Parent command"},{"location":"reference/file-formats/bitstream/","text":"The bitstream kernels are the core component of every FPGA accelerated application. In order to allow automation pipelines, InAccel introduces the bitstream specification that can be used to automatically deploy FPGA binaries on any cluster of FPGAs. It is the single source of information about user's repository of accelerators, that enables Coral to explicitly manage all the supported multi-FPGA platforms through a uniform interface, transforming any set of FPGA bitstreams/kernels to a single pool of hardware accelerators. What is a bitstream repository? - A bitstream repository manages your end-to-end bitstream lifecycle while providing consistency to your CI/CD workflow. A bitstream repository is both a source for bitstream artifacts needed by an accelerated application, and a target to deploy bitstreams generated in the build process. The bitstream repository is crucial for the software development process. Multiple developers from different sites use artifacts and 3 rd party components from different sources, causing testing problems to arise and thus slowing down your release process. Add to all the above, the complexity of handling dozens of different types of technologies and your software development process will be brought to a halt. InAccel\u2019s universal bitstream packaging format supports the requirements of both FPGA vendors (Intel, Xilinx) and is also integrated with all major repository managers currently available (including JFrog Artifactory and Sonatype Nexus ). What is a bitstream artifact? - bitstream - Field name Type Required/Optional Description name string required The actual name of the FPGA bitstream. bitstreamId string required The desired package name (recommended layout a reverse-DNS [organisation].[package] ). version string required The artifact version. description string optional Short description that may include hardware design notes. platform *platform required Defines platform specific information. kernels []kernel required Describes the available kernels (inside the bitstream) and their properties. Sample: JSON XML { \"name\" : \"bitstream.bin\" , \"bitstreamId\" : \"com.inaccel.math.vector\" , \"version\" : \"0.1\" , \"description\" : \"vector operations\" , \"platform\" : { ... }, \"kernels\" : [ ... ] } <bitstream> <name> bitstream.bin </name> <bitstreamId> com.inaccel.math.vector </name> <version> 0.1 </version> <description> vector operations </description> <platform> ... </platform> <kernels> ... </kernels> </bitstream> platform - Field name Type Required/Optional Description vendor string required The FPGA vendor ( intel or xilinx ). name string required The FPGA board identifier. version string required The DSA (for Xilinx) or SDK (for Intel) version. Sample: JSON XML \"platform\" : { \"vendor\" : \"intel / xilinx\" , \"name\" : \"cool-fpga-platform-name\" , \"version\" : \"cool-fpga-platform-version\" }, <platform> <vendor> intel / xilinx </vendor> <name> cool-fpga-platform-name </name> <version> cool-fpga-platform-version </version> </platform> kernel - Field name Type Required/Optional Description name []string required The actual name list of the FPGA kernels. kernelId string required The desired accelerator name. arguments []argument required The list of the accelerator arguments. Sample: JSON XML \"kernels\" : [ ... { \"name\" : [ \"vadd_kernel_1\" ], \"kernelId\" : \"addition\" , \"arguments\" : [ ... ] }, { \"name\" : [ \"vsub_kernel_0\" ], \"kernelId\" : \"subtraction\" , \"arguments\" : [ ... ] }, ... ] <kernels> ... <kernel> <name> vadd_kernel_1 </name> <kernelId> addition </kernelId> <arguments> ... </arguments> </kernel> <kernel> <name> vsub_kernel_0 </name> <kernelId> subtraction </kernelId> <arguments> ... </arguments> </kernel> ... </kernels> argument - Field name Type Required/Optional Description index []string optional Used to modify the default argument sequence. type string required The argument type. name string optional The argument name. size string optional Used to define the size of unknown argument types. memory []string optional Defines the memory bank connections. (default: 0 ) access string optional Defines the access modifier. (default: rw ) Sample: JSON XML \"arguments\" : [ { \"type\" : \"float*\" , \"name\" : \"input1\" , \"memory\" : [ \"0\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"input2\" , \"memory\" : [ \"0\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"output\" , \"memory\" : [ \"0\" ], \"access\" : \"w\" }, { \"type\" : \"int\" , \"name\" : \"size\" } ] <arguments> <argument> <type> float* </type> <name> input1 </name> <memory> 0 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> input2 </name> <memory> 0 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> output </name> <memory> 0 </memory> <access> w </access> </argument> <argument> <type> int </type> <name> size </name> </argument> </arguments> Full Sample - Let's suppose that we have built a new bitstream binary (with name bitstream.bin ) for our cool-fpga-platform . Our bitstream contains four kernels and we want to describe it in a specification file in order to deploy it as a package. JSON XML { \"name\" : \"bitstream.bin\" , \"bitstreamId\" : \"com.inaccel.math.vector\" , \"version\" : \"0.1\" , \"description\" : \"vector operations\" , \"platform\" : { \"vendor\" : \"intel / xilinx\" , \"name\" : \"cool-fpga-fpga-platform-name\" , \"version\" : \"cool-fpga-platform-version\" }, \"kernels\" : [ { \"name\" : [ \"vadd_kernel_0\" ], \"kernelId\" : \"addition\" , \"arguments\" : [ { \"type\" : \"float*\" , \"name\" : \"input1\" , \"memory\" : [ \"0\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"input2\" , \"memory\" : [ \"0\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"output\" , \"memory\" : [ \"0\" ], \"access\" : \"w\" }, { \"type\" : \"int\" , \"name\" : \"size\" } ] }, { \"name\" : [ \"vadd_kernel_1\" ], \"kernelId\" : \"addition\" , \"arguments\" : [ { \"type\" : \"float*\" , \"name\" : \"input1\" , \"memory\" : [ \"1\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"input2\" , \"memory\" : [ \"1\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"output\" , \"memory\" : [ \"1\" ], \"access\" : \"w\" }, { \"type\" : \"int\" , \"name\" : \"size\" } ] }, { \"name\" : [ \"vsub_kernel_0\" ], \"kernelId\" : \"subtraction\" , \"arguments\" : [ { \"type\" : \"float*\" , \"name\" : \"input1\" , \"memory\" : [ \"2\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"input2\" , \"memory\" : [ \"2\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"output\" , \"memory\" : [ \"2\" ], \"access\" : \"w\" }, { \"type\" : \"int\" , \"name\" : \"size\" } ] }, { \"name\" : [ \"vsub_kernel_1\" ], \"kernelId\" : \"subtraction\" , \"arguments\" : [ { \"type\" : \"float*\" , \"name\" : \"input1\" , \"memory\" : [ \"3\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"input2\" , \"memory\" : [ \"3\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"output\" , \"memory\" : [ \"3\" ], \"access\" : \"w\" }, { \"type\" : \"int\" , \"name\" : \"size\" } ] } ] } <bitstream> <name> bitstream.bin </name> <bitstreamId> com.inaccel.math.vector </bitstreamId> <version> 0.1 </version> <description> vector operations </description> <platform> <vendor> intel / xilinx </vendor> <name> cool-fpga-fpga-platform-name </name> <version> cool-fpga-platform-version </version> </platform> <kernels> <kernel> <name> vadd_kernel_0 </name> <kernelId> addition </kernelId> <arguments> <argument> <type> float* </type> <name> input1 </name> <memory> 0 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> input2 </name> <memory> 0 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> output </name> <memory> 0 </memory> <access> w </access> </argument> <argument> <type> int </type> <name> size </name> </argument> </arguments> </kernel> <kernel> <name> vadd_kernel_1 </name> <kernelId> addition </kernelId> <arguments> <argument> <type> float* </type> <name> input1 </name> <memory> 1 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> input2 </name> <memory> 1 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> output </name> <memory> 1 </memory> <access> w </access> </argument> <argument> <type> int </type> <name> size </name> </argument> </arguments> </kernel> <kernel> <name> vsub_kernel_0 </name> <kernelId> subtraction </kernelId> <arguments> <argument> <type> float* </type> <name> input1 </name> <memory> 2 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> input2 </name> <memory> 2 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> output </name> <memory> 2 </memory> <access> w </access> </argument> <argument> <type> int </type> <name> size </name> </argument> </arguments> </kernel> <kernel> <name> vsub_kernel_1 </name> <kernelId> subtraction </kernelId> <arguments> <argument> <type> float* </type> <name> input1 </name> <memory> 3 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> input2 </name> <memory> 3 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> output </name> <memory> 3 </memory> <access> w </access> </argument> <argument> <type> int </type> <name> size </name> </argument> </arguments> </kernel> </kernels> </bitstream>","title":"Bitstream file reference"},{"location":"reference/file-formats/bitstream/#what-is-a-bitstream-repository","text":"A bitstream repository manages your end-to-end bitstream lifecycle while providing consistency to your CI/CD workflow. A bitstream repository is both a source for bitstream artifacts needed by an accelerated application, and a target to deploy bitstreams generated in the build process. The bitstream repository is crucial for the software development process. Multiple developers from different sites use artifacts and 3 rd party components from different sources, causing testing problems to arise and thus slowing down your release process. Add to all the above, the complexity of handling dozens of different types of technologies and your software development process will be brought to a halt. InAccel\u2019s universal bitstream packaging format supports the requirements of both FPGA vendors (Intel, Xilinx) and is also integrated with all major repository managers currently available (including JFrog Artifactory and Sonatype Nexus ).","title":"What is a bitstream repository?"},{"location":"reference/file-formats/bitstream/#what-is-a-bitstream-artifact","text":"","title":"What is a bitstream artifact?"},{"location":"reference/file-formats/bitstream/#bitstream","text":"Field name Type Required/Optional Description name string required The actual name of the FPGA bitstream. bitstreamId string required The desired package name (recommended layout a reverse-DNS [organisation].[package] ). version string required The artifact version. description string optional Short description that may include hardware design notes. platform *platform required Defines platform specific information. kernels []kernel required Describes the available kernels (inside the bitstream) and their properties. Sample: JSON XML { \"name\" : \"bitstream.bin\" , \"bitstreamId\" : \"com.inaccel.math.vector\" , \"version\" : \"0.1\" , \"description\" : \"vector operations\" , \"platform\" : { ... }, \"kernels\" : [ ... ] } <bitstream> <name> bitstream.bin </name> <bitstreamId> com.inaccel.math.vector </name> <version> 0.1 </version> <description> vector operations </description> <platform> ... </platform> <kernels> ... </kernels> </bitstream>","title":"bitstream"},{"location":"reference/file-formats/bitstream/#platform","text":"Field name Type Required/Optional Description vendor string required The FPGA vendor ( intel or xilinx ). name string required The FPGA board identifier. version string required The DSA (for Xilinx) or SDK (for Intel) version. Sample: JSON XML \"platform\" : { \"vendor\" : \"intel / xilinx\" , \"name\" : \"cool-fpga-platform-name\" , \"version\" : \"cool-fpga-platform-version\" }, <platform> <vendor> intel / xilinx </vendor> <name> cool-fpga-platform-name </name> <version> cool-fpga-platform-version </version> </platform>","title":"platform"},{"location":"reference/file-formats/bitstream/#kernel","text":"Field name Type Required/Optional Description name []string required The actual name list of the FPGA kernels. kernelId string required The desired accelerator name. arguments []argument required The list of the accelerator arguments. Sample: JSON XML \"kernels\" : [ ... { \"name\" : [ \"vadd_kernel_1\" ], \"kernelId\" : \"addition\" , \"arguments\" : [ ... ] }, { \"name\" : [ \"vsub_kernel_0\" ], \"kernelId\" : \"subtraction\" , \"arguments\" : [ ... ] }, ... ] <kernels> ... <kernel> <name> vadd_kernel_1 </name> <kernelId> addition </kernelId> <arguments> ... </arguments> </kernel> <kernel> <name> vsub_kernel_0 </name> <kernelId> subtraction </kernelId> <arguments> ... </arguments> </kernel> ... </kernels>","title":"kernel"},{"location":"reference/file-formats/bitstream/#argument","text":"Field name Type Required/Optional Description index []string optional Used to modify the default argument sequence. type string required The argument type. name string optional The argument name. size string optional Used to define the size of unknown argument types. memory []string optional Defines the memory bank connections. (default: 0 ) access string optional Defines the access modifier. (default: rw ) Sample: JSON XML \"arguments\" : [ { \"type\" : \"float*\" , \"name\" : \"input1\" , \"memory\" : [ \"0\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"input2\" , \"memory\" : [ \"0\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"output\" , \"memory\" : [ \"0\" ], \"access\" : \"w\" }, { \"type\" : \"int\" , \"name\" : \"size\" } ] <arguments> <argument> <type> float* </type> <name> input1 </name> <memory> 0 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> input2 </name> <memory> 0 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> output </name> <memory> 0 </memory> <access> w </access> </argument> <argument> <type> int </type> <name> size </name> </argument> </arguments>","title":"argument"},{"location":"reference/file-formats/bitstream/#full-sample","text":"Let's suppose that we have built a new bitstream binary (with name bitstream.bin ) for our cool-fpga-platform . Our bitstream contains four kernels and we want to describe it in a specification file in order to deploy it as a package. JSON XML { \"name\" : \"bitstream.bin\" , \"bitstreamId\" : \"com.inaccel.math.vector\" , \"version\" : \"0.1\" , \"description\" : \"vector operations\" , \"platform\" : { \"vendor\" : \"intel / xilinx\" , \"name\" : \"cool-fpga-fpga-platform-name\" , \"version\" : \"cool-fpga-platform-version\" }, \"kernels\" : [ { \"name\" : [ \"vadd_kernel_0\" ], \"kernelId\" : \"addition\" , \"arguments\" : [ { \"type\" : \"float*\" , \"name\" : \"input1\" , \"memory\" : [ \"0\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"input2\" , \"memory\" : [ \"0\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"output\" , \"memory\" : [ \"0\" ], \"access\" : \"w\" }, { \"type\" : \"int\" , \"name\" : \"size\" } ] }, { \"name\" : [ \"vadd_kernel_1\" ], \"kernelId\" : \"addition\" , \"arguments\" : [ { \"type\" : \"float*\" , \"name\" : \"input1\" , \"memory\" : [ \"1\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"input2\" , \"memory\" : [ \"1\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"output\" , \"memory\" : [ \"1\" ], \"access\" : \"w\" }, { \"type\" : \"int\" , \"name\" : \"size\" } ] }, { \"name\" : [ \"vsub_kernel_0\" ], \"kernelId\" : \"subtraction\" , \"arguments\" : [ { \"type\" : \"float*\" , \"name\" : \"input1\" , \"memory\" : [ \"2\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"input2\" , \"memory\" : [ \"2\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"output\" , \"memory\" : [ \"2\" ], \"access\" : \"w\" }, { \"type\" : \"int\" , \"name\" : \"size\" } ] }, { \"name\" : [ \"vsub_kernel_1\" ], \"kernelId\" : \"subtraction\" , \"arguments\" : [ { \"type\" : \"float*\" , \"name\" : \"input1\" , \"memory\" : [ \"3\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"input2\" , \"memory\" : [ \"3\" ], \"access\" : \"r\" }, { \"type\" : \"float*\" , \"name\" : \"output\" , \"memory\" : [ \"3\" ], \"access\" : \"w\" }, { \"type\" : \"int\" , \"name\" : \"size\" } ] } ] } <bitstream> <name> bitstream.bin </name> <bitstreamId> com.inaccel.math.vector </bitstreamId> <version> 0.1 </version> <description> vector operations </description> <platform> <vendor> intel / xilinx </vendor> <name> cool-fpga-fpga-platform-name </name> <version> cool-fpga-platform-version </version> </platform> <kernels> <kernel> <name> vadd_kernel_0 </name> <kernelId> addition </kernelId> <arguments> <argument> <type> float* </type> <name> input1 </name> <memory> 0 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> input2 </name> <memory> 0 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> output </name> <memory> 0 </memory> <access> w </access> </argument> <argument> <type> int </type> <name> size </name> </argument> </arguments> </kernel> <kernel> <name> vadd_kernel_1 </name> <kernelId> addition </kernelId> <arguments> <argument> <type> float* </type> <name> input1 </name> <memory> 1 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> input2 </name> <memory> 1 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> output </name> <memory> 1 </memory> <access> w </access> </argument> <argument> <type> int </type> <name> size </name> </argument> </arguments> </kernel> <kernel> <name> vsub_kernel_0 </name> <kernelId> subtraction </kernelId> <arguments> <argument> <type> float* </type> <name> input1 </name> <memory> 2 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> input2 </name> <memory> 2 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> output </name> <memory> 2 </memory> <access> w </access> </argument> <argument> <type> int </type> <name> size </name> </argument> </arguments> </kernel> <kernel> <name> vsub_kernel_1 </name> <kernelId> subtraction </kernelId> <arguments> <argument> <type> float* </type> <name> input1 </name> <memory> 3 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> input2 </name> <memory> 3 </memory> <access> r </access> </argument> <argument> <type> float* </type> <name> output </name> <memory> 3 </memory> <access> w </access> </argument> <argument> <type> int </type> <name> size </name> </argument> </arguments> </kernel> </kernels> </bitstream>","title":"Full Sample"},{"location":"reference/inaccel/","text":"Description - The base command for the InAccel CLI Usage - inaccel [global options] command [command options] [arguments...] Child commands - Command Description inaccel bitstream Manages bitstream repositories inaccel config Configures bitstream repositories","title":"inaccel (base command)"},{"location":"reference/inaccel/#description","text":"The base command for the InAccel CLI","title":"Description"},{"location":"reference/inaccel/#usage","text":"inaccel [global options] command [command options] [arguments...]","title":"Usage"},{"location":"reference/inaccel/#child-commands","text":"Command Description inaccel bitstream Manages bitstream repositories inaccel config Configures bitstream repositories","title":"Child commands"},{"location":"reference/inaccel/bitstream/","text":"Description - Manages bitstream repositories Usage - inaccel bitstream command [command options] [arguments...] Child commands - Command Description inaccel bitstream parse Parse FPGA binary build-metadata inaccel bitstream install Install a bitstream to the local or a remote repository, from a local or a remote source inaccel bitstream list List all the bitstreams or detailed information for specific bitstreams in the local or a remote repository inaccel bitstream remove Remove one or more bitstreams from the local or a remote repository Parent command - Command Description inaccel The base command for the InAccel CLI","title":"inaccel bitstream"},{"location":"reference/inaccel/bitstream/#description","text":"Manages bitstream repositories","title":"Description"},{"location":"reference/inaccel/bitstream/#usage","text":"inaccel bitstream command [command options] [arguments...]","title":"Usage"},{"location":"reference/inaccel/bitstream/#child-commands","text":"Command Description inaccel bitstream parse Parse FPGA binary build-metadata inaccel bitstream install Install a bitstream to the local or a remote repository, from a local or a remote source inaccel bitstream list List all the bitstreams or detailed information for specific bitstreams in the local or a remote repository inaccel bitstream remove Remove one or more bitstreams from the local or a remote repository","title":"Child commands"},{"location":"reference/inaccel/bitstream/#parent-command","text":"Command Description inaccel The base command for the InAccel CLI","title":"Parent command"},{"location":"reference/inaccel/bitstream/install/","text":"Description - Install a bitstream to the local or a remote repository, from a local or a remote source Usage - inaccel bitstream install [command options] [<directory>] Where <directory> is the path to the bitstream directory source, defaults to the current directory. Options - Name, shorthand Default Description --repository local Choose the target repository id --mode, -m user Set access rights (if target is the local repository): [u]ser, [g]roup, [o]thers --user, -u Specify the user for both FTP and HTTP file retrieval --password, -p Specify the password for both FTP and HTTP file retrieval Parent command - Command Description inaccel bitstream Manages bitstream repositories","title":"inaccel bitstream install"},{"location":"reference/inaccel/bitstream/install/#description","text":"Install a bitstream to the local or a remote repository, from a local or a remote source","title":"Description"},{"location":"reference/inaccel/bitstream/install/#usage","text":"inaccel bitstream install [command options] [<directory>] Where <directory> is the path to the bitstream directory source, defaults to the current directory.","title":"Usage"},{"location":"reference/inaccel/bitstream/install/#options","text":"Name, shorthand Default Description --repository local Choose the target repository id --mode, -m user Set access rights (if target is the local repository): [u]ser, [g]roup, [o]thers --user, -u Specify the user for both FTP and HTTP file retrieval --password, -p Specify the password for both FTP and HTTP file retrieval","title":"Options"},{"location":"reference/inaccel/bitstream/install/#parent-command","text":"Command Description inaccel bitstream Manages bitstream repositories","title":"Parent command"},{"location":"reference/inaccel/bitstream/list/","text":"Description - List all the bitstreams or detailed information for specific bitstreams in the local or a remote repository Usage - inaccel bitstream list [command options] [<checksum>...] Where <checksum> is the md5 hash of the bitstream information file that you are listing. Options - Name, shorthand Default Description --repository local Choose the target repository id Parent command - Command Description inaccel bitstream Manages bitstream repositories","title":"inaccel bitstream list"},{"location":"reference/inaccel/bitstream/list/#description","text":"List all the bitstreams or detailed information for specific bitstreams in the local or a remote repository","title":"Description"},{"location":"reference/inaccel/bitstream/list/#usage","text":"inaccel bitstream list [command options] [<checksum>...] Where <checksum> is the md5 hash of the bitstream information file that you are listing.","title":"Usage"},{"location":"reference/inaccel/bitstream/list/#options","text":"Name, shorthand Default Description --repository local Choose the target repository id","title":"Options"},{"location":"reference/inaccel/bitstream/list/#parent-command","text":"Command Description inaccel bitstream Manages bitstream repositories","title":"Parent command"},{"location":"reference/inaccel/bitstream/parse/","text":"Description - Parse FPGA binary build-metadata Usage - inaccel bitstream parse [command options] Options - Name, shorthand Default Description --input, -i /dev/stdin The bitstream binary build-metadata source --output, -o /dev/stdout The bistream specification target --format json Choose the print format between JSON and XML --raw Skip field decoration process Parent command - Command Description inaccel bitstream Manages bitstream repositories Examples - Extract the bitstream specification... - ...from an Intel binary file ( .aocx ) - BITSTREAM = \"binary.aocx\" ; ( aocl binedit ${ BITSTREAM } get .acl.fpga.bin /tmp/fpga.bin && \\ aocl binedit /tmp/fpga.bin print .acl.gbs.gz | gunzip | packager gbs-info --gbs = < ( cat ) && \\ rm /tmp/fpga.bin ; aocl binedit ${ BITSTREAM } print .acl.kernel_arg_info.xml ) | \\ inaccel parse -o bitstream.json ...from a Xilinx binary file ( .xclbin ) - BITSTREAM = \"binary.xclbin\" ; echo -n \"BUILD_METADATA CONNECTIVITY IP_LAYOUT\" | xargs --delimiter \" \" --replace \\ xclbinutil --input ${ BITSTREAM } --dump-section {} :JSON:> ( cat ) --force & > /dev/null | \\ inaccel parse -o bitstream.json","title":"inaccel bitstream parse"},{"location":"reference/inaccel/bitstream/parse/#description","text":"Parse FPGA binary build-metadata","title":"Description"},{"location":"reference/inaccel/bitstream/parse/#usage","text":"inaccel bitstream parse [command options]","title":"Usage"},{"location":"reference/inaccel/bitstream/parse/#options","text":"Name, shorthand Default Description --input, -i /dev/stdin The bitstream binary build-metadata source --output, -o /dev/stdout The bistream specification target --format json Choose the print format between JSON and XML --raw Skip field decoration process","title":"Options"},{"location":"reference/inaccel/bitstream/parse/#parent-command","text":"Command Description inaccel bitstream Manages bitstream repositories","title":"Parent command"},{"location":"reference/inaccel/bitstream/parse/#examples","text":"","title":"Examples"},{"location":"reference/inaccel/bitstream/parse/#extract-the-bitstream-specification","text":"","title":"Extract the bitstream specification..."},{"location":"reference/inaccel/bitstream/remove/","text":"Description - Remove one or more bitstreams from the local or a remote repository Usage - inaccel bitstream remove [command options] <checksum>... Where <checksum> is the md5 hash of the bitstream information file that you are removing. Options - Name, shorthand Default Description --repository local Choose the target repository id --force, -f Force the removal of the bitstreams, never prompt Parent command - Command Description inaccel bitstream Manages bitstream repositories","title":"inaccel bitstream remove"},{"location":"reference/inaccel/bitstream/remove/#description","text":"Remove one or more bitstreams from the local or a remote repository","title":"Description"},{"location":"reference/inaccel/bitstream/remove/#usage","text":"inaccel bitstream remove [command options] <checksum>... Where <checksum> is the md5 hash of the bitstream information file that you are removing.","title":"Usage"},{"location":"reference/inaccel/bitstream/remove/#options","text":"Name, shorthand Default Description --repository local Choose the target repository id --force, -f Force the removal of the bitstreams, never prompt","title":"Options"},{"location":"reference/inaccel/bitstream/remove/#parent-command","text":"Command Description inaccel bitstream Manages bitstream repositories","title":"Parent command"},{"location":"reference/inaccel/config/","text":"Description - Configures bitstream repositories Usage - inaccel config command [command options] [arguments...] Child commands - Command Description inaccel config repository Define the details of a remote repository Parent command - Command Description inaccel The base command for the InAccel CLI","title":"inaccel config"},{"location":"reference/inaccel/config/#description","text":"Configures bitstream repositories","title":"Description"},{"location":"reference/inaccel/config/#usage","text":"inaccel config command [command options] [arguments...]","title":"Usage"},{"location":"reference/inaccel/config/#child-commands","text":"Command Description inaccel config repository Define the details of a remote repository","title":"Child commands"},{"location":"reference/inaccel/config/#parent-command","text":"Command Description inaccel The base command for the InAccel CLI","title":"Parent command"},{"location":"reference/inaccel/config/repository/","text":"Description - Define the details of a remote repository Usage - inaccel config repository [command options] <id> Where <id> is the id of the remote repository. Options - Name, shorthand Default Description --user, -u The user required to authenticate to this repository --password, -p The password required to authenticate to this repository --url The base URL of this repository Parent command - Command Description inaccel config Configures bitstream repositories and coral container settings","title":"inaccel config repository"},{"location":"reference/inaccel/config/repository/#description","text":"Define the details of a remote repository","title":"Description"},{"location":"reference/inaccel/config/repository/#usage","text":"inaccel config repository [command options] <id> Where <id> is the id of the remote repository.","title":"Usage"},{"location":"reference/inaccel/config/repository/#options","text":"Name, shorthand Default Description --user, -u The user required to authenticate to this repository --password, -p The password required to authenticate to this repository --url The base URL of this repository","title":"Options"},{"location":"reference/inaccel/config/repository/#parent-command","text":"Command Description inaccel config Configures bitstream repositories and coral container settings","title":"Parent command"},{"location":"setup/kubernetes/","text":"Kubernetes on FPGAs - Scale-out FPGA-Accelerated Applications - Kubernetes on FPGAs enables enterprises to scale out application deployment to multi-cloud FPGA clusters seamlessly. It lets you automate the deployment , maintenance , scheduling and operation of multiple FPGA accelerated application containers across clusters of nodes. With increasing number of FPGA powered applications and services and the broad availability of FPGAs in public cloud, there is a need for open-source Kubernetes to be FPGA-aware. With Kubernetes on FPGAs , software developers and DevOps engineers can build and deploy FPGA-accelerated applications to heterogeneous FPGA clusters at scale, seamlessly. Prerequisites - This section details the prerequisites for setting up a Kubernetes node. The prerequisites include: The worker nodes must be provisioned with the Intel FPGA drivers. Cluster Management - Kubernetes offers a number of features that cluster admins can leverage in order to better manage FPGAs: InAccel FPGA plugin allows you to expose the FPGA resources to the Kubernetes API. Labels allow you to identify FPGA nodes and attributes to steer workloads accordingly. The following sections describe how these features can be used. Device Plugin - Starting in version 1.8, Kubernetes provides a device plugin framework for vendors to advertise their resources to the kubelet without changing Kubernetes core code. Instead of writing custom Kubernetes code, vendors can implement a device plugin that can be deployed manually or as a DaemonSet. The InAccel FPGA plugin for Kubernetes is a DaemonSet that allows you to automatically: Expose the type and number of FPGAs on each node of your cluster Keep track of the health of your FPGAs Run FPGA enabled containers in your Kubernetes cluster If you have setup your nodes as presented in the above sections, you only need to deploy a DaemonSet. The FPGA information will show up on your node fairly quickly. Run the following commands to create the device plugin and watch the FPGA informations being exposed inside your cluster: helm repo add inaccel https://setup.inaccel.com/helm helm install inaccel inaccel/fpga-operator --set license = ... To check the health of your cluster, run the following command on the master node and make sure your FPGA worker nodes appear. kubectl describe nodes InAccel Labels - InAccel exposes a standard set of labels for steering your workloads to different nodes. If you describe the nodes you will also see a number of labels: Labels: ... intel/pac_a10=38d782e3b6125343b9342433e348ac4c ... With InAccel FPGA Operator in use, you can specify the FPGA platform version in the Pod spec: apiVersion : v1 kind : Pod metadata : name : jupyter-lab labels : inaccel/fpga : enabled spec : containers : - name : jupyter-lab image : inaccel/jupyter:lab ports : - containerPort : 8888 resources : limits : intel/pac_a10 : 2 nodeSelector : intel/pac_a10 : 38d782e3b6125343b9342433e348ac4c # pr/interface_id This will ensure that the Pod will be scheduled to a node that has the FPGA requirements you specified.","title":"Kubernetes"},{"location":"setup/kubernetes/#kubernetes-on-fpgas","text":"","title":"Kubernetes on FPGAs"},{"location":"setup/kubernetes/#scale-out-fpga-accelerated-applications","text":"Kubernetes on FPGAs enables enterprises to scale out application deployment to multi-cloud FPGA clusters seamlessly. It lets you automate the deployment , maintenance , scheduling and operation of multiple FPGA accelerated application containers across clusters of nodes. With increasing number of FPGA powered applications and services and the broad availability of FPGAs in public cloud, there is a need for open-source Kubernetes to be FPGA-aware. With Kubernetes on FPGAs , software developers and DevOps engineers can build and deploy FPGA-accelerated applications to heterogeneous FPGA clusters at scale, seamlessly.","title":"Scale-out FPGA-Accelerated Applications"},{"location":"setup/kubernetes/#prerequisites","text":"This section details the prerequisites for setting up a Kubernetes node. The prerequisites include: The worker nodes must be provisioned with the Intel FPGA drivers.","title":"Prerequisites"},{"location":"setup/kubernetes/#cluster-management","text":"Kubernetes offers a number of features that cluster admins can leverage in order to better manage FPGAs: InAccel FPGA plugin allows you to expose the FPGA resources to the Kubernetes API. Labels allow you to identify FPGA nodes and attributes to steer workloads accordingly. The following sections describe how these features can be used.","title":"Cluster Management"},{"location":"setup/kubernetes/#device-plugin","text":"Starting in version 1.8, Kubernetes provides a device plugin framework for vendors to advertise their resources to the kubelet without changing Kubernetes core code. Instead of writing custom Kubernetes code, vendors can implement a device plugin that can be deployed manually or as a DaemonSet. The InAccel FPGA plugin for Kubernetes is a DaemonSet that allows you to automatically: Expose the type and number of FPGAs on each node of your cluster Keep track of the health of your FPGAs Run FPGA enabled containers in your Kubernetes cluster If you have setup your nodes as presented in the above sections, you only need to deploy a DaemonSet. The FPGA information will show up on your node fairly quickly. Run the following commands to create the device plugin and watch the FPGA informations being exposed inside your cluster: helm repo add inaccel https://setup.inaccel.com/helm helm install inaccel inaccel/fpga-operator --set license = ... To check the health of your cluster, run the following command on the master node and make sure your FPGA worker nodes appear. kubectl describe nodes","title":"Device Plugin"},{"location":"setup/kubernetes/#inaccel-labels","text":"InAccel exposes a standard set of labels for steering your workloads to different nodes. If you describe the nodes you will also see a number of labels: Labels: ... intel/pac_a10=38d782e3b6125343b9342433e348ac4c ... With InAccel FPGA Operator in use, you can specify the FPGA platform version in the Pod spec: apiVersion : v1 kind : Pod metadata : name : jupyter-lab labels : inaccel/fpga : enabled spec : containers : - name : jupyter-lab image : inaccel/jupyter:lab ports : - containerPort : 8888 resources : limits : intel/pac_a10 : 2 nodeSelector : intel/pac_a10 : 38d782e3b6125343b9342433e348ac4c # pr/interface_id This will ensure that the Pod will be scheduled to a node that has the FPGA requirements you specified.","title":"InAccel Labels"}]}